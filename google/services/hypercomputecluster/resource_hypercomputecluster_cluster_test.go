// Copyright (c) HashiCorp, Inc.
// SPDX-License-Identifier: MPL-2.0
// ----------------------------------------------------------------------------
//
//	***     AUTO GENERATED CODE    ***    Type: Handwritten     ***
//
// ----------------------------------------------------------------------------
//
//	This code is generated by Magic Modules using the following:
//
//	Source file: https://github.com/GoogleCloudPlatform/magic-modules/tree/main/mmv1/third_party/terraform/services/hypercomputecluster/resource_hypercomputecluster_cluster_test.go
//
//	DO NOT EDIT this file directly. Any changes made to this file will be
//	overwritten during the next generation cycle.
//
// ----------------------------------------------------------------------------
package hypercomputecluster_test

import (
	"testing"

	"github.com/hashicorp/terraform-plugin-testing/helper/resource"
	"github.com/hashicorp/terraform-plugin-testing/plancheck"

	"github.com/hashicorp/terraform-provider-google/google/acctest"
)

func TestAccHypercomputeclusterCluster_update(t *testing.T) {
	t.Parallel()

	context := map[string]interface{}{
		"random_suffix": acctest.RandString(t, 8),
	}

	acctest.VcrTest(t, resource.TestCase{
		PreCheck:                 func() { acctest.AccTestPreCheck(t) },
		ProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories(t),
		Steps: []resource.TestStep{
			{
				Config: testAccHypercomputeclusterCluster_full(context),
			},
			{
				ResourceName:            "google_hypercomputecluster_cluster.cluster",
				ImportState:             true,
				ImportStateVerify:       true,
				ImportStateVerifyIgnore: []string{"cluster_id", "labels", "location", "terraform_labels"},
			},
			{
				Config: testAccHypercomputeclusterCluster_update(context),
				ConfigPlanChecks: resource.ConfigPlanChecks{
					PreApply: []plancheck.PlanCheck{
						plancheck.ExpectResourceAction(
							"google_hypercomputecluster_cluster.cluster",
							plancheck.ResourceActionUpdate,
						),
					},
				},
			},
			{
				ResourceName:            "google_hypercomputecluster_cluster.cluster",
				ImportState:             true,
				ImportStateVerify:       true,
				ImportStateVerifyIgnore: []string{"cluster_id", "labels", "location", "terraform_labels"},
			},
		},
	})
}

func TestAccHypercomputeclusterCluster_existing(t *testing.T) {
	// TODO: fix permissions in CI project
	t.Skip("Skipping test temporarily: pending permission fix")
	t.Parallel()

	context := map[string]interface{}{
		"random_suffix": acctest.RandString(t, 8),
	}

	acctest.VcrTest(t, resource.TestCase{
		PreCheck:                 func() { acctest.AccTestPreCheck(t) },
		ProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories(t),
		Steps: []resource.TestStep{
			{
				Config: testAccHypercomputeclusterCluster_existing(context),
			},
			{
				ResourceName:            "google_hypercomputecluster_cluster.cluster",
				ImportState:             true,
				ImportStateVerify:       true,
				ImportStateVerifyIgnore: []string{"cluster_id", "labels", "location", "terraform_labels"},
			},
		},
	})
}

func TestAccHypercomputeclusterCluster_new(t *testing.T) {
	t.Parallel()

	context := map[string]interface{}{
		"random_suffix": acctest.RandString(t, 8),
	}

	acctest.VcrTest(t, resource.TestCase{
		PreCheck:                 func() { acctest.AccTestPreCheck(t) },
		ProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories(t),
		Steps: []resource.TestStep{
			{
				Config: testAccHypercomputeclusterCluster_new(context),
			},
			{
				ResourceName:            "google_hypercomputecluster_cluster.cluster",
				ImportState:             true,
				ImportStateVerify:       true,
				ImportStateVerifyIgnore: []string{"cluster_id", "labels", "location", "terraform_labels"},
			},
		},
	})
}

func testAccHypercomputeclusterCluster_full(context map[string]interface{}) string {
	return acctest.Nprintf(`
data "google_project" "project" {
}

locals {
  project_id = data.google_project.project.name
}

resource "google_hypercomputecluster_cluster" "cluster" {
  cluster_id                  = "tf%{random_suffix}"
  location                    = "us-central1"
  description                 = "Cluster Director instance created through Terraform"
  labels = {
    old-key = "old-value"
  }
  network_resources {
    id = "network-default"
    config {
      new_network {
        network = "projects/${local.project_id}/global/networks/net-%{random_suffix}"
      }
    }
  }
  compute_resources {
    id = "compute-spot"
    config {
      new_spot_instances {
        machine_type = "n2-standard-2"
        zone = "us-central1-a"
        termination_action = "STOP"
      }
    }
  }
  storage_resources {
    id = "storage-old"
    config {
      new_bucket {
        storage_class = "STANDARD"
        bucket = "bucket-old-%{random_suffix}"
      }
    }
  }
  orchestrator {
    slurm {
      login_nodes {
        machine_type = "n2-standard-2"        
        count = 1
        zone = "us-central1-a"
        boot_disk {
          size_gb = "100"
          type = "pd-balanced"
        }
        enable_os_login = "true"
        enable_public_ips = "true"
        labels = {
          old-key = "old-value"
        }
        startup_script = "#! /bin/bash"
        storage_configs {
          id = "storage-old"
          local_mount = "/home"
        }
      }
      node_sets {
        id = "nodeset"
        compute_id = "compute-spot"
        static_node_count = 1
        max_dynamic_node_count = 1
        compute_instance {
          boot_disk {
            size_gb = "100"
            type = "pd-balanced"
          }
          labels = {
            old-key = "old-value"
          }
          startup_script = "#! /bin/bash"
        }
        storage_configs {
          id = "storage-old"
          local_mount = "/home"
        }
      }
      partitions {
        id = "partition"
        node_set_ids = ["nodeset"]
      }
      default_partition = "partition"
      epilog_bash_scripts = ["#! /bin/bash"]
      prolog_bash_scripts = ["#! /bin/bash"]
    }
  }
}
`, context)
}

func testAccHypercomputeclusterCluster_update(context map[string]interface{}) string {
	return acctest.Nprintf(`
data "google_project" "project" {
}

locals {
  project_id = data.google_project.project.name
}

resource "google_hypercomputecluster_cluster" "cluster" {
  cluster_id                  = "tf%{random_suffix}"
  location                    = "us-central1"
  description                 = "Cluster Director instance created through Terraform (updated)"
  labels = {
    new-key = "new-value"
  }
  network_resources {
    id = "network-default"
    config {
      new_network {
        network = "projects/${local.project_id}/global/networks/net-%{random_suffix}"
      }
    }
  }
  compute_resources {
    id = "compute-spot-new"
    config {
      new_spot_instances {
        machine_type = "n2-standard-2"
        zone = "us-central1-a"
        termination_action = "DELETE"
      }
    }
  }
  storage_resources {
    id = "storage-new"
    config {
      new_bucket {
        storage_class = "STANDARD"
        bucket = "bucket-new-%{random_suffix}"
      }
    }
  }
  orchestrator {
    slurm {
      login_nodes {
        machine_type = "n2-standard-4"        
        count = 2
        zone = "us-central1-a"
        boot_disk {
          size_gb = "100"
          type = "pd-balanced"
        }
        enable_os_login = "false"
        enable_public_ips = "false"
        labels = {
          new-key = "new-value"
        }
        storage_configs {
          id = "storage-new"
          local_mount = "/home"
        }
      }
      node_sets {
        id = "nodesetnew"
        compute_id = "compute-spot-new"
        static_node_count = 2
        max_dynamic_node_count = 2
        compute_instance {
          boot_disk {
            size_gb = "100"
            type = "pd-balanced"
          }
          labels = {
            new-key = "new-value"
          }
        }
        storage_configs {
          id = "storage-new"
          local_mount = "/home"
        }
      }
      partitions {
        id = "partitionnew"
        node_set_ids = ["nodesetnew"]
      }
      default_partition = "partitionnew"
    }
  }
}
`, context)
}

func testAccHypercomputeclusterCluster_existing(context map[string]interface{}) string {
	return acctest.Nprintf(`
data "google_project" "project" {
}

locals {
  project_id = data.google_project.project.name
}

resource "google_compute_network" "vpc" {
  name                    = "existing-net-%{random_suffix}"
  auto_create_subnetworks = false
  routing_mode            = "REGIONAL"
}

resource "google_compute_subnetwork" "subnet" {
  name          = "existing-subnet-%{random_suffix}"
  ip_cidr_range = "10.0.1.0/24"
  region        = "us-central1"
  network       = google_compute_network.vpc.id
  private_ip_google_access = true
}

resource "google_compute_firewall" "allow_internal_subnet_communication" {
  name    = "allow-internal-%{random_suffix}"
  network = google_compute_network.vpc.id
  direction = "INGRESS"
  allow {
    protocol = "all"
  }
  source_ranges = ["10.0.1.0/24"]
  priority = 1000
}

resource "google_storage_bucket" "bucket" {
  name     = "bucket-%{random_suffix}"
  location = "US"
  project  = "cloud-hypercomp-dev"
  uniform_bucket_level_access = true
}

resource "google_filestore_instance" "filestore_instance" {
  name     = "filestore-%{random_suffix}"
  location = "us-central1-b"
  tier     = "BASIC_HDD"
  file_shares {
    capacity_gb = 1024
    name        = "share"
  }
  networks {
    network = "existing-net-%{random_suffix}"
    modes   = ["MODE_IPV4"]
  }
  depends_on = [google_compute_network.vpc] 
}

resource "google_compute_global_address" "private_ip_alloc" {
  name          = "lustre-ip-alloc-%{random_suffix}"
  purpose       = "VPC_PEERING"
  address_type  = "INTERNAL"
  prefix_length = 24
  network       = google_compute_network.vpc.id
}

resource "google_service_networking_connection" "servicenetworking_conn" {
  network                 = google_compute_network.vpc.id
  service                 = "servicenetworking.googleapis.com"
  reserved_peering_ranges = [google_compute_global_address.private_ip_alloc.name]
}

resource "google_lustre_instance" "lustre_instance" {
  instance_id                 = "lustre-%{random_suffix}"
  location                    = "us-central1-b"
  filesystem                  = "tffs"
  capacity_gib                = 18000
  network                     = google_compute_network.vpc.id
  per_unit_storage_throughput = 1000
  timeouts {
    create = "120m"
  }
  depends_on = [google_service_networking_connection.servicenetworking_conn]
}

resource "google_hypercomputecluster_cluster" "cluster" {
  cluster_id                  = "tf%{random_suffix}"
  location                    = "us-central1"
  description                 = "Cluster Director instance created through Terraform"
  network_resources {
    id = "network-existing"
    config {
      existing_network {
        network = google_compute_network.vpc.id
        subnetwork = google_compute_subnetwork.subnet.id
      }
    }
  }
  storage_resources {
    id = "bucket-existing"
    config {
      existing_bucket {
        bucket = google_storage_bucket.bucket.name
      }
    }
  }
  storage_resources {
    id = "filestore-existing"
    config {
      existing_filestore {
        filestore = google_filestore_instance.filestore_instance.id
      }
    }
  }
  storage_resources {
    id = "lustre-existing"
    config {
      existing_lustre {
        lustre = google_lustre_instance.lustre_instance.id
      }
    }
  }
  compute_resources {
    id = "compute1"
    config {
      new_on_demand_instances {
        machine_type = "n2-standard-2"
        zone = "us-central1-b"
      }
    }
  }
  orchestrator {
    slurm {
      login_nodes {
        machine_type = "n2-standard-2"        
        count = 1
        zone = "us-central1-b"
        boot_disk {
          size_gb = "100"
          type = "pd-balanced"
        }
        storage_configs {
          id = "bucket-existing"
          local_mount = "/home"
        }
      }
      node_sets {
        id = "nodeset1"
        compute_id = "compute1"
        static_node_count = 1
        compute_instance {
          boot_disk {
            size_gb = "100"
            type = "pd-balanced"
          }
        }
        storage_configs {
          id = "bucket-existing"
          local_mount = "/home"
        }
      }
      partitions {
        id = "partition1"
        node_set_ids = ["nodeset1"]
      }
      default_partition = "partition1"
    }
  }
}
`, context)
}

func testAccHypercomputeclusterCluster_new(context map[string]interface{}) string {
	return acctest.Nprintf(`
data "google_project" "project" {
}

locals {
  project_id = data.google_project.project.name
}

resource "google_compute_reservation" "gce_reservation" {
  name = "gce-reservation-%{random_suffix}"
  zone = "us-central1-a"
  specific_reservation {
    count = 1
    instance_properties {
      min_cpu_platform = "Intel Cascade Lake"
      machine_type     = "n2-standard-2"
    }
  }
}

resource "google_hypercomputecluster_cluster" "cluster" {
  cluster_id                  = "tf%{random_suffix}"
  location                    = "us-central1"
  description                 = "Cluster Director instance created through Terraform"
  network_resources {
    id = "network1"
    config {
      new_network {
        description = "Network one"
        network = "projects/${local.project_id}/global/networks/net-%{random_suffix}"
      }
    }
  }
  storage_resources {
    id = "bucket-new-1"
    config {
      new_bucket {
        storage_class = "STANDARD"
        bucket = "bucket-new-1-%{random_suffix}"
        hierarchical_namespace {
          enabled = false
        }
      }
    }
  }
  storage_resources {
    id = "bucket-new-2"
    config {
      new_bucket {
        bucket = "bucket-new-2-%{random_suffix}"
        autoclass {
          enabled = true
        }
        hierarchical_namespace {
          enabled = false
        }
      }
    }
  }
  storage_resources {
    id = "filestore-new"
    config {
      new_filestore {
        description = "Filestore instance created via Terraform"
        filestore = "projects/${local.project_id}/locations/us-central1-a/instances/filestore-%{random_suffix}"
        protocol = "NFSV3"
        tier = "ZONAL"
        file_shares {
          capacity_gb = "1024"
          file_share = "share"
        }
      }
    }
  }
  storage_resources {
    id = "lustre-new"
    config {
      new_lustre {
        capacity_gb = "18000"
        description = "Lustre instance created via Terraform"
        filesystem = "lustrefs"
        lustre = "projects/${local.project_id}/locations/us-central1-a/instances/lustre-%{random_suffix}"
      }
    }
  }
  compute_resources {
    id = "compute1"
    config {
      new_on_demand_instances {
        machine_type = "n2-standard-2"
        zone = "us-central1-a"
      }
    }
  }
  compute_resources {
    id = "compute2"
    config {
      new_flex_start_instances {
        machine_type = "a3-megagpu-8g"
        max_duration = "6000s"
        zone = "us-central1-a"
      }
    }
  }
  compute_resources {
    id = "compute3"
    config {
      new_reserved_instances {
        reservation = google_compute_reservation.gce_reservation.id
      }
    }
  }
  orchestrator {
    slurm {
      login_nodes {
        machine_type = "n2-standard-2"        
        count = 1
        zone = "us-central1-a"
        boot_disk {
          size_gb = "100"
          type = "pd-balanced"
        }
        storage_configs {
          id = "bucket-new-1"
          local_mount = "/home"
        }
      }
      node_sets {
        id = "nodeset1"
        compute_id = "compute1"
        static_node_count = 1
        compute_instance {
          boot_disk {
            size_gb = "100"
            type = "pd-balanced"
          }
        }
        storage_configs {
          id = "bucket-new-1"
          local_mount = "/home"
        }
      }
      partitions {
        id = "partition1"
        node_set_ids = ["nodeset1"]
      }
      default_partition = "partition1"
    }
  }
  timeouts {
    create = "60m"
    delete = "60m"
  }
}
`, context)
}
