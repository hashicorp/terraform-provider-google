// Copyright (c) HashiCorp, Inc.
// SPDX-License-Identifier: MPL-2.0

// ----------------------------------------------------------------------------
//
//     ***     AUTO GENERATED CODE    ***    Type: MMv1     ***
//
// ----------------------------------------------------------------------------
//
//     This code is generated by Magic Modules using the following:
//
//     Configuration: https://github.com/GoogleCloudPlatform/magic-modules/tree/main/mmv1/products/hypercomputecluster/Cluster.yaml
//     Template:      https://github.com/GoogleCloudPlatform/magic-modules/tree/main/mmv1/templates/terraform/resource.go.tmpl
//
//     DO NOT EDIT this file directly. Any changes made to this file will be
//     overwritten during the next generation cycle.
//
// ----------------------------------------------------------------------------

package hypercomputecluster

import (
	"bytes"
	"context"
	"encoding/base64"
	"encoding/json"
	"fmt"
	"log"
	"net/http"
	"reflect"
	"regexp"
	"slices"
	"sort"
	"strconv"
	"strings"
	"time"

	"github.com/hashicorp/errwrap"
	"github.com/hashicorp/go-cty/cty"
	"github.com/hashicorp/terraform-plugin-sdk/v2/diag"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/customdiff"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/id"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/logging"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/retry"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/schema"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/structure"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/validation"
	"github.com/hashicorp/terraform-plugin-sdk/v2/terraform"

	"github.com/hashicorp/terraform-provider-google/google/registry"
	"github.com/hashicorp/terraform-provider-google/google/tpgresource"
	transport_tpg "github.com/hashicorp/terraform-provider-google/google/transport"
	"github.com/hashicorp/terraform-provider-google/google/verify"

	"google.golang.org/api/googleapi"
)

func resourceHypercomputeclusterClusterResourceHash(v interface{}) int {
	var buf bytes.Buffer
	m := v.(map[string]interface{})
	if v, ok := m["id"]; ok {
		buf.WriteString(fmt.Sprintf("%s-", v.(string)))
	}
	return schema.HashString(buf.String())
}

var (
	_ = bytes.Clone
	_ = context.WithCancel
	_ = base64.NewDecoder
	_ = json.Marshal
	_ = fmt.Sprintf
	_ = log.Print
	_ = http.Get
	_ = reflect.ValueOf
	_ = regexp.Match
	_ = slices.Min([]int{1})
	_ = sort.IntSlice{}
	_ = strconv.Atoi
	_ = strings.Trim
	_ = time.Now
	_ = errwrap.Wrap
	_ = cty.BoolVal
	_ = diag.Diagnostic{}
	_ = customdiff.All
	_ = id.UniqueId
	_ = logging.LogLevel
	_ = retry.Retry
	_ = schema.Noop
	_ = validation.All
	_ = structure.ExpandJsonFromString
	_ = terraform.State{}
	_ = tpgresource.SetLabels
	_ = transport_tpg.Config{}
	_ = verify.ValidateEnum
	_ = googleapi.Error{}
)

func init() {
	registry.Schema{
		Name:        "google_hypercomputecluster_cluster",
		ProductName: "hypercomputecluster",
		Type:        registry.SchemaTypeResource,
		Schema:      ResourceHypercomputeclusterCluster(),
	}.Register()
}

func ResourceHypercomputeclusterCluster() *schema.Resource {
	return &schema.Resource{
		Create: resourceHypercomputeclusterClusterCreate,
		Read:   resourceHypercomputeclusterClusterRead,
		Update: resourceHypercomputeclusterClusterUpdate,
		Delete: resourceHypercomputeclusterClusterDelete,

		Importer: &schema.ResourceImporter{
			State: resourceHypercomputeclusterClusterImport,
		},

		Timeouts: &schema.ResourceTimeout{
			Create: schema.DefaultTimeout(20 * time.Minute),
			Update: schema.DefaultTimeout(20 * time.Minute),
			Delete: schema.DefaultTimeout(20 * time.Minute),
		},

		CustomizeDiff: customdiff.All(
			tpgresource.SetLabelsDiff,
			tpgresource.DefaultProviderProject,
		),

		Schema: map[string]*schema.Schema{
			"cluster_id": {
				Type:     schema.TypeString,
				Required: true,
				ForceNew: true,
				Description: `ID of the cluster to create. Must conform to
[RFC-1034](https://datatracker.ietf.org/doc/html/rfc1034) (lower-case,
alphanumeric, and at most 63 characters).`,
			},
			"location": {
				Type:        schema.TypeString,
				Required:    true,
				ForceNew:    true,
				Description: `Resource ID segment making up resource 'name'. It identifies the resource within its parent collection as described in https://google.aip.dev/122.`,
			},
			"compute_resources": {
				Type:     schema.TypeSet,
				Optional: true,
				Description: `Compute resources available to the cluster. Keys specify the ID of the
compute resource by which it can be referenced elsewhere, and must conform
to [RFC-1034](https://datatracker.ietf.org/doc/html/rfc1034) (lower-case,
alphanumeric, and at most 63 characters).`,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"id": {
							Type:     schema.TypeString,
							Required: true,
						},
						"config": {
							Type:        schema.TypeList,
							Required:    true,
							Description: `Describes how a compute resource should be created at runtime.`,
							MaxItems:    1,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"new_flex_start_instances": {
										Type:     schema.TypeList,
										Optional: true,
										Description: `When set in a ComputeResourceConfig, indicates that VM instances should
be created using [Flex
Start](https://cloud.google.com/compute/docs/instances/provisioning-models).`,
										MaxItems: 1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"machine_type": {
													Type:     schema.TypeString,
													Required: true,
													Description: `Name of the Compute Engine [machine
type](https://cloud.google.com/compute/docs/machine-resource) to use, e.g.
'n2-standard-2'.`,
												},
												"max_duration": {
													Type:     schema.TypeString,
													Required: true,
													Description: `Specifies the time limit for created instances. Instances will be
terminated at the end of this duration.`,
												},
												"zone": {
													Type:     schema.TypeString,
													Required: true,
													Description: `Name of the zone in which VM instances should run, e.g., 'us-central1-a'.
Must be in the same region as the cluster, and must match the zone of any
other resources specified in the cluster.`,
												},
											},
										},
									},
									"new_on_demand_instances": {
										Type:     schema.TypeList,
										Optional: true,
										Description: `When set in a ComputeResourceConfig, indicates that on-demand (i.e.,
using the standard provisioning model) VM instances should be created.`,
										MaxItems: 1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"machine_type": {
													Type:     schema.TypeString,
													Required: true,
													Description: `Name of the Compute Engine [machine
type](https://cloud.google.com/compute/docs/machine-resource) to use, e.g.
'n2-standard-2'.`,
												},
												"zone": {
													Type:     schema.TypeString,
													Required: true,
													Description: `Name of the zone in which VM instances should run, e.g., 'us-central1-a'.
Must be in the same region as the cluster, and must match the zone of any
other resources specified in the cluster.`,
												},
											},
										},
									},
									"new_reserved_instances": {
										Type:     schema.TypeList,
										Optional: true,
										Description: `When set in a ComputeResourceConfig, indicates that VM instances should
be created from a
[reservation](https://cloud.google.com/compute/docs/instances/reservations-overview).`,
										MaxItems: 1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"reservation": {
													Type:     schema.TypeString,
													Optional: true,
													Description: `Name of the reservation from which VM instances should be created, in the
format 'projects/{project}/zones/{zone}/reservations/{reservation}'.`,
												},
											},
										},
									},
									"new_spot_instances": {
										Type:     schema.TypeList,
										Optional: true,
										Description: `When set in a ComputeResourceConfig, indicates that [spot
VM](https://cloud.google.com/compute/docs/instances/spot) instances should be
created.`,
										MaxItems: 1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"machine_type": {
													Type:     schema.TypeString,
													Required: true,
													Description: `Name of the Compute Engine [machine
type](https://cloud.google.com/compute/docs/machine-resource) to use, e.g.
'n2-standard-2'.`,
												},
												"zone": {
													Type:     schema.TypeString,
													Required: true,
													Description: `Name of the zone in which VM instances should run, e.g., 'us-central1-a'.
Must be in the same region as the cluster, and must match the zone of any
other resources specified in the cluster.`,
												},
												"termination_action": {
													Type:     schema.TypeString,
													Optional: true,
													Description: `Specifies the termination action of the instance
Possible values:
STOP
DELETE`,
												},
											},
										},
									},
								},
							},
						},
					},
				},
				Set: resourceHypercomputeclusterClusterResourceHash,
			},
			"description": {
				Type:        schema.TypeString,
				Optional:    true,
				Description: `User-provided description of the cluster.`,
			},
			"labels": {
				Type:     schema.TypeMap,
				Optional: true,
				Description: `[Labels](https://cloud.google.com/compute/docs/labeling-resources) applied
to the cluster. Labels can be used to organize clusters and to filter them
in queries.

**Note**: This field is non-authoritative, and will only manage the labels present in your configuration.
Please refer to the field 'effective_labels' for all of the labels present on the resource.`,
				Elem: &schema.Schema{Type: schema.TypeString},
			},
			"network_resources": {
				Type:     schema.TypeSet,
				Optional: true,
				Description: `Network resources available to the cluster. Must contain at most one value.
Keys specify the ID of the network resource by which it can be referenced
elsewhere, and must conform to
[RFC-1034](https://datatracker.ietf.org/doc/html/rfc1034) (lower-case,
alphanumeric, and at most 63 characters).`,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"id": {
							Type:     schema.TypeString,
							Required: true,
						},
						"config": {
							Type:     schema.TypeList,
							Optional: true,
							Description: `Describes how a network resource should be initialized. Each network resource
can either be imported from an existing Google Cloud resource or initialized
when the cluster is created.`,
							MaxItems: 1,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"existing_network": {
										Type:     schema.TypeList,
										Optional: true,
										Description: `When set in a NetworkResourceConfig, indicates that an existing network
should be imported.`,
										MaxItems: 1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"network": {
													Type:     schema.TypeString,
													Required: true,
													Description: `Name of the network to import, in the format
'projects/{project}/global/networks/{network}'.`,
												},
												"subnetwork": {
													Type:     schema.TypeString,
													Required: true,
													Description: `Particular subnetwork to use, in the format
'projects/{project}/regions/{region}/subnetworks/{subnetwork}'.`,
												},
											},
										},
									},
									"new_network": {
										Type:     schema.TypeList,
										Optional: true,
										Description: `When set in a NetworkResourceConfig, indicates that a new network should
be created.`,
										MaxItems: 1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"network": {
													Type:     schema.TypeString,
													Required: true,
													Description: `Name of the network to create, in the format
'projects/{project}/global/networks/{network}'.`,
												},
												"description": {
													Type:        schema.TypeString,
													Optional:    true,
													Description: `Description of the network. Maximum of 2048 characters.`,
												},
											},
										},
									},
								},
							},
						},
						"network": {
							Type:     schema.TypeList,
							Computed: true,
							Description: `A reference to a [VPC network](https://cloud.google.com/vpc/docs/vpc) in
Google Compute Engine.`,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"network": {
										Type:     schema.TypeString,
										Computed: true,
										Description: `Name of the network, in the format
'projects/{project}/global/networks/{network}'.`,
									},
									"subnetwork": {
										Type:     schema.TypeString,
										Computed: true,
										Description: `Name of the particular subnetwork being used by the cluster, in the format
'projects/{project}/regions/{region}/subnetworks/{subnetwork}'.`,
									},
								},
							},
						},
					},
				},
				Set: resourceHypercomputeclusterClusterResourceHash,
			},
			"orchestrator": {
				Type:     schema.TypeList,
				Optional: true,
				Description: `The component responsible for scheduling and running workloads on the
cluster as well as providing the user interface for interacting with the
cluster at runtime.`,
				MaxItems: 1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"slurm": {
							Type:     schema.TypeList,
							Optional: true,
							Description: `When set in Orchestrator, indicates that the cluster should use
[Slurm](https://slurm.schedmd.com/) as the orchestrator.`,
							MaxItems: 1,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"login_nodes": {
										Type:     schema.TypeList,
										Required: true,
										Description: `Configuration for Slurm [login
nodes](https://slurm.schedmd.com/quickstart_admin.html#login) in the cluster.
Login nodes are Compute Engine VM instances that allow users to access the
cluster over SSH.`,
										MaxItems: 1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"count": {
													Type:        schema.TypeString,
													Required:    true,
													Description: `Number of login node instances to create.`,
												},
												"machine_type": {
													Type:     schema.TypeString,
													Required: true,
													Description: `Name of the Compute Engine [machine
type](https://cloud.google.com/compute/docs/machine-resource) to use for
login nodes, e.g. 'n2-standard-2'.`,
												},
												"zone": {
													Type:     schema.TypeString,
													Required: true,
													ForceNew: true,
													Description: `Name of the zone in which login nodes should run, e.g., 'us-central1-a'.
Must be in the same region as the cluster, and must match the zone of any
other resources specified in the cluster.`,
												},
												"boot_disk": {
													Type:     schema.TypeList,
													Optional: true,
													Description: `A [Persistent disk](https://cloud.google.com/compute/docs/disks) used as the
boot disk for a Compute Engine VM instance.`,
													MaxItems: 1,
													Elem: &schema.Resource{
														Schema: map[string]*schema.Schema{
															"size_gb": {
																Type:        schema.TypeString,
																Required:    true,
																ForceNew:    true,
																Description: `Size of the disk in gigabytes. Must be at least 10GB.`,
															},
															"type": {
																Type:     schema.TypeString,
																Required: true,
																ForceNew: true,
																Description: `[Persistent disk
type](https://cloud.google.com/compute/docs/disks#disk-types), in the
format 'projects/{project}/zones/{zone}/diskTypes/{disk_type}'.`,
															},
														},
													},
												},
												"enable_os_login": {
													Type:     schema.TypeBool,
													Optional: true,
													Description: `Whether [OS Login](https://cloud.google.com/compute/docs/oslogin) should be
enabled on login node instances.`,
												},
												"enable_public_ips": {
													Type:     schema.TypeBool,
													Optional: true,
													Description: `Whether login node instances should be assigned [external IP
addresses](https://cloud.google.com/compute/docs/ip-addresses#externaladdresses).`,
												},
												"labels": {
													Type:     schema.TypeMap,
													Computed: true,
													Optional: true,
													Description: `[Labels](https://cloud.google.com/compute/docs/labeling-resources) that
should be applied to each login node instance.`,
													Elem: &schema.Schema{Type: schema.TypeString},
												},
												"startup_script": {
													Type:     schema.TypeString,
													Optional: true,
													Description: `[Startup
script](https://cloud.google.com/compute/docs/instances/startup-scripts/linux)
to be run on each login node instance. Max 256KB.
The script must complete within the system-defined default timeout of 5
minutes. For tasks that require more time, consider running them in the
background using methods such as '&' or 'nohup'.`,
												},
												"storage_configs": {
													Type:     schema.TypeList,
													Optional: true,
													Description: `How storage resources should be mounted on each login
node.`,
													Elem: &schema.Resource{
														Schema: map[string]*schema.Schema{
															"id": {
																Type:     schema.TypeString,
																Required: true,
																Description: `ID of the storage resource to mount, which must match a key in the
cluster's [storage_resources](Cluster.storage_resources).`,
															},
															"local_mount": {
																Type:     schema.TypeString,
																Required: true,
																Description: `A directory inside the VM instance's file system where the storage resource
should be mounted (e.g., '/mnt/share').`,
															},
														},
													},
												},
												"instances": {
													Type:     schema.TypeList,
													Computed: true,
													Description: `Information about the login node instances that were created in Compute
Engine.`,
													Elem: &schema.Resource{
														Schema: map[string]*schema.Schema{
															"instance": {
																Type:     schema.TypeString,
																Computed: true,
																Description: `Name of the VM instance, in the format
'projects/{project}/zones/{zone}/instances/{instance}'.`,
															},
														},
													},
												},
											},
										},
									},
									"node_sets": {
										Type:     schema.TypeList,
										Required: true,
										Description: `Configuration of Slurm nodesets, which define groups of compute resources
that can be used by Slurm. At least one compute node is required.`,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"id": {
													Type:     schema.TypeString,
													Required: true,
													Description: `Identifier for the nodeset, which allows it to be referenced by partitions.
Must conform to
[RFC-1034](https://datatracker.ietf.org/doc/html/rfc1034) (lower-case,
alphanumeric, and at most 63 characters).`,
												},
												"compute_id": {
													Type:     schema.TypeString,
													Optional: true,
													Description: `ID of the compute resource on which this nodeset will run. Must match a key
in the cluster's [compute_resources](Cluster.compute_resources).`,
												},
												"compute_instance": {
													Type:     schema.TypeList,
													Optional: true,
													Description: `When set in a SlurmNodeSet, indicates that the nodeset should be backed
by Compute Engine VM instances.`,
													MaxItems: 1,
													Elem: &schema.Resource{
														Schema: map[string]*schema.Schema{
															"boot_disk": {
																Type:     schema.TypeList,
																Optional: true,
																Description: `A [Persistent disk](https://cloud.google.com/compute/docs/disks) used as the
boot disk for a Compute Engine VM instance.`,
																MaxItems: 1,
																Elem: &schema.Resource{
																	Schema: map[string]*schema.Schema{
																		"size_gb": {
																			Type:        schema.TypeString,
																			Required:    true,
																			ForceNew:    true,
																			Description: `Size of the disk in gigabytes. Must be at least 10GB.`,
																		},
																		"type": {
																			Type:     schema.TypeString,
																			Required: true,
																			ForceNew: true,
																			Description: `[Persistent disk
type](https://cloud.google.com/compute/docs/disks#disk-types), in the
format 'projects/{project}/zones/{zone}/diskTypes/{disk_type}'.`,
																		},
																	},
																},
															},
															"labels": {
																Type:     schema.TypeMap,
																Computed: true,
																Optional: true,
																Description: `[Labels](https://cloud.google.com/compute/docs/labeling-resources) that
should be applied to each VM instance in the nodeset.`,
																Elem: &schema.Schema{Type: schema.TypeString},
															},
															"startup_script": {
																Type:     schema.TypeString,
																Optional: true,
																Description: `[Startup
script](https://cloud.google.com/compute/docs/instances/startup-scripts/linux)
to be run on each VM instance in the nodeset. Max 256KB.`,
															},
														},
													},
												},
												"max_dynamic_node_count": {
													Type:     schema.TypeString,
													Optional: true,
													Description: `Controls how many additional nodes a cluster can bring online to handle
workloads. Set this value to enable dynamic node creation and limit the
number of additional nodes the cluster can bring online. Leave empty if you
do not want the cluster to create nodes dynamically, and instead rely only
on static nodes.`,
												},
												"static_node_count": {
													Type:     schema.TypeString,
													Optional: true,
													Description: `Number of nodes to be statically created for this nodeset. The cluster will
attempt to ensure that at least this many nodes exist at all times.`,
												},
												"storage_configs": {
													Type:     schema.TypeList,
													Optional: true,
													Description: `How storage resources should be mounted on each compute
node.`,
													Elem: &schema.Resource{
														Schema: map[string]*schema.Schema{
															"id": {
																Type:     schema.TypeString,
																Required: true,
																Description: `ID of the storage resource to mount, which must match a key in the
cluster's [storage_resources](Cluster.storage_resources).`,
															},
															"local_mount": {
																Type:     schema.TypeString,
																Required: true,
																Description: `A directory inside the VM instance's file system where the storage resource
should be mounted (e.g., '/mnt/share').`,
															},
														},
													},
												},
											},
										},
									},
									"partitions": {
										Type:     schema.TypeList,
										Required: true,
										Description: `Configuration of Slurm partitions, which group one or more nodesets. Acts
as a queue against which jobs can be submitted. At least one partition is
required.`,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"id": {
													Type:     schema.TypeString,
													Required: true,
													Description: `ID of the partition, which is how users will identify it. Must conform to
[RFC-1034](https://datatracker.ietf.org/doc/html/rfc1034) (lower-case,
alphanumeric, and at most 63 characters).`,
												},
												"node_set_ids": {
													Type:     schema.TypeList,
													Required: true,
													Description: `IDs of the nodesets that make up this partition. Values must match
SlurmNodeSet.id.`,
													Elem: &schema.Schema{
														Type: schema.TypeString,
													},
												},
											},
										},
									},
									"default_partition": {
										Type:     schema.TypeString,
										Optional: true,
										Description: `Default partition to use for submitted jobs that do not explicitly specify
a partition. Required if and only if there is more than one partition, in
which case it must match the id of one of the partitions.`,
									},
									"epilog_bash_scripts": {
										Type:     schema.TypeList,
										Computed: true,
										Optional: true,
										Description: `Slurm [epilog scripts](https://slurm.schedmd.com/prolog_epilog.html), which
will be executed by compute nodes whenever a node finishes running a job.
Values must not be empty.`,
										Elem: &schema.Schema{
											Type: schema.TypeString,
										},
									},
									"prolog_bash_scripts": {
										Type:     schema.TypeList,
										Computed: true,
										Optional: true,
										Description: `Slurm [prolog scripts](https://slurm.schedmd.com/prolog_epilog.html), which
will be executed by compute nodes before a node begins running a new job.
Values must not be empty.`,
										Elem: &schema.Schema{
											Type: schema.TypeString,
										},
									},
								},
							},
						},
					},
				},
			},
			"storage_resources": {
				Type:     schema.TypeSet,
				Optional: true,
				Description: `Storage resources available to the cluster. Keys specify the ID of the
storage resource by which it can be referenced elsewhere, and must conform
to [RFC-1034](https://datatracker.ietf.org/doc/html/rfc1034) (lower-case,
alphanumeric, and at most 63 characters).`,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"id": {
							Type:     schema.TypeString,
							Required: true,
						},
						"config": {
							Type:     schema.TypeList,
							Required: true,
							Description: `Describes how a storage resource should be initialized. Each storage resource
can either be imported from an existing Google Cloud resource or initialized
when the cluster is created.`,
							MaxItems: 1,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"existing_bucket": {
										Type:     schema.TypeList,
										Optional: true,
										Description: `When set in a StorageResourceConfig, indicates that an existing
[Google Cloud Storage](https://cloud.google.com/storage) bucket should be
imported.`,
										MaxItems: 1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"bucket": {
													Type:        schema.TypeString,
													Required:    true,
													Description: `Name of the Cloud Storage bucket to import.`,
												},
											},
										},
									},
									"existing_filestore": {
										Type:     schema.TypeList,
										Optional: true,
										Description: `When set in a StorageResourceConfig, indicates that an existing
[Filestore](https://cloud.google.com/filestore) instance should be imported.`,
										MaxItems: 1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"filestore": {
													Type:     schema.TypeString,
													Required: true,
													Description: `Name of the Filestore instance to import, in the format
'projects/{project}/locations/{location}/instances/{instance}'`,
												},
											},
										},
									},
									"existing_lustre": {
										Type:     schema.TypeList,
										Optional: true,
										Description: `When set in a StorageResourceConfig, indicates that an existing
[Managed Lustre](https://cloud.google.com/products/managed-lustre) instance
should be imported.`,
										MaxItems: 1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"lustre": {
													Type:     schema.TypeString,
													Required: true,
													Description: `Name of the Managed Lustre instance to import, in the format
'projects/{project}/locations/{location}/instances/{instance}'`,
												},
											},
										},
									},
									"new_bucket": {
										Type:     schema.TypeList,
										Optional: true,
										Description: `When set in a StorageResourceConfig, indicates that a new
[Google Cloud Storage](https://cloud.google.com/storage) bucket should be
created.`,
										MaxItems: 1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"bucket": {
													Type:        schema.TypeString,
													Required:    true,
													Description: `Name of the Cloud Storage bucket to create.`,
												},
												"autoclass": {
													Type:        schema.TypeList,
													Optional:    true,
													Description: `Message describing Google Cloud Storage autoclass configuration`,
													MaxItems:    1,
													Elem: &schema.Resource{
														Schema: map[string]*schema.Schema{
															"enabled": {
																Type:        schema.TypeBool,
																Required:    true,
																Description: `Enables Auto-class feature.`,
															},
														},
													},
												},
												"hierarchical_namespace": {
													Type:             schema.TypeList,
													Computed:         true,
													Optional:         true,
													DiffSuppressFunc: tpgresource.EmptyOrUnsetBlockDiffSuppress,
													Description:      `Message describing Google Cloud Storage hierarchical namespace configuration`,
													MaxItems:         1,
													Elem: &schema.Resource{
														Schema: map[string]*schema.Schema{
															"enabled": {
																Type:             schema.TypeBool,
																Computed:         true,
																Optional:         true,
																DiffSuppressFunc: tpgresource.EmptyOrFalseSuppressBoolean,
																Description:      `Enables hierarchical namespace setup for the bucket.`,
															},
														},
													},
												},
												"storage_class": {
													Type:     schema.TypeString,
													Optional: true,
													Description: `If set, uses the provided storage class as the bucket's default storage
class.
Possible values:
STANDARD
NEARLINE
COLDLINE
ARCHIVE`,
												},
											},
										},
									},
									"new_filestore": {
										Type:     schema.TypeList,
										Optional: true,
										Description: `When set in a StorageResourceConfig, indicates that a new
[Filestore](https://cloud.google.com/filestore) instance should be created.`,
										MaxItems: 1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"file_shares": {
													Type:     schema.TypeList,
													Required: true,
													Description: `File system shares on the instance. Exactly one file share must be
specified.`,
													Elem: &schema.Resource{
														Schema: map[string]*schema.Schema{
															"capacity_gb": {
																Type:     schema.TypeString,
																Required: true,
																Description: `Size of the filestore in GB. Must be between 1024 and 102400, and must meet
scalability requirements described at
https://cloud.google.com/filestore/docs/service-tiers.`,
															},
															"file_share": {
																Type:        schema.TypeString,
																Required:    true,
																Description: `Filestore share location`,
															},
														},
													},
												},
												"filestore": {
													Type:     schema.TypeString,
													Required: true,
													Description: `Name of the Filestore instance to create, in the format
'projects/{project}/locations/{location}/instances/{instance}'`,
												},
												"tier": {
													Type:         schema.TypeString,
													Required:     true,
													ValidateFunc: verify.ValidateEnum([]string{"TIER_UNSPECIFIED", "ZONAL", "REGIONAL"}),
													Description: `Service tier to use for the instance.
Possible values:
ZONAL
REGIONAL Possible values: ["TIER_UNSPECIFIED", "ZONAL", "REGIONAL"]`,
												},
												"description": {
													Type:        schema.TypeString,
													Optional:    true,
													Description: `Description of the instance. Maximum of 2048 characters.`,
												},
												"protocol": {
													Type:         schema.TypeString,
													Optional:     true,
													ValidateFunc: verify.ValidateEnum([]string{"PROTOCOL_UNSPECIFIED", "NFSV3", "NFSV41", ""}),
													Description: `Access protocol to use for all file shares in the instance. Defaults to NFS
V3 if not set.
Possible values:
NFSV3
NFSV41 Possible values: ["PROTOCOL_UNSPECIFIED", "NFSV3", "NFSV41"]`,
												},
											},
										},
									},
									"new_lustre": {
										Type:     schema.TypeList,
										Optional: true,
										Description: `When set in a StorageResourceConfig, indicates that a new
[Managed Lustre](https://cloud.google.com/products/managed-lustre) instance
should be created.`,
										MaxItems: 1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"capacity_gb": {
													Type:     schema.TypeString,
													Required: true,
													Description: `Storage capacity of the instance in gibibytes (GiB). Allowed values are
between 18000 and 7632000.`,
												},
												"filesystem": {
													Type:     schema.TypeString,
													Required: true,
													Description: `Filesystem name for this instance. This name is used by client-side tools,
including when mounting the instance. Must be 8 characters or less and can
only contain letters and numbers.`,
												},
												"lustre": {
													Type:     schema.TypeString,
													Required: true,
													Description: `Name of the Managed Lustre instance to create, in the format
'projects/{project}/locations/{location}/instances/{instance}'`,
												},
												"description": {
													Type:        schema.TypeString,
													Optional:    true,
													Description: `Description of the Managed Lustre instance. Maximum of 2048 characters.`,
												},
											},
										},
									},
								},
							},
						},
						"bucket": {
							Type:     schema.TypeList,
							Computed: true,
							Description: `A reference to a [Google Cloud Storage](https://cloud.google.com/storage)
bucket.`,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"bucket": {
										Type:        schema.TypeString,
										Computed:    true,
										Description: `Name of the bucket.`,
									},
								},
							},
						},
						"filestore": {
							Type:        schema.TypeList,
							Computed:    true,
							Description: `A reference to a [Filestore](https://cloud.google.com/filestore) instance.`,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"filestore": {
										Type:     schema.TypeString,
										Computed: true,
										Description: `Name of the Filestore instance, in the format
'projects/{project}/locations/{location}/instances/{instance}'`,
									},
								},
							},
						},
						"lustre": {
							Type:     schema.TypeList,
							Computed: true,
							Description: `A reference to a [Managed
Lustre](https://cloud.google.com/products/managed-lustre) instance.`,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"lustre": {
										Type:     schema.TypeString,
										Computed: true,
										Description: `Name of the Managed Lustre instance, in the format
'projects/{project}/locations/{location}/instances/{instance}'`,
									},
								},
							},
						},
					},
				},
				Set: resourceHypercomputeclusterClusterResourceHash,
			},
			"create_time": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `Time that the cluster was originally created.`,
			},
			"effective_labels": {
				Type:        schema.TypeMap,
				Computed:    true,
				Description: `All of labels (key/value pairs) present on the resource in GCP, including the labels configured through Terraform, other clients and services.`,
				Elem:        &schema.Schema{Type: schema.TypeString},
			},
			"name": {
				Type:     schema.TypeString,
				Computed: true,
				Description: `Identifier. [Relative resource name](https://google.aip.dev/122) of the cluster, in the
format 'projects/{project}/locations/{location}/clusters/{cluster}'.`,
			},
			"reconciling": {
				Type:     schema.TypeBool,
				Computed: true,
				Description: `Indicates whether changes to the cluster are currently in flight. If this
is 'true', then the current state might not match the cluster's intended
state.`,
			},
			"terraform_labels": {
				Type:     schema.TypeMap,
				Computed: true,
				Description: `The combination of labels configured directly on the resource
 and default labels configured on the provider.`,
				Elem: &schema.Schema{Type: schema.TypeString},
			},
			"update_time": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `Time that the cluster was most recently updated.`,
			},
			"project": {
				Type:     schema.TypeString,
				Optional: true,
				Computed: true,
				ForceNew: true,
			},
		},
		UseJSONNumber: true,
	}
}

func resourceHypercomputeclusterClusterCreate(d *schema.ResourceData, meta interface{}) error {
	config := meta.(*transport_tpg.Config)
	userAgent, err := tpgresource.GenerateUserAgentString(d, config.UserAgent)
	if err != nil {
		return err
	}

	obj := make(map[string]interface{})
	computeResourcesProp, err := expandHypercomputeclusterClusterComputeResources(d.Get("compute_resources"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("compute_resources"); !tpgresource.IsEmptyValue(reflect.ValueOf(computeResourcesProp)) && (ok || !reflect.DeepEqual(v, computeResourcesProp)) {
		obj["computeResources"] = computeResourcesProp
	}
	descriptionProp, err := expandHypercomputeclusterClusterDescription(d.Get("description"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("description"); !tpgresource.IsEmptyValue(reflect.ValueOf(descriptionProp)) && (ok || !reflect.DeepEqual(v, descriptionProp)) {
		obj["description"] = descriptionProp
	}
	networkResourcesProp, err := expandHypercomputeclusterClusterNetworkResources(d.Get("network_resources"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("network_resources"); !tpgresource.IsEmptyValue(reflect.ValueOf(networkResourcesProp)) && (ok || !reflect.DeepEqual(v, networkResourcesProp)) {
		obj["networkResources"] = networkResourcesProp
	}
	orchestratorProp, err := expandHypercomputeclusterClusterOrchestrator(d.Get("orchestrator"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("orchestrator"); !tpgresource.IsEmptyValue(reflect.ValueOf(orchestratorProp)) && (ok || !reflect.DeepEqual(v, orchestratorProp)) {
		obj["orchestrator"] = orchestratorProp
	}
	storageResourcesProp, err := expandHypercomputeclusterClusterStorageResources(d.Get("storage_resources"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("storage_resources"); !tpgresource.IsEmptyValue(reflect.ValueOf(storageResourcesProp)) && (ok || !reflect.DeepEqual(v, storageResourcesProp)) {
		obj["storageResources"] = storageResourcesProp
	}
	effectiveLabelsProp, err := expandHypercomputeclusterClusterEffectiveLabels(d.Get("effective_labels"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("effective_labels"); !tpgresource.IsEmptyValue(reflect.ValueOf(effectiveLabelsProp)) && (ok || !reflect.DeepEqual(v, effectiveLabelsProp)) {
		obj["labels"] = effectiveLabelsProp
	}

	url, err := tpgresource.ReplaceVars(d, config, "{{HypercomputeclusterBasePath}}projects/{{project}}/locations/{{location}}/clusters?clusterId={{cluster_id}}")
	if err != nil {
		return err
	}

	log.Printf("[DEBUG] Creating new Cluster: %#v", obj)
	billingProject := ""

	project, err := tpgresource.GetProject(d, config)
	if err != nil {
		return fmt.Errorf("Error fetching project for Cluster: %s", err)
	}
	billingProject = project

	// err == nil indicates that the billing_project value was found
	if bp, err := tpgresource.GetBillingProject(d, config); err == nil {
		billingProject = bp
	}

	headers := make(http.Header)
	res, err := transport_tpg.SendRequest(transport_tpg.SendRequestOptions{
		Config:    config,
		Method:    "POST",
		Project:   billingProject,
		RawURL:    url,
		UserAgent: userAgent,
		Body:      obj,
		Timeout:   d.Timeout(schema.TimeoutCreate),
		Headers:   headers,
	})
	if err != nil {
		return fmt.Errorf("Error creating Cluster: %s", err)
	}

	// Store the ID now
	id, err := tpgresource.ReplaceVars(d, config, "projects/{{project}}/locations/{{location}}/clusters/{{cluster_id}}")
	if err != nil {
		return fmt.Errorf("Error constructing id: %s", err)
	}
	d.SetId(id)

	err = HypercomputeclusterOperationWaitTime(
		config, res, project, "Creating Cluster", userAgent,
		d.Timeout(schema.TimeoutCreate))

	if err != nil {
		// The resource didn't actually create
		d.SetId("")
		return fmt.Errorf("Error waiting to create Cluster: %s", err)
	}

	log.Printf("[DEBUG] Finished creating Cluster %q: %#v", d.Id(), res)

	return resourceHypercomputeclusterClusterRead(d, meta)
}

func resourceHypercomputeclusterClusterRead(d *schema.ResourceData, meta interface{}) error {
	config := meta.(*transport_tpg.Config)
	userAgent, err := tpgresource.GenerateUserAgentString(d, config.UserAgent)
	if err != nil {
		return err
	}

	url, err := tpgresource.ReplaceVars(d, config, "{{HypercomputeclusterBasePath}}projects/{{project}}/locations/{{location}}/clusters/{{cluster_id}}")
	if err != nil {
		return err
	}

	billingProject := ""

	project, err := tpgresource.GetProject(d, config)
	if err != nil {
		return fmt.Errorf("Error fetching project for Cluster: %s", err)
	}
	billingProject = project

	// err == nil indicates that the billing_project value was found
	if bp, err := tpgresource.GetBillingProject(d, config); err == nil {
		billingProject = bp
	}

	headers := make(http.Header)
	res, err := transport_tpg.SendRequest(transport_tpg.SendRequestOptions{
		Config:    config,
		Method:    "GET",
		Project:   billingProject,
		RawURL:    url,
		UserAgent: userAgent,
		Headers:   headers,
	})
	if err != nil {
		return transport_tpg.HandleNotFoundError(err, d, fmt.Sprintf("HypercomputeclusterCluster %q", d.Id()))
	}

	if err := d.Set("project", project); err != nil {
		return fmt.Errorf("Error reading Cluster: %s", err)
	}

	if err := d.Set("compute_resources", flattenHypercomputeclusterClusterComputeResources(res["computeResources"], d, config)); err != nil {
		return fmt.Errorf("Error reading Cluster: %s", err)
	}
	if err := d.Set("create_time", flattenHypercomputeclusterClusterCreateTime(res["createTime"], d, config)); err != nil {
		return fmt.Errorf("Error reading Cluster: %s", err)
	}
	if err := d.Set("description", flattenHypercomputeclusterClusterDescription(res["description"], d, config)); err != nil {
		return fmt.Errorf("Error reading Cluster: %s", err)
	}
	if err := d.Set("labels", flattenHypercomputeclusterClusterLabels(res["labels"], d, config)); err != nil {
		return fmt.Errorf("Error reading Cluster: %s", err)
	}
	if err := d.Set("name", flattenHypercomputeclusterClusterName(res["name"], d, config)); err != nil {
		return fmt.Errorf("Error reading Cluster: %s", err)
	}
	if err := d.Set("network_resources", flattenHypercomputeclusterClusterNetworkResources(res["networkResources"], d, config)); err != nil {
		return fmt.Errorf("Error reading Cluster: %s", err)
	}
	if err := d.Set("orchestrator", flattenHypercomputeclusterClusterOrchestrator(res["orchestrator"], d, config)); err != nil {
		return fmt.Errorf("Error reading Cluster: %s", err)
	}
	if err := d.Set("reconciling", flattenHypercomputeclusterClusterReconciling(res["reconciling"], d, config)); err != nil {
		return fmt.Errorf("Error reading Cluster: %s", err)
	}
	if err := d.Set("storage_resources", flattenHypercomputeclusterClusterStorageResources(res["storageResources"], d, config)); err != nil {
		return fmt.Errorf("Error reading Cluster: %s", err)
	}
	if err := d.Set("update_time", flattenHypercomputeclusterClusterUpdateTime(res["updateTime"], d, config)); err != nil {
		return fmt.Errorf("Error reading Cluster: %s", err)
	}
	if err := d.Set("terraform_labels", flattenHypercomputeclusterClusterTerraformLabels(res["labels"], d, config)); err != nil {
		return fmt.Errorf("Error reading Cluster: %s", err)
	}
	if err := d.Set("effective_labels", flattenHypercomputeclusterClusterEffectiveLabels(res["labels"], d, config)); err != nil {
		return fmt.Errorf("Error reading Cluster: %s", err)
	}

	return nil
}

func resourceHypercomputeclusterClusterUpdate(d *schema.ResourceData, meta interface{}) error {
	config := meta.(*transport_tpg.Config)
	userAgent, err := tpgresource.GenerateUserAgentString(d, config.UserAgent)
	if err != nil {
		return err
	}

	billingProject := ""

	project, err := tpgresource.GetProject(d, config)
	if err != nil {
		return fmt.Errorf("Error fetching project for Cluster: %s", err)
	}
	billingProject = project

	obj := make(map[string]interface{})
	computeResourcesProp, err := expandHypercomputeclusterClusterComputeResources(d.Get("compute_resources"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("compute_resources"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, computeResourcesProp)) {
		obj["computeResources"] = computeResourcesProp
	}
	descriptionProp, err := expandHypercomputeclusterClusterDescription(d.Get("description"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("description"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, descriptionProp)) {
		obj["description"] = descriptionProp
	}
	networkResourcesProp, err := expandHypercomputeclusterClusterNetworkResources(d.Get("network_resources"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("network_resources"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, networkResourcesProp)) {
		obj["networkResources"] = networkResourcesProp
	}
	orchestratorProp, err := expandHypercomputeclusterClusterOrchestrator(d.Get("orchestrator"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("orchestrator"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, orchestratorProp)) {
		obj["orchestrator"] = orchestratorProp
	}
	storageResourcesProp, err := expandHypercomputeclusterClusterStorageResources(d.Get("storage_resources"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("storage_resources"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, storageResourcesProp)) {
		obj["storageResources"] = storageResourcesProp
	}
	effectiveLabelsProp, err := expandHypercomputeclusterClusterEffectiveLabels(d.Get("effective_labels"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("effective_labels"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, effectiveLabelsProp)) {
		obj["labels"] = effectiveLabelsProp
	}

	url, err := tpgresource.ReplaceVars(d, config, "{{HypercomputeclusterBasePath}}projects/{{project}}/locations/{{location}}/clusters/{{cluster_id}}")
	if err != nil {
		return err
	}

	log.Printf("[DEBUG] Updating Cluster %q: %#v", d.Id(), obj)
	headers := make(http.Header)
	updateMask := []string{}

	if d.HasChange("compute_resources") {
		updateMask = append(updateMask, "computeResources")
	}

	if d.HasChange("description") {
		updateMask = append(updateMask, "description")
	}

	if d.HasChange("network_resources") {
		updateMask = append(updateMask, "networkResources")
	}

	if d.HasChange("orchestrator") {
		updateMask = append(updateMask, "orchestrator")
	}

	if d.HasChange("storage_resources") {
		updateMask = append(updateMask, "storageResources")
	}

	if d.HasChange("effective_labels") {
		updateMask = append(updateMask, "labels")
	}
	// updateMask is a URL parameter but not present in the schema, so ReplaceVars
	// won't set it
	url, err = transport_tpg.AddQueryParams(url, map[string]string{"updateMask": strings.Join(updateMask, ",")})
	if err != nil {
		return err
	}

	// err == nil indicates that the billing_project value was found
	if bp, err := tpgresource.GetBillingProject(d, config); err == nil {
		billingProject = bp
	}

	// if updateMask is empty we are not updating anything so skip the post
	if len(updateMask) > 0 {
		res, err := transport_tpg.SendRequest(transport_tpg.SendRequestOptions{
			Config:    config,
			Method:    "PATCH",
			Project:   billingProject,
			RawURL:    url,
			UserAgent: userAgent,
			Body:      obj,
			Timeout:   d.Timeout(schema.TimeoutUpdate),
			Headers:   headers,
		})

		if err != nil {
			return fmt.Errorf("Error updating Cluster %q: %s", d.Id(), err)
		} else {
			log.Printf("[DEBUG] Finished updating Cluster %q: %#v", d.Id(), res)
		}

		err = HypercomputeclusterOperationWaitTime(
			config, res, project, "Updating Cluster", userAgent,
			d.Timeout(schema.TimeoutUpdate))

		if err != nil {
			return err
		}
	}

	return resourceHypercomputeclusterClusterRead(d, meta)
}

func resourceHypercomputeclusterClusterDelete(d *schema.ResourceData, meta interface{}) error {
	config := meta.(*transport_tpg.Config)
	userAgent, err := tpgresource.GenerateUserAgentString(d, config.UserAgent)
	if err != nil {
		return err
	}

	billingProject := ""

	project, err := tpgresource.GetProject(d, config)
	if err != nil {
		return fmt.Errorf("Error fetching project for Cluster: %s", err)
	}
	billingProject = project

	url, err := tpgresource.ReplaceVars(d, config, "{{HypercomputeclusterBasePath}}projects/{{project}}/locations/{{location}}/clusters/{{cluster_id}}")
	if err != nil {
		return err
	}

	var obj map[string]interface{}

	// err == nil indicates that the billing_project value was found
	if bp, err := tpgresource.GetBillingProject(d, config); err == nil {
		billingProject = bp
	}

	headers := make(http.Header)

	log.Printf("[DEBUG] Deleting Cluster %q", d.Id())
	res, err := transport_tpg.SendRequest(transport_tpg.SendRequestOptions{
		Config:    config,
		Method:    "DELETE",
		Project:   billingProject,
		RawURL:    url,
		UserAgent: userAgent,
		Body:      obj,
		Timeout:   d.Timeout(schema.TimeoutDelete),
		Headers:   headers,
	})
	if err != nil {
		return transport_tpg.HandleNotFoundError(err, d, "Cluster")
	}

	err = HypercomputeclusterOperationWaitTime(
		config, res, project, "Deleting Cluster", userAgent,
		d.Timeout(schema.TimeoutDelete))

	if err != nil {
		return err
	}

	log.Printf("[DEBUG] Finished deleting Cluster %q: %#v", d.Id(), res)
	return nil
}

func resourceHypercomputeclusterClusterImport(d *schema.ResourceData, meta interface{}) ([]*schema.ResourceData, error) {
	config := meta.(*transport_tpg.Config)
	if err := tpgresource.ParseImportId([]string{
		"^projects/(?P<project>[^/]+)/locations/(?P<location>[^/]+)/clusters/(?P<cluster_id>[^/]+)$",
		"^(?P<project>[^/]+)/(?P<location>[^/]+)/(?P<cluster_id>[^/]+)$",
		"^(?P<location>[^/]+)/(?P<cluster_id>[^/]+)$",
	}, d, config); err != nil {
		return nil, err
	}

	// Replace import id for the resource id
	id, err := tpgresource.ReplaceVars(d, config, "projects/{{project}}/locations/{{location}}/clusters/{{cluster_id}}")
	if err != nil {
		return nil, fmt.Errorf("Error constructing id: %s", err)
	}
	d.SetId(id)

	return []*schema.ResourceData{d}, nil
}

func flattenHypercomputeclusterClusterComputeResources(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}
	l := v.(map[string]interface{})
	transformed := make([]interface{}, 0, len(l))
	for k, raw := range l {
		original := raw.(map[string]interface{})
		transformed = append(transformed, map[string]interface{}{
			"id":     k,
			"config": flattenHypercomputeclusterClusterComputeResourcesConfig(original["config"], d, config),
		})
	}
	return transformed
}
func flattenHypercomputeclusterClusterComputeResourcesConfig(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["new_flex_start_instances"] =
		flattenHypercomputeclusterClusterComputeResourcesConfigNewFlexStartInstances(original["newFlexStartInstances"], d, config)
	transformed["new_on_demand_instances"] =
		flattenHypercomputeclusterClusterComputeResourcesConfigNewOnDemandInstances(original["newOnDemandInstances"], d, config)
	transformed["new_reserved_instances"] =
		flattenHypercomputeclusterClusterComputeResourcesConfigNewReservedInstances(original["newReservedInstances"], d, config)
	transformed["new_spot_instances"] =
		flattenHypercomputeclusterClusterComputeResourcesConfigNewSpotInstances(original["newSpotInstances"], d, config)
	return []interface{}{transformed}
}
func flattenHypercomputeclusterClusterComputeResourcesConfigNewFlexStartInstances(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["machine_type"] =
		flattenHypercomputeclusterClusterComputeResourcesConfigNewFlexStartInstancesMachineType(original["machineType"], d, config)
	transformed["max_duration"] =
		flattenHypercomputeclusterClusterComputeResourcesConfigNewFlexStartInstancesMaxDuration(original["maxDuration"], d, config)
	transformed["zone"] =
		flattenHypercomputeclusterClusterComputeResourcesConfigNewFlexStartInstancesZone(original["zone"], d, config)
	return []interface{}{transformed}
}
func flattenHypercomputeclusterClusterComputeResourcesConfigNewFlexStartInstancesMachineType(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterComputeResourcesConfigNewFlexStartInstancesMaxDuration(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterComputeResourcesConfigNewFlexStartInstancesZone(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterComputeResourcesConfigNewOnDemandInstances(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["machine_type"] =
		flattenHypercomputeclusterClusterComputeResourcesConfigNewOnDemandInstancesMachineType(original["machineType"], d, config)
	transformed["zone"] =
		flattenHypercomputeclusterClusterComputeResourcesConfigNewOnDemandInstancesZone(original["zone"], d, config)
	return []interface{}{transformed}
}
func flattenHypercomputeclusterClusterComputeResourcesConfigNewOnDemandInstancesMachineType(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterComputeResourcesConfigNewOnDemandInstancesZone(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterComputeResourcesConfigNewReservedInstances(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["reservation"] =
		flattenHypercomputeclusterClusterComputeResourcesConfigNewReservedInstancesReservation(original["reservation"], d, config)
	return []interface{}{transformed}
}
func flattenHypercomputeclusterClusterComputeResourcesConfigNewReservedInstancesReservation(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterComputeResourcesConfigNewSpotInstances(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["machine_type"] =
		flattenHypercomputeclusterClusterComputeResourcesConfigNewSpotInstancesMachineType(original["machineType"], d, config)
	transformed["termination_action"] =
		flattenHypercomputeclusterClusterComputeResourcesConfigNewSpotInstancesTerminationAction(original["terminationAction"], d, config)
	transformed["zone"] =
		flattenHypercomputeclusterClusterComputeResourcesConfigNewSpotInstancesZone(original["zone"], d, config)
	return []interface{}{transformed}
}
func flattenHypercomputeclusterClusterComputeResourcesConfigNewSpotInstancesMachineType(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterComputeResourcesConfigNewSpotInstancesTerminationAction(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterComputeResourcesConfigNewSpotInstancesZone(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterCreateTime(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterDescription(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterLabels(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}

	transformed := make(map[string]interface{})
	if l, ok := d.GetOkExists("labels"); ok {
		for k := range l.(map[string]interface{}) {
			transformed[k] = v.(map[string]interface{})[k]
		}
	}

	return transformed
}

func flattenHypercomputeclusterClusterName(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterNetworkResources(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}
	l := v.(map[string]interface{})
	transformed := make([]interface{}, 0, len(l))
	for k, raw := range l {
		original := raw.(map[string]interface{})
		transformed = append(transformed, map[string]interface{}{
			"id":      k,
			"config":  flattenHypercomputeclusterClusterNetworkResourcesConfig(original["config"], d, config),
			"network": flattenHypercomputeclusterClusterNetworkResourcesNetwork(original["network"], d, config),
		})
	}
	return transformed
}
func flattenHypercomputeclusterClusterNetworkResourcesConfig(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["existing_network"] =
		flattenHypercomputeclusterClusterNetworkResourcesConfigExistingNetwork(original["existingNetwork"], d, config)
	transformed["new_network"] =
		flattenHypercomputeclusterClusterNetworkResourcesConfigNewNetwork(original["newNetwork"], d, config)
	return []interface{}{transformed}
}
func flattenHypercomputeclusterClusterNetworkResourcesConfigExistingNetwork(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["network"] =
		flattenHypercomputeclusterClusterNetworkResourcesConfigExistingNetworkNetwork(original["network"], d, config)
	transformed["subnetwork"] =
		flattenHypercomputeclusterClusterNetworkResourcesConfigExistingNetworkSubnetwork(original["subnetwork"], d, config)
	return []interface{}{transformed}
}
func flattenHypercomputeclusterClusterNetworkResourcesConfigExistingNetworkNetwork(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterNetworkResourcesConfigExistingNetworkSubnetwork(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterNetworkResourcesConfigNewNetwork(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["description"] =
		flattenHypercomputeclusterClusterNetworkResourcesConfigNewNetworkDescription(original["description"], d, config)
	transformed["network"] =
		flattenHypercomputeclusterClusterNetworkResourcesConfigNewNetworkNetwork(original["network"], d, config)
	return []interface{}{transformed}
}
func flattenHypercomputeclusterClusterNetworkResourcesConfigNewNetworkDescription(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterNetworkResourcesConfigNewNetworkNetwork(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterNetworkResourcesNetwork(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["network"] =
		flattenHypercomputeclusterClusterNetworkResourcesNetworkNetwork(original["network"], d, config)
	transformed["subnetwork"] =
		flattenHypercomputeclusterClusterNetworkResourcesNetworkSubnetwork(original["subnetwork"], d, config)
	return []interface{}{transformed}
}
func flattenHypercomputeclusterClusterNetworkResourcesNetworkNetwork(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterNetworkResourcesNetworkSubnetwork(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterOrchestrator(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["slurm"] =
		flattenHypercomputeclusterClusterOrchestratorSlurm(original["slurm"], d, config)
	return []interface{}{transformed}
}
func flattenHypercomputeclusterClusterOrchestratorSlurm(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["default_partition"] =
		flattenHypercomputeclusterClusterOrchestratorSlurmDefaultPartition(original["defaultPartition"], d, config)
	transformed["epilog_bash_scripts"] =
		flattenHypercomputeclusterClusterOrchestratorSlurmEpilogBashScripts(original["epilogBashScripts"], d, config)
	transformed["login_nodes"] =
		flattenHypercomputeclusterClusterOrchestratorSlurmLoginNodes(original["loginNodes"], d, config)
	transformed["node_sets"] =
		flattenHypercomputeclusterClusterOrchestratorSlurmNodeSets(original["nodeSets"], d, config)
	transformed["partitions"] =
		flattenHypercomputeclusterClusterOrchestratorSlurmPartitions(original["partitions"], d, config)
	transformed["prolog_bash_scripts"] =
		flattenHypercomputeclusterClusterOrchestratorSlurmPrologBashScripts(original["prologBashScripts"], d, config)
	return []interface{}{transformed}
}
func flattenHypercomputeclusterClusterOrchestratorSlurmDefaultPartition(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterOrchestratorSlurmEpilogBashScripts(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterOrchestratorSlurmLoginNodes(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["boot_disk"] =
		flattenHypercomputeclusterClusterOrchestratorSlurmLoginNodesBootDisk(original["bootDisk"], d, config)
	transformed["count"] =
		flattenHypercomputeclusterClusterOrchestratorSlurmLoginNodesCount(original["count"], d, config)
	transformed["enable_os_login"] =
		flattenHypercomputeclusterClusterOrchestratorSlurmLoginNodesEnableOsLogin(original["enableOsLogin"], d, config)
	transformed["enable_public_ips"] =
		flattenHypercomputeclusterClusterOrchestratorSlurmLoginNodesEnablePublicIps(original["enablePublicIps"], d, config)
	transformed["instances"] =
		flattenHypercomputeclusterClusterOrchestratorSlurmLoginNodesInstances(original["instances"], d, config)
	transformed["labels"] =
		flattenHypercomputeclusterClusterOrchestratorSlurmLoginNodesLabels(original["labels"], d, config)
	transformed["machine_type"] =
		flattenHypercomputeclusterClusterOrchestratorSlurmLoginNodesMachineType(original["machineType"], d, config)
	transformed["startup_script"] =
		flattenHypercomputeclusterClusterOrchestratorSlurmLoginNodesStartupScript(original["startupScript"], d, config)
	transformed["storage_configs"] =
		flattenHypercomputeclusterClusterOrchestratorSlurmLoginNodesStorageConfigs(original["storageConfigs"], d, config)
	transformed["zone"] =
		flattenHypercomputeclusterClusterOrchestratorSlurmLoginNodesZone(original["zone"], d, config)
	return []interface{}{transformed}
}
func flattenHypercomputeclusterClusterOrchestratorSlurmLoginNodesBootDisk(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["size_gb"] =
		flattenHypercomputeclusterClusterOrchestratorSlurmLoginNodesBootDiskSizeGb(original["sizeGb"], d, config)
	transformed["type"] =
		flattenHypercomputeclusterClusterOrchestratorSlurmLoginNodesBootDiskType(original["type"], d, config)
	return []interface{}{transformed}
}
func flattenHypercomputeclusterClusterOrchestratorSlurmLoginNodesBootDiskSizeGb(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterOrchestratorSlurmLoginNodesBootDiskType(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterOrchestratorSlurmLoginNodesCount(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterOrchestratorSlurmLoginNodesEnableOsLogin(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterOrchestratorSlurmLoginNodesEnablePublicIps(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterOrchestratorSlurmLoginNodesInstances(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}
	l := v.([]interface{})
	transformed := make([]interface{}, 0, len(l))
	for _, raw := range l {
		original := raw.(map[string]interface{})
		if len(original) < 1 {
			// Do not include empty json objects coming back from the api
			continue
		}
		transformed = append(transformed, map[string]interface{}{
			"instance": flattenHypercomputeclusterClusterOrchestratorSlurmLoginNodesInstancesInstance(original["instance"], d, config),
		})
	}
	return transformed
}
func flattenHypercomputeclusterClusterOrchestratorSlurmLoginNodesInstancesInstance(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterOrchestratorSlurmLoginNodesLabels(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterOrchestratorSlurmLoginNodesMachineType(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterOrchestratorSlurmLoginNodesStartupScript(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterOrchestratorSlurmLoginNodesStorageConfigs(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}
	l := v.([]interface{})
	transformed := make([]interface{}, 0, len(l))
	for _, raw := range l {
		original := raw.(map[string]interface{})
		if len(original) < 1 {
			// Do not include empty json objects coming back from the api
			continue
		}
		transformed = append(transformed, map[string]interface{}{
			"id":          flattenHypercomputeclusterClusterOrchestratorSlurmLoginNodesStorageConfigsId(original["id"], d, config),
			"local_mount": flattenHypercomputeclusterClusterOrchestratorSlurmLoginNodesStorageConfigsLocalMount(original["localMount"], d, config),
		})
	}
	return transformed
}
func flattenHypercomputeclusterClusterOrchestratorSlurmLoginNodesStorageConfigsId(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterOrchestratorSlurmLoginNodesStorageConfigsLocalMount(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterOrchestratorSlurmLoginNodesZone(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterOrchestratorSlurmNodeSets(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}
	l := v.([]interface{})
	transformed := make([]interface{}, 0, len(l))
	for _, raw := range l {
		original := raw.(map[string]interface{})
		if len(original) < 1 {
			// Do not include empty json objects coming back from the api
			continue
		}
		transformed = append(transformed, map[string]interface{}{
			"compute_id":             flattenHypercomputeclusterClusterOrchestratorSlurmNodeSetsComputeId(original["computeId"], d, config),
			"compute_instance":       flattenHypercomputeclusterClusterOrchestratorSlurmNodeSetsComputeInstance(original["computeInstance"], d, config),
			"id":                     flattenHypercomputeclusterClusterOrchestratorSlurmNodeSetsId(original["id"], d, config),
			"max_dynamic_node_count": flattenHypercomputeclusterClusterOrchestratorSlurmNodeSetsMaxDynamicNodeCount(original["maxDynamicNodeCount"], d, config),
			"static_node_count":      flattenHypercomputeclusterClusterOrchestratorSlurmNodeSetsStaticNodeCount(original["staticNodeCount"], d, config),
			"storage_configs":        flattenHypercomputeclusterClusterOrchestratorSlurmNodeSetsStorageConfigs(original["storageConfigs"], d, config),
		})
	}
	return transformed
}
func flattenHypercomputeclusterClusterOrchestratorSlurmNodeSetsComputeId(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterOrchestratorSlurmNodeSetsComputeInstance(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["boot_disk"] =
		flattenHypercomputeclusterClusterOrchestratorSlurmNodeSetsComputeInstanceBootDisk(original["bootDisk"], d, config)
	transformed["labels"] =
		flattenHypercomputeclusterClusterOrchestratorSlurmNodeSetsComputeInstanceLabels(original["labels"], d, config)
	transformed["startup_script"] =
		flattenHypercomputeclusterClusterOrchestratorSlurmNodeSetsComputeInstanceStartupScript(original["startupScript"], d, config)
	return []interface{}{transformed}
}
func flattenHypercomputeclusterClusterOrchestratorSlurmNodeSetsComputeInstanceBootDisk(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["size_gb"] =
		flattenHypercomputeclusterClusterOrchestratorSlurmNodeSetsComputeInstanceBootDiskSizeGb(original["sizeGb"], d, config)
	transformed["type"] =
		flattenHypercomputeclusterClusterOrchestratorSlurmNodeSetsComputeInstanceBootDiskType(original["type"], d, config)
	return []interface{}{transformed}
}
func flattenHypercomputeclusterClusterOrchestratorSlurmNodeSetsComputeInstanceBootDiskSizeGb(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterOrchestratorSlurmNodeSetsComputeInstanceBootDiskType(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterOrchestratorSlurmNodeSetsComputeInstanceLabels(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterOrchestratorSlurmNodeSetsComputeInstanceStartupScript(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterOrchestratorSlurmNodeSetsId(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterOrchestratorSlurmNodeSetsMaxDynamicNodeCount(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterOrchestratorSlurmNodeSetsStaticNodeCount(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterOrchestratorSlurmNodeSetsStorageConfigs(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}
	l := v.([]interface{})
	transformed := make([]interface{}, 0, len(l))
	for _, raw := range l {
		original := raw.(map[string]interface{})
		if len(original) < 1 {
			// Do not include empty json objects coming back from the api
			continue
		}
		transformed = append(transformed, map[string]interface{}{
			"id":          flattenHypercomputeclusterClusterOrchestratorSlurmNodeSetsStorageConfigsId(original["id"], d, config),
			"local_mount": flattenHypercomputeclusterClusterOrchestratorSlurmNodeSetsStorageConfigsLocalMount(original["localMount"], d, config),
		})
	}
	return transformed
}
func flattenHypercomputeclusterClusterOrchestratorSlurmNodeSetsStorageConfigsId(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterOrchestratorSlurmNodeSetsStorageConfigsLocalMount(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterOrchestratorSlurmPartitions(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}
	l := v.([]interface{})
	transformed := make([]interface{}, 0, len(l))
	for _, raw := range l {
		original := raw.(map[string]interface{})
		if len(original) < 1 {
			// Do not include empty json objects coming back from the api
			continue
		}
		transformed = append(transformed, map[string]interface{}{
			"id":           flattenHypercomputeclusterClusterOrchestratorSlurmPartitionsId(original["id"], d, config),
			"node_set_ids": flattenHypercomputeclusterClusterOrchestratorSlurmPartitionsNodeSetIds(original["nodeSetIds"], d, config),
		})
	}
	return transformed
}
func flattenHypercomputeclusterClusterOrchestratorSlurmPartitionsId(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterOrchestratorSlurmPartitionsNodeSetIds(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterOrchestratorSlurmPrologBashScripts(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterReconciling(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterStorageResources(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}
	l := v.(map[string]interface{})
	transformed := make([]interface{}, 0, len(l))
	for k, raw := range l {
		original := raw.(map[string]interface{})
		transformed = append(transformed, map[string]interface{}{
			"id":        k,
			"bucket":    flattenHypercomputeclusterClusterStorageResourcesBucket(original["bucket"], d, config),
			"config":    flattenHypercomputeclusterClusterStorageResourcesConfig(original["config"], d, config),
			"filestore": flattenHypercomputeclusterClusterStorageResourcesFilestore(original["filestore"], d, config),
			"lustre":    flattenHypercomputeclusterClusterStorageResourcesLustre(original["lustre"], d, config),
		})
	}
	return transformed
}
func flattenHypercomputeclusterClusterStorageResourcesBucket(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["bucket"] =
		flattenHypercomputeclusterClusterStorageResourcesBucketBucket(original["bucket"], d, config)
	return []interface{}{transformed}
}
func flattenHypercomputeclusterClusterStorageResourcesBucketBucket(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterStorageResourcesConfig(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["existing_bucket"] =
		flattenHypercomputeclusterClusterStorageResourcesConfigExistingBucket(original["existingBucket"], d, config)
	transformed["existing_filestore"] =
		flattenHypercomputeclusterClusterStorageResourcesConfigExistingFilestore(original["existingFilestore"], d, config)
	transformed["existing_lustre"] =
		flattenHypercomputeclusterClusterStorageResourcesConfigExistingLustre(original["existingLustre"], d, config)
	transformed["new_bucket"] =
		flattenHypercomputeclusterClusterStorageResourcesConfigNewBucket(original["newBucket"], d, config)
	transformed["new_filestore"] =
		flattenHypercomputeclusterClusterStorageResourcesConfigNewFilestore(original["newFilestore"], d, config)
	transformed["new_lustre"] =
		flattenHypercomputeclusterClusterStorageResourcesConfigNewLustre(original["newLustre"], d, config)
	return []interface{}{transformed}
}
func flattenHypercomputeclusterClusterStorageResourcesConfigExistingBucket(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["bucket"] =
		flattenHypercomputeclusterClusterStorageResourcesConfigExistingBucketBucket(original["bucket"], d, config)
	return []interface{}{transformed}
}
func flattenHypercomputeclusterClusterStorageResourcesConfigExistingBucketBucket(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterStorageResourcesConfigExistingFilestore(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["filestore"] =
		flattenHypercomputeclusterClusterStorageResourcesConfigExistingFilestoreFilestore(original["filestore"], d, config)
	return []interface{}{transformed}
}
func flattenHypercomputeclusterClusterStorageResourcesConfigExistingFilestoreFilestore(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterStorageResourcesConfigExistingLustre(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["lustre"] =
		flattenHypercomputeclusterClusterStorageResourcesConfigExistingLustreLustre(original["lustre"], d, config)
	return []interface{}{transformed}
}
func flattenHypercomputeclusterClusterStorageResourcesConfigExistingLustreLustre(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterStorageResourcesConfigNewBucket(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["autoclass"] =
		flattenHypercomputeclusterClusterStorageResourcesConfigNewBucketAutoclass(original["autoclass"], d, config)
	transformed["bucket"] =
		flattenHypercomputeclusterClusterStorageResourcesConfigNewBucketBucket(original["bucket"], d, config)
	transformed["hierarchical_namespace"] =
		flattenHypercomputeclusterClusterStorageResourcesConfigNewBucketHierarchicalNamespace(original["hierarchicalNamespace"], d, config)
	transformed["storage_class"] =
		flattenHypercomputeclusterClusterStorageResourcesConfigNewBucketStorageClass(original["storageClass"], d, config)
	return []interface{}{transformed}
}
func flattenHypercomputeclusterClusterStorageResourcesConfigNewBucketAutoclass(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["enabled"] =
		flattenHypercomputeclusterClusterStorageResourcesConfigNewBucketAutoclassEnabled(original["enabled"], d, config)
	return []interface{}{transformed}
}
func flattenHypercomputeclusterClusterStorageResourcesConfigNewBucketAutoclassEnabled(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterStorageResourcesConfigNewBucketBucket(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterStorageResourcesConfigNewBucketHierarchicalNamespace(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["enabled"] =
		flattenHypercomputeclusterClusterStorageResourcesConfigNewBucketHierarchicalNamespaceEnabled(original["enabled"], d, config)
	return []interface{}{transformed}
}
func flattenHypercomputeclusterClusterStorageResourcesConfigNewBucketHierarchicalNamespaceEnabled(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterStorageResourcesConfigNewBucketStorageClass(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterStorageResourcesConfigNewFilestore(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["description"] =
		flattenHypercomputeclusterClusterStorageResourcesConfigNewFilestoreDescription(original["description"], d, config)
	transformed["file_shares"] =
		flattenHypercomputeclusterClusterStorageResourcesConfigNewFilestoreFileShares(original["fileShares"], d, config)
	transformed["filestore"] =
		flattenHypercomputeclusterClusterStorageResourcesConfigNewFilestoreFilestore(original["filestore"], d, config)
	transformed["protocol"] =
		flattenHypercomputeclusterClusterStorageResourcesConfigNewFilestoreProtocol(original["protocol"], d, config)
	transformed["tier"] =
		flattenHypercomputeclusterClusterStorageResourcesConfigNewFilestoreTier(original["tier"], d, config)
	return []interface{}{transformed}
}
func flattenHypercomputeclusterClusterStorageResourcesConfigNewFilestoreDescription(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterStorageResourcesConfigNewFilestoreFileShares(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}
	l := v.([]interface{})
	transformed := make([]interface{}, 0, len(l))
	for _, raw := range l {
		original := raw.(map[string]interface{})
		if len(original) < 1 {
			// Do not include empty json objects coming back from the api
			continue
		}
		transformed = append(transformed, map[string]interface{}{
			"capacity_gb": flattenHypercomputeclusterClusterStorageResourcesConfigNewFilestoreFileSharesCapacityGb(original["capacityGb"], d, config),
			"file_share":  flattenHypercomputeclusterClusterStorageResourcesConfigNewFilestoreFileSharesFileShare(original["fileShare"], d, config),
		})
	}
	return transformed
}
func flattenHypercomputeclusterClusterStorageResourcesConfigNewFilestoreFileSharesCapacityGb(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterStorageResourcesConfigNewFilestoreFileSharesFileShare(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterStorageResourcesConfigNewFilestoreFilestore(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterStorageResourcesConfigNewFilestoreProtocol(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterStorageResourcesConfigNewFilestoreTier(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterStorageResourcesConfigNewLustre(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["capacity_gb"] =
		flattenHypercomputeclusterClusterStorageResourcesConfigNewLustreCapacityGb(original["capacityGb"], d, config)
	transformed["description"] =
		flattenHypercomputeclusterClusterStorageResourcesConfigNewLustreDescription(original["description"], d, config)
	transformed["filesystem"] =
		flattenHypercomputeclusterClusterStorageResourcesConfigNewLustreFilesystem(original["filesystem"], d, config)
	transformed["lustre"] =
		flattenHypercomputeclusterClusterStorageResourcesConfigNewLustreLustre(original["lustre"], d, config)
	return []interface{}{transformed}
}
func flattenHypercomputeclusterClusterStorageResourcesConfigNewLustreCapacityGb(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterStorageResourcesConfigNewLustreDescription(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterStorageResourcesConfigNewLustreFilesystem(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterStorageResourcesConfigNewLustreLustre(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterStorageResourcesFilestore(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["filestore"] =
		flattenHypercomputeclusterClusterStorageResourcesFilestoreFilestore(original["filestore"], d, config)
	return []interface{}{transformed}
}
func flattenHypercomputeclusterClusterStorageResourcesFilestoreFilestore(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterStorageResourcesLustre(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["lustre"] =
		flattenHypercomputeclusterClusterStorageResourcesLustreLustre(original["lustre"], d, config)
	return []interface{}{transformed}
}
func flattenHypercomputeclusterClusterStorageResourcesLustreLustre(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterUpdateTime(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenHypercomputeclusterClusterTerraformLabels(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}

	transformed := make(map[string]interface{})
	if l, ok := d.GetOkExists("terraform_labels"); ok {
		for k := range l.(map[string]interface{}) {
			transformed[k] = v.(map[string]interface{})[k]
		}
	}

	return transformed
}

func flattenHypercomputeclusterClusterEffectiveLabels(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func expandHypercomputeclusterClusterComputeResources(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (map[string]interface{}, error) {
	if v == nil {
		return map[string]interface{}{}, nil
	}
	m := make(map[string]interface{})
	for _, raw := range v.(*schema.Set).List() {
		original := raw.(map[string]interface{})
		transformed := make(map[string]interface{})

		transformedConfig, err := expandHypercomputeclusterClusterComputeResourcesConfig(original["config"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedConfig); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["config"] = transformedConfig
		}

		transformedId, err := tpgresource.ExpandString(original["id"], d, config)
		if err != nil {
			return nil, err
		}
		m[transformedId] = transformed
	}
	return m, nil
}

func expandHypercomputeclusterClusterComputeResourcesConfig(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	if v == nil {
		return nil, nil
	}
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedNewFlexStartInstances, err := expandHypercomputeclusterClusterComputeResourcesConfigNewFlexStartInstances(original["new_flex_start_instances"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedNewFlexStartInstances); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["newFlexStartInstances"] = transformedNewFlexStartInstances
	}

	transformedNewOnDemandInstances, err := expandHypercomputeclusterClusterComputeResourcesConfigNewOnDemandInstances(original["new_on_demand_instances"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedNewOnDemandInstances); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["newOnDemandInstances"] = transformedNewOnDemandInstances
	}

	transformedNewReservedInstances, err := expandHypercomputeclusterClusterComputeResourcesConfigNewReservedInstances(original["new_reserved_instances"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedNewReservedInstances); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["newReservedInstances"] = transformedNewReservedInstances
	}

	transformedNewSpotInstances, err := expandHypercomputeclusterClusterComputeResourcesConfigNewSpotInstances(original["new_spot_instances"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedNewSpotInstances); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["newSpotInstances"] = transformedNewSpotInstances
	}

	return transformed, nil
}

func expandHypercomputeclusterClusterComputeResourcesConfigNewFlexStartInstances(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	if v == nil {
		return nil, nil
	}
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedMachineType, err := expandHypercomputeclusterClusterComputeResourcesConfigNewFlexStartInstancesMachineType(original["machine_type"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedMachineType); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["machineType"] = transformedMachineType
	}

	transformedMaxDuration, err := expandHypercomputeclusterClusterComputeResourcesConfigNewFlexStartInstancesMaxDuration(original["max_duration"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedMaxDuration); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["maxDuration"] = transformedMaxDuration
	}

	transformedZone, err := expandHypercomputeclusterClusterComputeResourcesConfigNewFlexStartInstancesZone(original["zone"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedZone); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["zone"] = transformedZone
	}

	return transformed, nil
}

func expandHypercomputeclusterClusterComputeResourcesConfigNewFlexStartInstancesMachineType(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterComputeResourcesConfigNewFlexStartInstancesMaxDuration(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterComputeResourcesConfigNewFlexStartInstancesZone(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterComputeResourcesConfigNewOnDemandInstances(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	if v == nil {
		return nil, nil
	}
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedMachineType, err := expandHypercomputeclusterClusterComputeResourcesConfigNewOnDemandInstancesMachineType(original["machine_type"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedMachineType); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["machineType"] = transformedMachineType
	}

	transformedZone, err := expandHypercomputeclusterClusterComputeResourcesConfigNewOnDemandInstancesZone(original["zone"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedZone); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["zone"] = transformedZone
	}

	return transformed, nil
}

func expandHypercomputeclusterClusterComputeResourcesConfigNewOnDemandInstancesMachineType(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterComputeResourcesConfigNewOnDemandInstancesZone(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterComputeResourcesConfigNewReservedInstances(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	if v == nil {
		return nil, nil
	}
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedReservation, err := expandHypercomputeclusterClusterComputeResourcesConfigNewReservedInstancesReservation(original["reservation"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedReservation); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["reservation"] = transformedReservation
	}

	return transformed, nil
}

func expandHypercomputeclusterClusterComputeResourcesConfigNewReservedInstancesReservation(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterComputeResourcesConfigNewSpotInstances(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	if v == nil {
		return nil, nil
	}
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedMachineType, err := expandHypercomputeclusterClusterComputeResourcesConfigNewSpotInstancesMachineType(original["machine_type"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedMachineType); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["machineType"] = transformedMachineType
	}

	transformedTerminationAction, err := expandHypercomputeclusterClusterComputeResourcesConfigNewSpotInstancesTerminationAction(original["termination_action"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedTerminationAction); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["terminationAction"] = transformedTerminationAction
	}

	transformedZone, err := expandHypercomputeclusterClusterComputeResourcesConfigNewSpotInstancesZone(original["zone"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedZone); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["zone"] = transformedZone
	}

	return transformed, nil
}

func expandHypercomputeclusterClusterComputeResourcesConfigNewSpotInstancesMachineType(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterComputeResourcesConfigNewSpotInstancesTerminationAction(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterComputeResourcesConfigNewSpotInstancesZone(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterDescription(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterNetworkResources(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (map[string]interface{}, error) {
	if v == nil {
		return map[string]interface{}{}, nil
	}
	m := make(map[string]interface{})
	for _, raw := range v.(*schema.Set).List() {
		original := raw.(map[string]interface{})
		transformed := make(map[string]interface{})

		transformedConfig, err := expandHypercomputeclusterClusterNetworkResourcesConfig(original["config"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedConfig); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["config"] = transformedConfig
		}

		transformedNetwork, err := expandHypercomputeclusterClusterNetworkResourcesNetwork(original["network"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedNetwork); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["network"] = transformedNetwork
		}

		transformedId, err := tpgresource.ExpandString(original["id"], d, config)
		if err != nil {
			return nil, err
		}
		m[transformedId] = transformed
	}
	return m, nil
}

func expandHypercomputeclusterClusterNetworkResourcesConfig(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	if v == nil {
		return nil, nil
	}
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedExistingNetwork, err := expandHypercomputeclusterClusterNetworkResourcesConfigExistingNetwork(original["existing_network"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedExistingNetwork); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["existingNetwork"] = transformedExistingNetwork
	}

	transformedNewNetwork, err := expandHypercomputeclusterClusterNetworkResourcesConfigNewNetwork(original["new_network"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedNewNetwork); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["newNetwork"] = transformedNewNetwork
	}

	return transformed, nil
}

func expandHypercomputeclusterClusterNetworkResourcesConfigExistingNetwork(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	if v == nil {
		return nil, nil
	}
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedNetwork, err := expandHypercomputeclusterClusterNetworkResourcesConfigExistingNetworkNetwork(original["network"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedNetwork); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["network"] = transformedNetwork
	}

	transformedSubnetwork, err := expandHypercomputeclusterClusterNetworkResourcesConfigExistingNetworkSubnetwork(original["subnetwork"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedSubnetwork); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["subnetwork"] = transformedSubnetwork
	}

	return transformed, nil
}

func expandHypercomputeclusterClusterNetworkResourcesConfigExistingNetworkNetwork(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterNetworkResourcesConfigExistingNetworkSubnetwork(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterNetworkResourcesConfigNewNetwork(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	if v == nil {
		return nil, nil
	}
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedDescription, err := expandHypercomputeclusterClusterNetworkResourcesConfigNewNetworkDescription(original["description"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedDescription); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["description"] = transformedDescription
	}

	transformedNetwork, err := expandHypercomputeclusterClusterNetworkResourcesConfigNewNetworkNetwork(original["network"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedNetwork); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["network"] = transformedNetwork
	}

	return transformed, nil
}

func expandHypercomputeclusterClusterNetworkResourcesConfigNewNetworkDescription(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterNetworkResourcesConfigNewNetworkNetwork(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterNetworkResourcesNetwork(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	if v == nil {
		return nil, nil
	}
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedNetwork, err := expandHypercomputeclusterClusterNetworkResourcesNetworkNetwork(original["network"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedNetwork); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["network"] = transformedNetwork
	}

	transformedSubnetwork, err := expandHypercomputeclusterClusterNetworkResourcesNetworkSubnetwork(original["subnetwork"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedSubnetwork); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["subnetwork"] = transformedSubnetwork
	}

	return transformed, nil
}

func expandHypercomputeclusterClusterNetworkResourcesNetworkNetwork(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterNetworkResourcesNetworkSubnetwork(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterOrchestrator(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	if v == nil {
		return nil, nil
	}
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedSlurm, err := expandHypercomputeclusterClusterOrchestratorSlurm(original["slurm"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedSlurm); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["slurm"] = transformedSlurm
	}

	return transformed, nil
}

func expandHypercomputeclusterClusterOrchestratorSlurm(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	if v == nil {
		return nil, nil
	}
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedDefaultPartition, err := expandHypercomputeclusterClusterOrchestratorSlurmDefaultPartition(original["default_partition"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedDefaultPartition); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["defaultPartition"] = transformedDefaultPartition
	}

	transformedEpilogBashScripts, err := expandHypercomputeclusterClusterOrchestratorSlurmEpilogBashScripts(original["epilog_bash_scripts"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedEpilogBashScripts); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["epilogBashScripts"] = transformedEpilogBashScripts
	}

	transformedLoginNodes, err := expandHypercomputeclusterClusterOrchestratorSlurmLoginNodes(original["login_nodes"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedLoginNodes); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["loginNodes"] = transformedLoginNodes
	}

	transformedNodeSets, err := expandHypercomputeclusterClusterOrchestratorSlurmNodeSets(original["node_sets"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedNodeSets); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["nodeSets"] = transformedNodeSets
	}

	transformedPartitions, err := expandHypercomputeclusterClusterOrchestratorSlurmPartitions(original["partitions"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedPartitions); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["partitions"] = transformedPartitions
	}

	transformedPrologBashScripts, err := expandHypercomputeclusterClusterOrchestratorSlurmPrologBashScripts(original["prolog_bash_scripts"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedPrologBashScripts); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["prologBashScripts"] = transformedPrologBashScripts
	}

	return transformed, nil
}

func expandHypercomputeclusterClusterOrchestratorSlurmDefaultPartition(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterOrchestratorSlurmEpilogBashScripts(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterOrchestratorSlurmLoginNodes(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	if v == nil {
		return nil, nil
	}
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedBootDisk, err := expandHypercomputeclusterClusterOrchestratorSlurmLoginNodesBootDisk(original["boot_disk"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedBootDisk); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["bootDisk"] = transformedBootDisk
	}

	transformedCount, err := expandHypercomputeclusterClusterOrchestratorSlurmLoginNodesCount(original["count"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedCount); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["count"] = transformedCount
	}

	transformedEnableOsLogin, err := expandHypercomputeclusterClusterOrchestratorSlurmLoginNodesEnableOsLogin(original["enable_os_login"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedEnableOsLogin); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["enableOsLogin"] = transformedEnableOsLogin
	}

	transformedEnablePublicIps, err := expandHypercomputeclusterClusterOrchestratorSlurmLoginNodesEnablePublicIps(original["enable_public_ips"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedEnablePublicIps); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["enablePublicIps"] = transformedEnablePublicIps
	}

	transformedInstances, err := expandHypercomputeclusterClusterOrchestratorSlurmLoginNodesInstances(original["instances"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedInstances); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["instances"] = transformedInstances
	}

	transformedLabels, err := expandHypercomputeclusterClusterOrchestratorSlurmLoginNodesLabels(original["labels"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedLabels); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["labels"] = transformedLabels
	}

	transformedMachineType, err := expandHypercomputeclusterClusterOrchestratorSlurmLoginNodesMachineType(original["machine_type"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedMachineType); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["machineType"] = transformedMachineType
	}

	transformedStartupScript, err := expandHypercomputeclusterClusterOrchestratorSlurmLoginNodesStartupScript(original["startup_script"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedStartupScript); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["startupScript"] = transformedStartupScript
	}

	transformedStorageConfigs, err := expandHypercomputeclusterClusterOrchestratorSlurmLoginNodesStorageConfigs(original["storage_configs"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedStorageConfigs); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["storageConfigs"] = transformedStorageConfigs
	}

	transformedZone, err := expandHypercomputeclusterClusterOrchestratorSlurmLoginNodesZone(original["zone"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedZone); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["zone"] = transformedZone
	}

	return transformed, nil
}

func expandHypercomputeclusterClusterOrchestratorSlurmLoginNodesBootDisk(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	if v == nil {
		return nil, nil
	}
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedSizeGb, err := expandHypercomputeclusterClusterOrchestratorSlurmLoginNodesBootDiskSizeGb(original["size_gb"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedSizeGb); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["sizeGb"] = transformedSizeGb
	}

	transformedType, err := expandHypercomputeclusterClusterOrchestratorSlurmLoginNodesBootDiskType(original["type"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedType); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["type"] = transformedType
	}

	return transformed, nil
}

func expandHypercomputeclusterClusterOrchestratorSlurmLoginNodesBootDiskSizeGb(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterOrchestratorSlurmLoginNodesBootDiskType(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterOrchestratorSlurmLoginNodesCount(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterOrchestratorSlurmLoginNodesEnableOsLogin(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterOrchestratorSlurmLoginNodesEnablePublicIps(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterOrchestratorSlurmLoginNodesInstances(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	if v == nil {
		return nil, nil
	}
	l := v.([]interface{})
	req := make([]interface{}, 0, len(l))
	for _, raw := range l {
		if raw == nil {
			continue
		}
		original := raw.(map[string]interface{})
		transformed := make(map[string]interface{})

		transformedInstance, err := expandHypercomputeclusterClusterOrchestratorSlurmLoginNodesInstancesInstance(original["instance"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedInstance); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["instance"] = transformedInstance
		}

		req = append(req, transformed)
	}
	return req, nil
}

func expandHypercomputeclusterClusterOrchestratorSlurmLoginNodesInstancesInstance(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterOrchestratorSlurmLoginNodesLabels(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (map[string]string, error) {
	if v == nil {
		return map[string]string{}, nil
	}
	m := make(map[string]string)
	for k, val := range v.(map[string]interface{}) {
		m[k] = val.(string)
	}
	return m, nil
}

func expandHypercomputeclusterClusterOrchestratorSlurmLoginNodesMachineType(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterOrchestratorSlurmLoginNodesStartupScript(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterOrchestratorSlurmLoginNodesStorageConfigs(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	if v == nil {
		return nil, nil
	}
	l := v.([]interface{})
	req := make([]interface{}, 0, len(l))
	for _, raw := range l {
		if raw == nil {
			continue
		}
		original := raw.(map[string]interface{})
		transformed := make(map[string]interface{})

		transformedId, err := expandHypercomputeclusterClusterOrchestratorSlurmLoginNodesStorageConfigsId(original["id"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedId); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["id"] = transformedId
		}

		transformedLocalMount, err := expandHypercomputeclusterClusterOrchestratorSlurmLoginNodesStorageConfigsLocalMount(original["local_mount"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedLocalMount); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["localMount"] = transformedLocalMount
		}

		req = append(req, transformed)
	}
	return req, nil
}

func expandHypercomputeclusterClusterOrchestratorSlurmLoginNodesStorageConfigsId(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterOrchestratorSlurmLoginNodesStorageConfigsLocalMount(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterOrchestratorSlurmLoginNodesZone(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterOrchestratorSlurmNodeSets(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	if v == nil {
		return nil, nil
	}
	l := v.([]interface{})
	req := make([]interface{}, 0, len(l))
	for _, raw := range l {
		if raw == nil {
			continue
		}
		original := raw.(map[string]interface{})
		transformed := make(map[string]interface{})

		transformedComputeId, err := expandHypercomputeclusterClusterOrchestratorSlurmNodeSetsComputeId(original["compute_id"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedComputeId); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["computeId"] = transformedComputeId
		}

		transformedComputeInstance, err := expandHypercomputeclusterClusterOrchestratorSlurmNodeSetsComputeInstance(original["compute_instance"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedComputeInstance); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["computeInstance"] = transformedComputeInstance
		}

		transformedId, err := expandHypercomputeclusterClusterOrchestratorSlurmNodeSetsId(original["id"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedId); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["id"] = transformedId
		}

		transformedMaxDynamicNodeCount, err := expandHypercomputeclusterClusterOrchestratorSlurmNodeSetsMaxDynamicNodeCount(original["max_dynamic_node_count"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedMaxDynamicNodeCount); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["maxDynamicNodeCount"] = transformedMaxDynamicNodeCount
		}

		transformedStaticNodeCount, err := expandHypercomputeclusterClusterOrchestratorSlurmNodeSetsStaticNodeCount(original["static_node_count"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedStaticNodeCount); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["staticNodeCount"] = transformedStaticNodeCount
		}

		transformedStorageConfigs, err := expandHypercomputeclusterClusterOrchestratorSlurmNodeSetsStorageConfigs(original["storage_configs"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedStorageConfigs); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["storageConfigs"] = transformedStorageConfigs
		}

		req = append(req, transformed)
	}
	return req, nil
}

func expandHypercomputeclusterClusterOrchestratorSlurmNodeSetsComputeId(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterOrchestratorSlurmNodeSetsComputeInstance(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	if v == nil {
		return nil, nil
	}
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedBootDisk, err := expandHypercomputeclusterClusterOrchestratorSlurmNodeSetsComputeInstanceBootDisk(original["boot_disk"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedBootDisk); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["bootDisk"] = transformedBootDisk
	}

	transformedLabels, err := expandHypercomputeclusterClusterOrchestratorSlurmNodeSetsComputeInstanceLabels(original["labels"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedLabels); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["labels"] = transformedLabels
	}

	transformedStartupScript, err := expandHypercomputeclusterClusterOrchestratorSlurmNodeSetsComputeInstanceStartupScript(original["startup_script"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedStartupScript); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["startupScript"] = transformedStartupScript
	}

	return transformed, nil
}

func expandHypercomputeclusterClusterOrchestratorSlurmNodeSetsComputeInstanceBootDisk(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	if v == nil {
		return nil, nil
	}
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedSizeGb, err := expandHypercomputeclusterClusterOrchestratorSlurmNodeSetsComputeInstanceBootDiskSizeGb(original["size_gb"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedSizeGb); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["sizeGb"] = transformedSizeGb
	}

	transformedType, err := expandHypercomputeclusterClusterOrchestratorSlurmNodeSetsComputeInstanceBootDiskType(original["type"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedType); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["type"] = transformedType
	}

	return transformed, nil
}

func expandHypercomputeclusterClusterOrchestratorSlurmNodeSetsComputeInstanceBootDiskSizeGb(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterOrchestratorSlurmNodeSetsComputeInstanceBootDiskType(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterOrchestratorSlurmNodeSetsComputeInstanceLabels(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (map[string]string, error) {
	if v == nil {
		return map[string]string{}, nil
	}
	m := make(map[string]string)
	for k, val := range v.(map[string]interface{}) {
		m[k] = val.(string)
	}
	return m, nil
}

func expandHypercomputeclusterClusterOrchestratorSlurmNodeSetsComputeInstanceStartupScript(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterOrchestratorSlurmNodeSetsId(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterOrchestratorSlurmNodeSetsMaxDynamicNodeCount(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterOrchestratorSlurmNodeSetsStaticNodeCount(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterOrchestratorSlurmNodeSetsStorageConfigs(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	if v == nil {
		return nil, nil
	}
	l := v.([]interface{})
	req := make([]interface{}, 0, len(l))
	for _, raw := range l {
		if raw == nil {
			continue
		}
		original := raw.(map[string]interface{})
		transformed := make(map[string]interface{})

		transformedId, err := expandHypercomputeclusterClusterOrchestratorSlurmNodeSetsStorageConfigsId(original["id"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedId); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["id"] = transformedId
		}

		transformedLocalMount, err := expandHypercomputeclusterClusterOrchestratorSlurmNodeSetsStorageConfigsLocalMount(original["local_mount"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedLocalMount); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["localMount"] = transformedLocalMount
		}

		req = append(req, transformed)
	}
	return req, nil
}

func expandHypercomputeclusterClusterOrchestratorSlurmNodeSetsStorageConfigsId(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterOrchestratorSlurmNodeSetsStorageConfigsLocalMount(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterOrchestratorSlurmPartitions(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	if v == nil {
		return nil, nil
	}
	l := v.([]interface{})
	req := make([]interface{}, 0, len(l))
	for _, raw := range l {
		if raw == nil {
			continue
		}
		original := raw.(map[string]interface{})
		transformed := make(map[string]interface{})

		transformedId, err := expandHypercomputeclusterClusterOrchestratorSlurmPartitionsId(original["id"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedId); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["id"] = transformedId
		}

		transformedNodeSetIds, err := expandHypercomputeclusterClusterOrchestratorSlurmPartitionsNodeSetIds(original["node_set_ids"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedNodeSetIds); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["nodeSetIds"] = transformedNodeSetIds
		}

		req = append(req, transformed)
	}
	return req, nil
}

func expandHypercomputeclusterClusterOrchestratorSlurmPartitionsId(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterOrchestratorSlurmPartitionsNodeSetIds(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterOrchestratorSlurmPrologBashScripts(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterStorageResources(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (map[string]interface{}, error) {
	if v == nil {
		return map[string]interface{}{}, nil
	}
	m := make(map[string]interface{})
	for _, raw := range v.(*schema.Set).List() {
		original := raw.(map[string]interface{})
		transformed := make(map[string]interface{})

		transformedBucket, err := expandHypercomputeclusterClusterStorageResourcesBucket(original["bucket"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedBucket); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["bucket"] = transformedBucket
		}

		transformedConfig, err := expandHypercomputeclusterClusterStorageResourcesConfig(original["config"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedConfig); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["config"] = transformedConfig
		}

		transformedFilestore, err := expandHypercomputeclusterClusterStorageResourcesFilestore(original["filestore"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedFilestore); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["filestore"] = transformedFilestore
		}

		transformedLustre, err := expandHypercomputeclusterClusterStorageResourcesLustre(original["lustre"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedLustre); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["lustre"] = transformedLustre
		}

		transformedId, err := tpgresource.ExpandString(original["id"], d, config)
		if err != nil {
			return nil, err
		}
		m[transformedId] = transformed
	}
	return m, nil
}

func expandHypercomputeclusterClusterStorageResourcesBucket(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	if v == nil {
		return nil, nil
	}
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedBucket, err := expandHypercomputeclusterClusterStorageResourcesBucketBucket(original["bucket"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedBucket); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["bucket"] = transformedBucket
	}

	return transformed, nil
}

func expandHypercomputeclusterClusterStorageResourcesBucketBucket(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterStorageResourcesConfig(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	if v == nil {
		return nil, nil
	}
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedExistingBucket, err := expandHypercomputeclusterClusterStorageResourcesConfigExistingBucket(original["existing_bucket"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedExistingBucket); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["existingBucket"] = transformedExistingBucket
	}

	transformedExistingFilestore, err := expandHypercomputeclusterClusterStorageResourcesConfigExistingFilestore(original["existing_filestore"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedExistingFilestore); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["existingFilestore"] = transformedExistingFilestore
	}

	transformedExistingLustre, err := expandHypercomputeclusterClusterStorageResourcesConfigExistingLustre(original["existing_lustre"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedExistingLustre); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["existingLustre"] = transformedExistingLustre
	}

	transformedNewBucket, err := expandHypercomputeclusterClusterStorageResourcesConfigNewBucket(original["new_bucket"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedNewBucket); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["newBucket"] = transformedNewBucket
	}

	transformedNewFilestore, err := expandHypercomputeclusterClusterStorageResourcesConfigNewFilestore(original["new_filestore"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedNewFilestore); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["newFilestore"] = transformedNewFilestore
	}

	transformedNewLustre, err := expandHypercomputeclusterClusterStorageResourcesConfigNewLustre(original["new_lustre"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedNewLustre); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["newLustre"] = transformedNewLustre
	}

	return transformed, nil
}

func expandHypercomputeclusterClusterStorageResourcesConfigExistingBucket(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	if v == nil {
		return nil, nil
	}
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedBucket, err := expandHypercomputeclusterClusterStorageResourcesConfigExistingBucketBucket(original["bucket"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedBucket); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["bucket"] = transformedBucket
	}

	return transformed, nil
}

func expandHypercomputeclusterClusterStorageResourcesConfigExistingBucketBucket(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterStorageResourcesConfigExistingFilestore(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	if v == nil {
		return nil, nil
	}
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedFilestore, err := expandHypercomputeclusterClusterStorageResourcesConfigExistingFilestoreFilestore(original["filestore"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedFilestore); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["filestore"] = transformedFilestore
	}

	return transformed, nil
}

func expandHypercomputeclusterClusterStorageResourcesConfigExistingFilestoreFilestore(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterStorageResourcesConfigExistingLustre(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	if v == nil {
		return nil, nil
	}
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedLustre, err := expandHypercomputeclusterClusterStorageResourcesConfigExistingLustreLustre(original["lustre"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedLustre); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["lustre"] = transformedLustre
	}

	return transformed, nil
}

func expandHypercomputeclusterClusterStorageResourcesConfigExistingLustreLustre(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterStorageResourcesConfigNewBucket(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	if v == nil {
		return nil, nil
	}
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedAutoclass, err := expandHypercomputeclusterClusterStorageResourcesConfigNewBucketAutoclass(original["autoclass"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedAutoclass); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["autoclass"] = transformedAutoclass
	}

	transformedBucket, err := expandHypercomputeclusterClusterStorageResourcesConfigNewBucketBucket(original["bucket"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedBucket); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["bucket"] = transformedBucket
	}

	transformedHierarchicalNamespace, err := expandHypercomputeclusterClusterStorageResourcesConfigNewBucketHierarchicalNamespace(original["hierarchical_namespace"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedHierarchicalNamespace); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["hierarchicalNamespace"] = transformedHierarchicalNamespace
	}

	transformedStorageClass, err := expandHypercomputeclusterClusterStorageResourcesConfigNewBucketStorageClass(original["storage_class"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedStorageClass); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["storageClass"] = transformedStorageClass
	}

	return transformed, nil
}

func expandHypercomputeclusterClusterStorageResourcesConfigNewBucketAutoclass(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	if v == nil {
		return nil, nil
	}
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedEnabled, err := expandHypercomputeclusterClusterStorageResourcesConfigNewBucketAutoclassEnabled(original["enabled"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedEnabled); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["enabled"] = transformedEnabled
	}

	return transformed, nil
}

func expandHypercomputeclusterClusterStorageResourcesConfigNewBucketAutoclassEnabled(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterStorageResourcesConfigNewBucketBucket(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterStorageResourcesConfigNewBucketHierarchicalNamespace(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	if v == nil {
		return nil, nil
	}
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedEnabled, err := expandHypercomputeclusterClusterStorageResourcesConfigNewBucketHierarchicalNamespaceEnabled(original["enabled"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedEnabled); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["enabled"] = transformedEnabled
	}

	return transformed, nil
}

func expandHypercomputeclusterClusterStorageResourcesConfigNewBucketHierarchicalNamespaceEnabled(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterStorageResourcesConfigNewBucketStorageClass(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterStorageResourcesConfigNewFilestore(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	if v == nil {
		return nil, nil
	}
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedDescription, err := expandHypercomputeclusterClusterStorageResourcesConfigNewFilestoreDescription(original["description"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedDescription); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["description"] = transformedDescription
	}

	transformedFileShares, err := expandHypercomputeclusterClusterStorageResourcesConfigNewFilestoreFileShares(original["file_shares"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedFileShares); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["fileShares"] = transformedFileShares
	}

	transformedFilestore, err := expandHypercomputeclusterClusterStorageResourcesConfigNewFilestoreFilestore(original["filestore"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedFilestore); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["filestore"] = transformedFilestore
	}

	transformedProtocol, err := expandHypercomputeclusterClusterStorageResourcesConfigNewFilestoreProtocol(original["protocol"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedProtocol); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["protocol"] = transformedProtocol
	}

	transformedTier, err := expandHypercomputeclusterClusterStorageResourcesConfigNewFilestoreTier(original["tier"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedTier); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["tier"] = transformedTier
	}

	return transformed, nil
}

func expandHypercomputeclusterClusterStorageResourcesConfigNewFilestoreDescription(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterStorageResourcesConfigNewFilestoreFileShares(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	if v == nil {
		return nil, nil
	}
	l := v.([]interface{})
	req := make([]interface{}, 0, len(l))
	for _, raw := range l {
		if raw == nil {
			continue
		}
		original := raw.(map[string]interface{})
		transformed := make(map[string]interface{})

		transformedCapacityGb, err := expandHypercomputeclusterClusterStorageResourcesConfigNewFilestoreFileSharesCapacityGb(original["capacity_gb"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedCapacityGb); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["capacityGb"] = transformedCapacityGb
		}

		transformedFileShare, err := expandHypercomputeclusterClusterStorageResourcesConfigNewFilestoreFileSharesFileShare(original["file_share"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedFileShare); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["fileShare"] = transformedFileShare
		}

		req = append(req, transformed)
	}
	return req, nil
}

func expandHypercomputeclusterClusterStorageResourcesConfigNewFilestoreFileSharesCapacityGb(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterStorageResourcesConfigNewFilestoreFileSharesFileShare(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterStorageResourcesConfigNewFilestoreFilestore(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterStorageResourcesConfigNewFilestoreProtocol(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterStorageResourcesConfigNewFilestoreTier(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterStorageResourcesConfigNewLustre(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	if v == nil {
		return nil, nil
	}
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedCapacityGb, err := expandHypercomputeclusterClusterStorageResourcesConfigNewLustreCapacityGb(original["capacity_gb"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedCapacityGb); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["capacityGb"] = transformedCapacityGb
	}

	transformedDescription, err := expandHypercomputeclusterClusterStorageResourcesConfigNewLustreDescription(original["description"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedDescription); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["description"] = transformedDescription
	}

	transformedFilesystem, err := expandHypercomputeclusterClusterStorageResourcesConfigNewLustreFilesystem(original["filesystem"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedFilesystem); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["filesystem"] = transformedFilesystem
	}

	transformedLustre, err := expandHypercomputeclusterClusterStorageResourcesConfigNewLustreLustre(original["lustre"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedLustre); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["lustre"] = transformedLustre
	}

	return transformed, nil
}

func expandHypercomputeclusterClusterStorageResourcesConfigNewLustreCapacityGb(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterStorageResourcesConfigNewLustreDescription(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterStorageResourcesConfigNewLustreFilesystem(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterStorageResourcesConfigNewLustreLustre(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterStorageResourcesFilestore(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	if v == nil {
		return nil, nil
	}
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedFilestore, err := expandHypercomputeclusterClusterStorageResourcesFilestoreFilestore(original["filestore"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedFilestore); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["filestore"] = transformedFilestore
	}

	return transformed, nil
}

func expandHypercomputeclusterClusterStorageResourcesFilestoreFilestore(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterStorageResourcesLustre(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	if v == nil {
		return nil, nil
	}
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedLustre, err := expandHypercomputeclusterClusterStorageResourcesLustreLustre(original["lustre"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedLustre); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["lustre"] = transformedLustre
	}

	return transformed, nil
}

func expandHypercomputeclusterClusterStorageResourcesLustreLustre(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandHypercomputeclusterClusterEffectiveLabels(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (map[string]string, error) {
	if v == nil {
		return map[string]string{}, nil
	}
	m := make(map[string]string)
	for k, val := range v.(map[string]interface{}) {
		m[k] = val.(string)
	}
	return m, nil
}
