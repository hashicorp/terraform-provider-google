// Copyright (c) HashiCorp, Inc.
// SPDX-License-Identifier: MPL-2.0

// ----------------------------------------------------------------------------
//
//     ***     AUTO GENERATED CODE    ***    Type: MMv1     ***
//
// ----------------------------------------------------------------------------
//
//     This file is automatically generated by Magic Modules and manual
//     changes will be clobbered when the file is regenerated.
//
//     Please read more about how to change this file in
//     .github/CONTRIBUTING.md.
//
// ----------------------------------------------------------------------------

package dataplex

import (
	"fmt"
	"log"
	"reflect"
	"strings"
	"time"

	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/customdiff"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/schema"

	"github.com/hashicorp/terraform-provider-google/google/tpgresource"
	transport_tpg "github.com/hashicorp/terraform-provider-google/google/transport"
	"github.com/hashicorp/terraform-provider-google/google/verify"
)

func ResourceDataplexTask() *schema.Resource {
	return &schema.Resource{
		Create: resourceDataplexTaskCreate,
		Read:   resourceDataplexTaskRead,
		Update: resourceDataplexTaskUpdate,
		Delete: resourceDataplexTaskDelete,

		Importer: &schema.ResourceImporter{
			State: resourceDataplexTaskImport,
		},

		Timeouts: &schema.ResourceTimeout{
			Create: schema.DefaultTimeout(5 * time.Minute),
			Update: schema.DefaultTimeout(5 * time.Minute),
			Delete: schema.DefaultTimeout(5 * time.Minute),
		},

		CustomizeDiff: customdiff.All(
			tpgresource.SetLabelsDiff,
			tpgresource.DefaultProviderProject,
		),

		Schema: map[string]*schema.Schema{
			"execution_spec": {
				Type:        schema.TypeList,
				Required:    true,
				Description: `Configuration for the cluster`,
				MaxItems:    1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"service_account": {
							Type:        schema.TypeString,
							Required:    true,
							Description: `Service account to use to execute a task. If not provided, the default Compute service account for the project is used.`,
						},
						"args": {
							Type:        schema.TypeMap,
							Optional:    true,
							Description: `The arguments to pass to the task. The args can use placeholders of the format ${placeholder} as part of key/value string. These will be interpolated before passing the args to the driver. Currently supported placeholders: - ${taskId} - ${job_time} To pass positional args, set the key as TASK_ARGS. The value should be a comma-separated string of all the positional arguments. To use a delimiter other than comma, refer to https://cloud.google.com/sdk/gcloud/reference/topic/escaping. In case of other keys being present in the args, then TASK_ARGS will be passed as the last argument. An object containing a list of 'key': value pairs. Example: { 'name': 'wrench', 'mass': '1.3kg', 'count': '3' }.`,
							Elem:        &schema.Schema{Type: schema.TypeString},
						},
						"kms_key": {
							Type:        schema.TypeString,
							Optional:    true,
							Description: `The Cloud KMS key to use for encryption, of the form: projects/{project_number}/locations/{locationId}/keyRings/{key-ring-name}/cryptoKeys/{key-name}.`,
						},
						"max_job_execution_lifetime": {
							Type:        schema.TypeString,
							Optional:    true,
							Description: `The maximum duration after which the job execution is expired. A duration in seconds with up to nine fractional digits, ending with 's'. Example: '3.5s'.`,
						},
						"project": {
							Type:        schema.TypeString,
							Optional:    true,
							Description: `The project in which jobs are run. By default, the project containing the Lake is used. If a project is provided, the ExecutionSpec.service_account must belong to this project.`,
						},
					},
				},
			},
			"trigger_spec": {
				Type:        schema.TypeList,
				Required:    true,
				Description: `Configuration for the cluster`,
				MaxItems:    1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"type": {
							Type:         schema.TypeString,
							Required:     true,
							ForceNew:     true,
							ValidateFunc: verify.ValidateEnum([]string{"ON_DEMAND", "RECURRING"}),
							Description:  `Trigger type of the user-specified Task Possible values: ["ON_DEMAND", "RECURRING"]`,
						},
						"disabled": {
							Type:        schema.TypeBool,
							Optional:    true,
							Description: `Prevent the task from executing. This does not cancel already running tasks. It is intended to temporarily disable RECURRING tasks.`,
						},
						"max_retries": {
							Type:        schema.TypeInt,
							Optional:    true,
							Description: `Number of retry attempts before aborting. Set to zero to never attempt to retry a failed task.`,
						},
						"schedule": {
							Type:        schema.TypeString,
							Optional:    true,
							Description: `Cron schedule (https://en.wikipedia.org/wiki/Cron) for running tasks periodically. To explicitly set a timezone to the cron tab, apply a prefix in the cron tab: 'CRON_TZ=${IANA_TIME_ZONE}' or 'TZ=${IANA_TIME_ZONE}'. The ${IANA_TIME_ZONE} may only be a valid string from IANA time zone database. For example, CRON_TZ=America/New_York 1 * * * *, or TZ=America/New_York 1 * * * *. This field is required for RECURRING tasks.`,
						},
						"start_time": {
							Type:        schema.TypeString,
							Optional:    true,
							Description: `The first run of the task will be after this time. If not specified, the task will run shortly after being submitted if ON_DEMAND and based on the schedule if RECURRING.`,
						},
					},
				},
			},
			"description": {
				Type:        schema.TypeString,
				Optional:    true,
				Description: `User-provided description of the task.`,
			},
			"display_name": {
				Type:        schema.TypeString,
				Optional:    true,
				Description: `User friendly display name.`,
			},
			"labels": {
				Type:     schema.TypeMap,
				Optional: true,
				Description: `User-defined labels for the task.


**Note**: This field is non-authoritative, and will only manage the labels present in your configuration.
Please refer to the field 'effective_labels' for all of the labels present on the resource.`,
				Elem: &schema.Schema{Type: schema.TypeString},
			},
			"lake": {
				Type:        schema.TypeString,
				Optional:    true,
				ForceNew:    true,
				Description: `The lake in which the task will be created in.`,
			},
			"location": {
				Type:        schema.TypeString,
				Optional:    true,
				ForceNew:    true,
				Description: `The location in which the task will be created in.`,
			},
			"notebook": {
				Type:        schema.TypeList,
				Optional:    true,
				Description: `A service with manual scaling runs continuously, allowing you to perform complex initialization and rely on the state of its memory over time.`,
				MaxItems:    1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"notebook": {
							Type:        schema.TypeString,
							Required:    true,
							Description: `Path to input notebook. This can be the Cloud Storage URI of the notebook file or the path to a Notebook Content. The execution args are accessible as environment variables (TASK_key=value).`,
						},
						"archive_uris": {
							Type:        schema.TypeList,
							Optional:    true,
							Description: `Cloud Storage URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.`,
							Elem: &schema.Schema{
								Type: schema.TypeString,
							},
						},
						"file_uris": {
							Type:        schema.TypeList,
							Optional:    true,
							Description: `Cloud Storage URIs of files to be placed in the working directory of each executor.`,
							Elem: &schema.Schema{
								Type: schema.TypeString,
							},
						},
						"infrastructure_spec": {
							Type:        schema.TypeList,
							Optional:    true,
							Description: `Infrastructure specification for the execution.`,
							MaxItems:    1,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"batch": {
										Type:        schema.TypeList,
										Optional:    true,
										Description: `Compute resources needed for a Task when using Dataproc Serverless.`,
										MaxItems:    1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"executors_count": {
													Type:        schema.TypeInt,
													Optional:    true,
													Description: `Total number of job executors. Executor Count should be between 2 and 100. [Default=2]`,
													Default:     2,
												},
												"max_executors_count": {
													Type:        schema.TypeInt,
													Optional:    true,
													Description: `Max configurable executors. If maxExecutorsCount > executorsCount, then auto-scaling is enabled. Max Executor Count should be between 2 and 1000. [Default=1000]`,
													Default:     1000,
												},
											},
										},
									},
									"container_image": {
										Type:        schema.TypeList,
										Optional:    true,
										Description: `Container Image Runtime Configuration.`,
										MaxItems:    1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"image": {
													Type:        schema.TypeString,
													Optional:    true,
													Description: `Container image to use.`,
												},
												"java_jars": {
													Type:        schema.TypeList,
													Optional:    true,
													Description: `A list of Java JARS to add to the classpath. Valid input includes Cloud Storage URIs to Jar binaries. For example, gs://bucket-name/my/path/to/file.jar`,
													Elem: &schema.Schema{
														Type: schema.TypeString,
													},
												},
												"properties": {
													Type:        schema.TypeMap,
													Optional:    true,
													Description: `Override to common configuration of open source components installed on the Dataproc cluster. The properties to set on daemon config files. Property keys are specified in prefix:property format, for example core:hadoop.tmp.dir. For more information, see Cluster properties.`,
													Elem:        &schema.Schema{Type: schema.TypeString},
												},
												"python_packages": {
													Type:        schema.TypeList,
													Optional:    true,
													Description: `A list of python packages to be installed. Valid formats include Cloud Storage URI to a PIP installable library. For example, gs://bucket-name/my/path/to/lib.tar.gz`,
													Elem: &schema.Schema{
														Type: schema.TypeString,
													},
												},
											},
										},
									},
									"vpc_network": {
										Type:        schema.TypeList,
										Optional:    true,
										Description: `Vpc network.`,
										MaxItems:    1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"network": {
													Type:         schema.TypeString,
													Optional:     true,
													Description:  `The Cloud VPC network in which the job is run. By default, the Cloud VPC network named Default within the project is used.`,
													ExactlyOneOf: []string{},
												},
												"network_tags": {
													Type:        schema.TypeList,
													Optional:    true,
													Description: `List of network tags to apply to the job.`,
													Elem: &schema.Schema{
														Type: schema.TypeString,
													},
												},
												"sub_network": {
													Type:         schema.TypeString,
													Optional:     true,
													Description:  `The Cloud VPC sub-network in which the job is run.`,
													ExactlyOneOf: []string{},
												},
											},
										},
									},
								},
							},
						},
					},
				},
				ExactlyOneOf: []string{"spark", "notebook"},
			},
			"spark": {
				Type:        schema.TypeList,
				Optional:    true,
				Description: `A service with manual scaling runs continuously, allowing you to perform complex initialization and rely on the state of its memory over time.`,
				MaxItems:    1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"archive_uris": {
							Type:        schema.TypeList,
							Optional:    true,
							Description: `Cloud Storage URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.`,
							Elem: &schema.Schema{
								Type: schema.TypeString,
							},
						},
						"file_uris": {
							Type:        schema.TypeList,
							Optional:    true,
							Description: `Cloud Storage URIs of files to be placed in the working directory of each executor.`,
							Elem: &schema.Schema{
								Type: schema.TypeString,
							},
						},
						"infrastructure_spec": {
							Type:        schema.TypeList,
							Optional:    true,
							Description: `Infrastructure specification for the execution.`,
							MaxItems:    1,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"batch": {
										Type:        schema.TypeList,
										Optional:    true,
										Description: `Compute resources needed for a Task when using Dataproc Serverless.`,
										MaxItems:    1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"executors_count": {
													Type:        schema.TypeInt,
													Optional:    true,
													Description: `Total number of job executors. Executor Count should be between 2 and 100. [Default=2]`,
													Default:     2,
												},
												"max_executors_count": {
													Type:        schema.TypeInt,
													Optional:    true,
													Description: `Max configurable executors. If maxExecutorsCount > executorsCount, then auto-scaling is enabled. Max Executor Count should be between 2 and 1000. [Default=1000]`,
													Default:     1000,
												},
											},
										},
									},
									"container_image": {
										Type:        schema.TypeList,
										Optional:    true,
										Description: `Container Image Runtime Configuration.`,
										MaxItems:    1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"image": {
													Type:        schema.TypeString,
													Optional:    true,
													Description: `Container image to use.`,
												},
												"java_jars": {
													Type:        schema.TypeList,
													Optional:    true,
													Description: `A list of Java JARS to add to the classpath. Valid input includes Cloud Storage URIs to Jar binaries. For example, gs://bucket-name/my/path/to/file.jar`,
													Elem: &schema.Schema{
														Type: schema.TypeString,
													},
												},
												"properties": {
													Type:        schema.TypeMap,
													Optional:    true,
													Description: `Override to common configuration of open source components installed on the Dataproc cluster. The properties to set on daemon config files. Property keys are specified in prefix:property format, for example core:hadoop.tmp.dir. For more information, see Cluster properties.`,
													Elem:        &schema.Schema{Type: schema.TypeString},
												},
												"python_packages": {
													Type:        schema.TypeList,
													Optional:    true,
													Description: `A list of python packages to be installed. Valid formats include Cloud Storage URI to a PIP installable library. For example, gs://bucket-name/my/path/to/lib.tar.gz`,
													Elem: &schema.Schema{
														Type: schema.TypeString,
													},
												},
											},
										},
									},
									"vpc_network": {
										Type:        schema.TypeList,
										Optional:    true,
										Description: `Vpc network.`,
										MaxItems:    1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"network": {
													Type:         schema.TypeString,
													Optional:     true,
													Description:  `The Cloud VPC network in which the job is run. By default, the Cloud VPC network named Default within the project is used.`,
													ExactlyOneOf: []string{},
												},
												"network_tags": {
													Type:        schema.TypeList,
													Optional:    true,
													Description: `List of network tags to apply to the job.`,
													Elem: &schema.Schema{
														Type: schema.TypeString,
													},
												},
												"sub_network": {
													Type:         schema.TypeString,
													Optional:     true,
													Description:  `The Cloud VPC sub-network in which the job is run.`,
													ExactlyOneOf: []string{},
												},
											},
										},
									},
								},
							},
						},
						"main_class": {
							Type:         schema.TypeString,
							Optional:     true,
							Description:  `The name of the driver's main class. The jar file that contains the class must be in the default CLASSPATH or specified in jar_file_uris. The execution args are passed in as a sequence of named process arguments (--key=value).`,
							ExactlyOneOf: []string{},
						},
						"main_jar_file_uri": {
							Type:         schema.TypeString,
							Optional:     true,
							Description:  `The Cloud Storage URI of the jar file that contains the main class. The execution args are passed in as a sequence of named process arguments (--key=value).`,
							ExactlyOneOf: []string{},
						},
						"python_script_file": {
							Type:         schema.TypeString,
							Optional:     true,
							Description:  `The Gcloud Storage URI of the main Python file to use as the driver. Must be a .py file. The execution args are passed in as a sequence of named process arguments (--key=value).`,
							ExactlyOneOf: []string{},
						},
						"sql_script": {
							Type:         schema.TypeString,
							Optional:     true,
							Description:  `The query text. The execution args are used to declare a set of script variables (set key='value';).`,
							ExactlyOneOf: []string{},
						},
						"sql_script_file": {
							Type:         schema.TypeString,
							Optional:     true,
							Description:  `A reference to a query file. This can be the Cloud Storage URI of the query file or it can the path to a SqlScript Content. The execution args are used to declare a set of script variables (set key='value';).`,
							ExactlyOneOf: []string{},
						},
					},
				},
				ExactlyOneOf: []string{"spark", "notebook"},
			},
			"task_id": {
				Type:        schema.TypeString,
				Optional:    true,
				ForceNew:    true,
				Description: `The task Id of the task.`,
			},
			"create_time": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `The time when the task was created.`,
			},
			"effective_labels": {
				Type:        schema.TypeMap,
				Computed:    true,
				Description: `All of labels (key/value pairs) present on the resource in GCP, including the labels configured through Terraform, other clients and services.`,
				Elem:        &schema.Schema{Type: schema.TypeString},
			},
			"execution_status": {
				Type:        schema.TypeList,
				Computed:    true,
				Description: `Configuration for the cluster`,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"latest_job": {
							Type:        schema.TypeList,
							Computed:    true,
							Description: `latest job execution.`,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"end_time": {
										Type:        schema.TypeString,
										Computed:    true,
										Description: `The time when the job ended.`,
									},
									"message": {
										Type:        schema.TypeString,
										Computed:    true,
										Description: `Additional information about the current state.`,
									},
									"name": {
										Type:        schema.TypeString,
										Computed:    true,
										Description: `The relative resource name of the job, of the form: projects/{project_number}/locations/{locationId}/lakes/{lakeId}/tasks/{taskId}/jobs/{jobId}.`,
									},
									"retry_count": {
										Type:        schema.TypeInt,
										Computed:    true,
										Description: `The number of times the job has been retried (excluding the initial attempt).`,
									},
									"service": {
										Type:        schema.TypeString,
										Computed:    true,
										Description: `The underlying service running a job.`,
									},
									"service_job": {
										Type:        schema.TypeString,
										Computed:    true,
										Description: `The full resource name for the job run under a particular service.`,
									},
									"start_time": {
										Type:        schema.TypeString,
										Computed:    true,
										Description: `The time when the job was started.`,
									},
									"state": {
										Type:        schema.TypeString,
										Computed:    true,
										Description: `Execution state for the job.`,
									},
									"uid": {
										Type:        schema.TypeString,
										Computed:    true,
										Description: `System generated globally unique ID for the job.`,
									},
								},
							},
						},
						"update_time": {
							Type:        schema.TypeString,
							Computed:    true,
							Description: `Last update time of the status.`,
						},
					},
				},
			},
			"name": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `The relative resource name of the task, of the form: projects/{project_number}/locations/{locationId}/lakes/{lakeId}/ tasks/{name}.`,
			},
			"state": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `Current state of the task.`,
			},
			"terraform_labels": {
				Type:     schema.TypeMap,
				Computed: true,
				Description: `The combination of labels configured directly on the resource
 and default labels configured on the provider.`,
				Elem: &schema.Schema{Type: schema.TypeString},
			},
			"uid": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `System generated globally unique ID for the task. This ID will be different if the task is deleted and re-created with the same name.`,
			},
			"update_time": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `The time when the task was last updated.`,
			},
			"project": {
				Type:     schema.TypeString,
				Optional: true,
				Computed: true,
				ForceNew: true,
			},
		},
		UseJSONNumber: true,
	}
}

func resourceDataplexTaskCreate(d *schema.ResourceData, meta interface{}) error {
	config := meta.(*transport_tpg.Config)
	userAgent, err := tpgresource.GenerateUserAgentString(d, config.UserAgent)
	if err != nil {
		return err
	}

	obj := make(map[string]interface{})
	descriptionProp, err := expandDataplexTaskDescription(d.Get("description"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("description"); !tpgresource.IsEmptyValue(reflect.ValueOf(descriptionProp)) && (ok || !reflect.DeepEqual(v, descriptionProp)) {
		obj["description"] = descriptionProp
	}
	displayNameProp, err := expandDataplexTaskDisplayName(d.Get("display_name"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("display_name"); !tpgresource.IsEmptyValue(reflect.ValueOf(displayNameProp)) && (ok || !reflect.DeepEqual(v, displayNameProp)) {
		obj["displayName"] = displayNameProp
	}
	triggerSpecProp, err := expandDataplexTaskTriggerSpec(d.Get("trigger_spec"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("trigger_spec"); !tpgresource.IsEmptyValue(reflect.ValueOf(triggerSpecProp)) && (ok || !reflect.DeepEqual(v, triggerSpecProp)) {
		obj["triggerSpec"] = triggerSpecProp
	}
	executionSpecProp, err := expandDataplexTaskExecutionSpec(d.Get("execution_spec"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("execution_spec"); !tpgresource.IsEmptyValue(reflect.ValueOf(executionSpecProp)) && (ok || !reflect.DeepEqual(v, executionSpecProp)) {
		obj["executionSpec"] = executionSpecProp
	}
	sparkProp, err := expandDataplexTaskSpark(d.Get("spark"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("spark"); !tpgresource.IsEmptyValue(reflect.ValueOf(sparkProp)) && (ok || !reflect.DeepEqual(v, sparkProp)) {
		obj["spark"] = sparkProp
	}
	notebookProp, err := expandDataplexTaskNotebook(d.Get("notebook"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("notebook"); !tpgresource.IsEmptyValue(reflect.ValueOf(notebookProp)) && (ok || !reflect.DeepEqual(v, notebookProp)) {
		obj["notebook"] = notebookProp
	}
	labelsProp, err := expandDataplexTaskEffectiveLabels(d.Get("effective_labels"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("effective_labels"); !tpgresource.IsEmptyValue(reflect.ValueOf(labelsProp)) && (ok || !reflect.DeepEqual(v, labelsProp)) {
		obj["labels"] = labelsProp
	}

	url, err := tpgresource.ReplaceVars(d, config, "{{DataplexBasePath}}projects/{{project}}/locations/{{location}}/lakes/{{lake}}/tasks?task_id={{task_id}}")
	if err != nil {
		return err
	}

	log.Printf("[DEBUG] Creating new Task: %#v", obj)
	billingProject := ""

	project, err := tpgresource.GetProject(d, config)
	if err != nil {
		return fmt.Errorf("Error fetching project for Task: %s", err)
	}
	billingProject = project

	// err == nil indicates that the billing_project value was found
	if bp, err := tpgresource.GetBillingProject(d, config); err == nil {
		billingProject = bp
	}

	res, err := transport_tpg.SendRequest(transport_tpg.SendRequestOptions{
		Config:    config,
		Method:    "POST",
		Project:   billingProject,
		RawURL:    url,
		UserAgent: userAgent,
		Body:      obj,
		Timeout:   d.Timeout(schema.TimeoutCreate),
	})
	if err != nil {
		return fmt.Errorf("Error creating Task: %s", err)
	}

	// Store the ID now
	id, err := tpgresource.ReplaceVars(d, config, "projects/{{project}}/locations/{{location}}/lakes/{{lake}}/tasks/{{task_id}}")
	if err != nil {
		return fmt.Errorf("Error constructing id: %s", err)
	}
	d.SetId(id)

	err = DataplexOperationWaitTime(
		config, res, project, "Creating Task", userAgent,
		d.Timeout(schema.TimeoutCreate))

	if err != nil {
		// The resource didn't actually create
		d.SetId("")
		return fmt.Errorf("Error waiting to create Task: %s", err)
	}

	log.Printf("[DEBUG] Finished creating Task %q: %#v", d.Id(), res)

	return resourceDataplexTaskRead(d, meta)
}

func resourceDataplexTaskRead(d *schema.ResourceData, meta interface{}) error {
	config := meta.(*transport_tpg.Config)
	userAgent, err := tpgresource.GenerateUserAgentString(d, config.UserAgent)
	if err != nil {
		return err
	}

	url, err := tpgresource.ReplaceVars(d, config, "{{DataplexBasePath}}projects/{{project}}/locations/{{location}}/lakes/{{lake}}/tasks/{{task_id}}")
	if err != nil {
		return err
	}

	billingProject := ""

	project, err := tpgresource.GetProject(d, config)
	if err != nil {
		return fmt.Errorf("Error fetching project for Task: %s", err)
	}
	billingProject = project

	// err == nil indicates that the billing_project value was found
	if bp, err := tpgresource.GetBillingProject(d, config); err == nil {
		billingProject = bp
	}

	res, err := transport_tpg.SendRequest(transport_tpg.SendRequestOptions{
		Config:    config,
		Method:    "GET",
		Project:   billingProject,
		RawURL:    url,
		UserAgent: userAgent,
	})
	if err != nil {
		return transport_tpg.HandleNotFoundError(err, d, fmt.Sprintf("DataplexTask %q", d.Id()))
	}

	if err := d.Set("project", project); err != nil {
		return fmt.Errorf("Error reading Task: %s", err)
	}

	if err := d.Set("name", flattenDataplexTaskName(res["name"], d, config)); err != nil {
		return fmt.Errorf("Error reading Task: %s", err)
	}
	if err := d.Set("uid", flattenDataplexTaskUid(res["uid"], d, config)); err != nil {
		return fmt.Errorf("Error reading Task: %s", err)
	}
	if err := d.Set("create_time", flattenDataplexTaskCreateTime(res["createTime"], d, config)); err != nil {
		return fmt.Errorf("Error reading Task: %s", err)
	}
	if err := d.Set("update_time", flattenDataplexTaskUpdateTime(res["updateTime"], d, config)); err != nil {
		return fmt.Errorf("Error reading Task: %s", err)
	}
	if err := d.Set("description", flattenDataplexTaskDescription(res["description"], d, config)); err != nil {
		return fmt.Errorf("Error reading Task: %s", err)
	}
	if err := d.Set("display_name", flattenDataplexTaskDisplayName(res["displayName"], d, config)); err != nil {
		return fmt.Errorf("Error reading Task: %s", err)
	}
	if err := d.Set("state", flattenDataplexTaskState(res["state"], d, config)); err != nil {
		return fmt.Errorf("Error reading Task: %s", err)
	}
	if err := d.Set("labels", flattenDataplexTaskLabels(res["labels"], d, config)); err != nil {
		return fmt.Errorf("Error reading Task: %s", err)
	}
	if err := d.Set("trigger_spec", flattenDataplexTaskTriggerSpec(res["triggerSpec"], d, config)); err != nil {
		return fmt.Errorf("Error reading Task: %s", err)
	}
	if err := d.Set("execution_spec", flattenDataplexTaskExecutionSpec(res["executionSpec"], d, config)); err != nil {
		return fmt.Errorf("Error reading Task: %s", err)
	}
	if err := d.Set("execution_status", flattenDataplexTaskExecutionStatus(res["executionStatus"], d, config)); err != nil {
		return fmt.Errorf("Error reading Task: %s", err)
	}
	if err := d.Set("spark", flattenDataplexTaskSpark(res["spark"], d, config)); err != nil {
		return fmt.Errorf("Error reading Task: %s", err)
	}
	if err := d.Set("notebook", flattenDataplexTaskNotebook(res["notebook"], d, config)); err != nil {
		return fmt.Errorf("Error reading Task: %s", err)
	}
	if err := d.Set("terraform_labels", flattenDataplexTaskTerraformLabels(res["labels"], d, config)); err != nil {
		return fmt.Errorf("Error reading Task: %s", err)
	}
	if err := d.Set("effective_labels", flattenDataplexTaskEffectiveLabels(res["labels"], d, config)); err != nil {
		return fmt.Errorf("Error reading Task: %s", err)
	}

	return nil
}

func resourceDataplexTaskUpdate(d *schema.ResourceData, meta interface{}) error {
	config := meta.(*transport_tpg.Config)
	userAgent, err := tpgresource.GenerateUserAgentString(d, config.UserAgent)
	if err != nil {
		return err
	}

	billingProject := ""

	project, err := tpgresource.GetProject(d, config)
	if err != nil {
		return fmt.Errorf("Error fetching project for Task: %s", err)
	}
	billingProject = project

	obj := make(map[string]interface{})
	descriptionProp, err := expandDataplexTaskDescription(d.Get("description"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("description"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, descriptionProp)) {
		obj["description"] = descriptionProp
	}
	displayNameProp, err := expandDataplexTaskDisplayName(d.Get("display_name"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("display_name"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, displayNameProp)) {
		obj["displayName"] = displayNameProp
	}
	triggerSpecProp, err := expandDataplexTaskTriggerSpec(d.Get("trigger_spec"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("trigger_spec"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, triggerSpecProp)) {
		obj["triggerSpec"] = triggerSpecProp
	}
	executionSpecProp, err := expandDataplexTaskExecutionSpec(d.Get("execution_spec"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("execution_spec"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, executionSpecProp)) {
		obj["executionSpec"] = executionSpecProp
	}
	sparkProp, err := expandDataplexTaskSpark(d.Get("spark"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("spark"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, sparkProp)) {
		obj["spark"] = sparkProp
	}
	notebookProp, err := expandDataplexTaskNotebook(d.Get("notebook"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("notebook"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, notebookProp)) {
		obj["notebook"] = notebookProp
	}
	labelsProp, err := expandDataplexTaskEffectiveLabels(d.Get("effective_labels"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("effective_labels"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, labelsProp)) {
		obj["labels"] = labelsProp
	}

	url, err := tpgresource.ReplaceVars(d, config, "{{DataplexBasePath}}projects/{{project}}/locations/{{location}}/lakes/{{lake}}/tasks/{{task_id}}")
	if err != nil {
		return err
	}

	log.Printf("[DEBUG] Updating Task %q: %#v", d.Id(), obj)
	updateMask := []string{}

	if d.HasChange("description") {
		updateMask = append(updateMask, "description")
	}

	if d.HasChange("display_name") {
		updateMask = append(updateMask, "displayName")
	}

	if d.HasChange("trigger_spec") {
		updateMask = append(updateMask, "triggerSpec")
	}

	if d.HasChange("execution_spec") {
		updateMask = append(updateMask, "executionSpec")
	}

	if d.HasChange("spark") {
		updateMask = append(updateMask, "spark")
	}

	if d.HasChange("notebook") {
		updateMask = append(updateMask, "notebook")
	}

	if d.HasChange("effective_labels") {
		updateMask = append(updateMask, "labels")
	}
	// updateMask is a URL parameter but not present in the schema, so ReplaceVars
	// won't set it
	url, err = transport_tpg.AddQueryParams(url, map[string]string{"updateMask": strings.Join(updateMask, ",")})
	if err != nil {
		return err
	}

	// err == nil indicates that the billing_project value was found
	if bp, err := tpgresource.GetBillingProject(d, config); err == nil {
		billingProject = bp
	}

	// if updateMask is empty we are not updating anything so skip the post
	if len(updateMask) > 0 {
		res, err := transport_tpg.SendRequest(transport_tpg.SendRequestOptions{
			Config:    config,
			Method:    "PATCH",
			Project:   billingProject,
			RawURL:    url,
			UserAgent: userAgent,
			Body:      obj,
			Timeout:   d.Timeout(schema.TimeoutUpdate),
		})

		if err != nil {
			return fmt.Errorf("Error updating Task %q: %s", d.Id(), err)
		} else {
			log.Printf("[DEBUG] Finished updating Task %q: %#v", d.Id(), res)
		}

		err = DataplexOperationWaitTime(
			config, res, project, "Updating Task", userAgent,
			d.Timeout(schema.TimeoutUpdate))

		if err != nil {
			return err
		}
	}

	return resourceDataplexTaskRead(d, meta)
}

func resourceDataplexTaskDelete(d *schema.ResourceData, meta interface{}) error {
	config := meta.(*transport_tpg.Config)
	userAgent, err := tpgresource.GenerateUserAgentString(d, config.UserAgent)
	if err != nil {
		return err
	}

	billingProject := ""

	project, err := tpgresource.GetProject(d, config)
	if err != nil {
		return fmt.Errorf("Error fetching project for Task: %s", err)
	}
	billingProject = project

	url, err := tpgresource.ReplaceVars(d, config, "{{DataplexBasePath}}projects/{{project}}/locations/{{location}}/lakes/{{lake}}/tasks/{{task_id}}")
	if err != nil {
		return err
	}

	var obj map[string]interface{}

	// err == nil indicates that the billing_project value was found
	if bp, err := tpgresource.GetBillingProject(d, config); err == nil {
		billingProject = bp
	}

	log.Printf("[DEBUG] Deleting Task %q", d.Id())
	res, err := transport_tpg.SendRequest(transport_tpg.SendRequestOptions{
		Config:    config,
		Method:    "DELETE",
		Project:   billingProject,
		RawURL:    url,
		UserAgent: userAgent,
		Body:      obj,
		Timeout:   d.Timeout(schema.TimeoutDelete),
	})
	if err != nil {
		return transport_tpg.HandleNotFoundError(err, d, "Task")
	}

	err = DataplexOperationWaitTime(
		config, res, project, "Deleting Task", userAgent,
		d.Timeout(schema.TimeoutDelete))

	if err != nil {
		return err
	}

	log.Printf("[DEBUG] Finished deleting Task %q: %#v", d.Id(), res)
	return nil
}

func resourceDataplexTaskImport(d *schema.ResourceData, meta interface{}) ([]*schema.ResourceData, error) {
	config := meta.(*transport_tpg.Config)
	if err := tpgresource.ParseImportId([]string{
		"^projects/(?P<project>[^/]+)/locations/(?P<location>[^/]+)/lakes/(?P<lake>[^/]+)/tasks/(?P<task_id>[^/]+)$",
		"^(?P<project>[^/]+)/(?P<location>[^/]+)/(?P<lake>[^/]+)/(?P<task_id>[^/]+)$",
		"^(?P<location>[^/]+)/(?P<lake>[^/]+)/(?P<task_id>[^/]+)$",
	}, d, config); err != nil {
		return nil, err
	}

	// Replace import id for the resource id
	id, err := tpgresource.ReplaceVars(d, config, "projects/{{project}}/locations/{{location}}/lakes/{{lake}}/tasks/{{task_id}}")
	if err != nil {
		return nil, fmt.Errorf("Error constructing id: %s", err)
	}
	d.SetId(id)

	return []*schema.ResourceData{d}, nil
}

func flattenDataplexTaskName(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskUid(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskCreateTime(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskUpdateTime(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskDescription(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskDisplayName(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskState(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskLabels(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}

	transformed := make(map[string]interface{})
	if l, ok := d.GetOkExists("labels"); ok {
		for k := range l.(map[string]interface{}) {
			transformed[k] = v.(map[string]interface{})[k]
		}
	}

	return transformed
}

func flattenDataplexTaskTriggerSpec(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["type"] =
		flattenDataplexTaskTriggerSpecType(original["type"], d, config)
	transformed["start_time"] =
		flattenDataplexTaskTriggerSpecStartTime(original["startTime"], d, config)
	transformed["disabled"] =
		flattenDataplexTaskTriggerSpecDisabled(original["disabled"], d, config)
	transformed["max_retries"] =
		flattenDataplexTaskTriggerSpecMaxRetries(original["maxRetries"], d, config)
	transformed["schedule"] =
		flattenDataplexTaskTriggerSpecSchedule(original["schedule"], d, config)
	return []interface{}{transformed}
}
func flattenDataplexTaskTriggerSpecType(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskTriggerSpecStartTime(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskTriggerSpecDisabled(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskTriggerSpecMaxRetries(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenDataplexTaskTriggerSpecSchedule(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskExecutionSpec(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["args"] =
		flattenDataplexTaskExecutionSpecArgs(original["args"], d, config)
	transformed["service_account"] =
		flattenDataplexTaskExecutionSpecServiceAccount(original["serviceAccount"], d, config)
	transformed["project"] =
		flattenDataplexTaskExecutionSpecProject(original["project"], d, config)
	transformed["max_job_execution_lifetime"] =
		flattenDataplexTaskExecutionSpecMaxJobExecutionLifetime(original["maxJobExecutionLifetime"], d, config)
	transformed["kms_key"] =
		flattenDataplexTaskExecutionSpecKmsKey(original["kmsKey"], d, config)
	return []interface{}{transformed}
}
func flattenDataplexTaskExecutionSpecArgs(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskExecutionSpecServiceAccount(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskExecutionSpecProject(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskExecutionSpecMaxJobExecutionLifetime(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskExecutionSpecKmsKey(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskExecutionStatus(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["update_time"] =
		flattenDataplexTaskExecutionStatusUpdateTime(original["updateTime"], d, config)
	transformed["latest_job"] =
		flattenDataplexTaskExecutionStatusLatestJob(original["latestJob"], d, config)
	return []interface{}{transformed}
}
func flattenDataplexTaskExecutionStatusUpdateTime(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskExecutionStatusLatestJob(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["name"] =
		flattenDataplexTaskExecutionStatusLatestJobName(original["name"], d, config)
	transformed["uid"] =
		flattenDataplexTaskExecutionStatusLatestJobUid(original["uid"], d, config)
	transformed["start_time"] =
		flattenDataplexTaskExecutionStatusLatestJobStartTime(original["startTime"], d, config)
	transformed["end_time"] =
		flattenDataplexTaskExecutionStatusLatestJobEndTime(original["endTime"], d, config)
	transformed["state"] =
		flattenDataplexTaskExecutionStatusLatestJobState(original["state"], d, config)
	transformed["retry_count"] =
		flattenDataplexTaskExecutionStatusLatestJobRetryCount(original["retryCount"], d, config)
	transformed["service"] =
		flattenDataplexTaskExecutionStatusLatestJobService(original["service"], d, config)
	transformed["service_job"] =
		flattenDataplexTaskExecutionStatusLatestJobServiceJob(original["serviceJob"], d, config)
	transformed["message"] =
		flattenDataplexTaskExecutionStatusLatestJobMessage(original["message"], d, config)
	return []interface{}{transformed}
}
func flattenDataplexTaskExecutionStatusLatestJobName(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskExecutionStatusLatestJobUid(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskExecutionStatusLatestJobStartTime(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskExecutionStatusLatestJobEndTime(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskExecutionStatusLatestJobState(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskExecutionStatusLatestJobRetryCount(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenDataplexTaskExecutionStatusLatestJobService(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskExecutionStatusLatestJobServiceJob(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskExecutionStatusLatestJobMessage(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskSpark(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["file_uris"] =
		flattenDataplexTaskSparkFileUris(original["fileUris"], d, config)
	transformed["archive_uris"] =
		flattenDataplexTaskSparkArchiveUris(original["archiveUris"], d, config)
	transformed["infrastructure_spec"] =
		flattenDataplexTaskSparkInfrastructureSpec(original["infrastructureSpec"], d, config)
	transformed["main_jar_file_uri"] =
		flattenDataplexTaskSparkMainJarFileUri(original["mainJarFileUri"], d, config)
	transformed["main_class"] =
		flattenDataplexTaskSparkMainClass(original["mainClass"], d, config)
	transformed["python_script_file"] =
		flattenDataplexTaskSparkPythonScriptFile(original["pythonScriptFile"], d, config)
	transformed["sql_script_file"] =
		flattenDataplexTaskSparkSqlScriptFile(original["sqlScriptFile"], d, config)
	transformed["sql_script"] =
		flattenDataplexTaskSparkSqlScript(original["sqlScript"], d, config)
	return []interface{}{transformed}
}
func flattenDataplexTaskSparkFileUris(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskSparkArchiveUris(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskSparkInfrastructureSpec(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["batch"] =
		flattenDataplexTaskSparkInfrastructureSpecBatch(original["batch"], d, config)
	transformed["container_image"] =
		flattenDataplexTaskSparkInfrastructureSpecContainerImage(original["containerImage"], d, config)
	transformed["vpc_network"] =
		flattenDataplexTaskSparkInfrastructureSpecVpcNetwork(original["vpcNetwork"], d, config)
	return []interface{}{transformed}
}
func flattenDataplexTaskSparkInfrastructureSpecBatch(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["executors_count"] =
		flattenDataplexTaskSparkInfrastructureSpecBatchExecutorsCount(original["executorsCount"], d, config)
	transformed["max_executors_count"] =
		flattenDataplexTaskSparkInfrastructureSpecBatchMaxExecutorsCount(original["maxExecutorsCount"], d, config)
	return []interface{}{transformed}
}
func flattenDataplexTaskSparkInfrastructureSpecBatchExecutorsCount(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenDataplexTaskSparkInfrastructureSpecBatchMaxExecutorsCount(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenDataplexTaskSparkInfrastructureSpecContainerImage(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["image"] =
		flattenDataplexTaskSparkInfrastructureSpecContainerImageImage(original["image"], d, config)
	transformed["java_jars"] =
		flattenDataplexTaskSparkInfrastructureSpecContainerImageJavaJars(original["javaJars"], d, config)
	transformed["python_packages"] =
		flattenDataplexTaskSparkInfrastructureSpecContainerImagePythonPackages(original["pythonPackages"], d, config)
	transformed["properties"] =
		flattenDataplexTaskSparkInfrastructureSpecContainerImageProperties(original["properties"], d, config)
	return []interface{}{transformed}
}
func flattenDataplexTaskSparkInfrastructureSpecContainerImageImage(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskSparkInfrastructureSpecContainerImageJavaJars(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskSparkInfrastructureSpecContainerImagePythonPackages(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskSparkInfrastructureSpecContainerImageProperties(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskSparkInfrastructureSpecVpcNetwork(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["network_tags"] =
		flattenDataplexTaskSparkInfrastructureSpecVpcNetworkNetworkTags(original["networkTags"], d, config)
	transformed["network"] =
		flattenDataplexTaskSparkInfrastructureSpecVpcNetworkNetwork(original["network"], d, config)
	transformed["sub_network"] =
		flattenDataplexTaskSparkInfrastructureSpecVpcNetworkSubNetwork(original["subNetwork"], d, config)
	return []interface{}{transformed}
}
func flattenDataplexTaskSparkInfrastructureSpecVpcNetworkNetworkTags(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskSparkInfrastructureSpecVpcNetworkNetwork(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskSparkInfrastructureSpecVpcNetworkSubNetwork(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskSparkMainJarFileUri(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskSparkMainClass(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskSparkPythonScriptFile(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskSparkSqlScriptFile(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskSparkSqlScript(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskNotebook(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["notebook"] =
		flattenDataplexTaskNotebookNotebook(original["notebook"], d, config)
	transformed["infrastructure_spec"] =
		flattenDataplexTaskNotebookInfrastructureSpec(original["infrastructureSpec"], d, config)
	transformed["file_uris"] =
		flattenDataplexTaskNotebookFileUris(original["fileUris"], d, config)
	transformed["archive_uris"] =
		flattenDataplexTaskNotebookArchiveUris(original["archiveUris"], d, config)
	return []interface{}{transformed}
}
func flattenDataplexTaskNotebookNotebook(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskNotebookInfrastructureSpec(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["batch"] =
		flattenDataplexTaskNotebookInfrastructureSpecBatch(original["batch"], d, config)
	transformed["container_image"] =
		flattenDataplexTaskNotebookInfrastructureSpecContainerImage(original["containerImage"], d, config)
	transformed["vpc_network"] =
		flattenDataplexTaskNotebookInfrastructureSpecVpcNetwork(original["vpcNetwork"], d, config)
	return []interface{}{transformed}
}
func flattenDataplexTaskNotebookInfrastructureSpecBatch(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["executors_count"] =
		flattenDataplexTaskNotebookInfrastructureSpecBatchExecutorsCount(original["executorsCount"], d, config)
	transformed["max_executors_count"] =
		flattenDataplexTaskNotebookInfrastructureSpecBatchMaxExecutorsCount(original["maxExecutorsCount"], d, config)
	return []interface{}{transformed}
}
func flattenDataplexTaskNotebookInfrastructureSpecBatchExecutorsCount(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenDataplexTaskNotebookInfrastructureSpecBatchMaxExecutorsCount(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenDataplexTaskNotebookInfrastructureSpecContainerImage(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["image"] =
		flattenDataplexTaskNotebookInfrastructureSpecContainerImageImage(original["image"], d, config)
	transformed["java_jars"] =
		flattenDataplexTaskNotebookInfrastructureSpecContainerImageJavaJars(original["javaJars"], d, config)
	transformed["python_packages"] =
		flattenDataplexTaskNotebookInfrastructureSpecContainerImagePythonPackages(original["pythonPackages"], d, config)
	transformed["properties"] =
		flattenDataplexTaskNotebookInfrastructureSpecContainerImageProperties(original["properties"], d, config)
	return []interface{}{transformed}
}
func flattenDataplexTaskNotebookInfrastructureSpecContainerImageImage(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskNotebookInfrastructureSpecContainerImageJavaJars(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskNotebookInfrastructureSpecContainerImagePythonPackages(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskNotebookInfrastructureSpecContainerImageProperties(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskNotebookInfrastructureSpecVpcNetwork(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["network_tags"] =
		flattenDataplexTaskNotebookInfrastructureSpecVpcNetworkNetworkTags(original["networkTags"], d, config)
	transformed["network"] =
		flattenDataplexTaskNotebookInfrastructureSpecVpcNetworkNetwork(original["network"], d, config)
	transformed["sub_network"] =
		flattenDataplexTaskNotebookInfrastructureSpecVpcNetworkSubNetwork(original["subNetwork"], d, config)
	return []interface{}{transformed}
}
func flattenDataplexTaskNotebookInfrastructureSpecVpcNetworkNetworkTags(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskNotebookInfrastructureSpecVpcNetworkNetwork(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskNotebookInfrastructureSpecVpcNetworkSubNetwork(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskNotebookFileUris(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskNotebookArchiveUris(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexTaskTerraformLabels(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}

	transformed := make(map[string]interface{})
	if l, ok := d.GetOkExists("terraform_labels"); ok {
		for k := range l.(map[string]interface{}) {
			transformed[k] = v.(map[string]interface{})[k]
		}
	}

	return transformed
}

func flattenDataplexTaskEffectiveLabels(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func expandDataplexTaskDescription(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexTaskDisplayName(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexTaskTriggerSpec(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedType, err := expandDataplexTaskTriggerSpecType(original["type"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedType); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["type"] = transformedType
	}

	transformedStartTime, err := expandDataplexTaskTriggerSpecStartTime(original["start_time"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedStartTime); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["startTime"] = transformedStartTime
	}

	transformedDisabled, err := expandDataplexTaskTriggerSpecDisabled(original["disabled"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedDisabled); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["disabled"] = transformedDisabled
	}

	transformedMaxRetries, err := expandDataplexTaskTriggerSpecMaxRetries(original["max_retries"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedMaxRetries); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["maxRetries"] = transformedMaxRetries
	}

	transformedSchedule, err := expandDataplexTaskTriggerSpecSchedule(original["schedule"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedSchedule); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["schedule"] = transformedSchedule
	}

	return transformed, nil
}

func expandDataplexTaskTriggerSpecType(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexTaskTriggerSpecStartTime(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexTaskTriggerSpecDisabled(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexTaskTriggerSpecMaxRetries(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexTaskTriggerSpecSchedule(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexTaskExecutionSpec(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedArgs, err := expandDataplexTaskExecutionSpecArgs(original["args"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedArgs); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["args"] = transformedArgs
	}

	transformedServiceAccount, err := expandDataplexTaskExecutionSpecServiceAccount(original["service_account"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedServiceAccount); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["serviceAccount"] = transformedServiceAccount
	}

	transformedProject, err := expandDataplexTaskExecutionSpecProject(original["project"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedProject); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["project"] = transformedProject
	}

	transformedMaxJobExecutionLifetime, err := expandDataplexTaskExecutionSpecMaxJobExecutionLifetime(original["max_job_execution_lifetime"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedMaxJobExecutionLifetime); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["maxJobExecutionLifetime"] = transformedMaxJobExecutionLifetime
	}

	transformedKmsKey, err := expandDataplexTaskExecutionSpecKmsKey(original["kms_key"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedKmsKey); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["kmsKey"] = transformedKmsKey
	}

	return transformed, nil
}

func expandDataplexTaskExecutionSpecArgs(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (map[string]string, error) {
	if v == nil {
		return map[string]string{}, nil
	}
	m := make(map[string]string)
	for k, val := range v.(map[string]interface{}) {
		m[k] = val.(string)
	}
	return m, nil
}

func expandDataplexTaskExecutionSpecServiceAccount(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexTaskExecutionSpecProject(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexTaskExecutionSpecMaxJobExecutionLifetime(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexTaskExecutionSpecKmsKey(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexTaskSpark(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedFileUris, err := expandDataplexTaskSparkFileUris(original["file_uris"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedFileUris); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["fileUris"] = transformedFileUris
	}

	transformedArchiveUris, err := expandDataplexTaskSparkArchiveUris(original["archive_uris"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedArchiveUris); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["archiveUris"] = transformedArchiveUris
	}

	transformedInfrastructureSpec, err := expandDataplexTaskSparkInfrastructureSpec(original["infrastructure_spec"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedInfrastructureSpec); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["infrastructureSpec"] = transformedInfrastructureSpec
	}

	transformedMainJarFileUri, err := expandDataplexTaskSparkMainJarFileUri(original["main_jar_file_uri"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedMainJarFileUri); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["mainJarFileUri"] = transformedMainJarFileUri
	}

	transformedMainClass, err := expandDataplexTaskSparkMainClass(original["main_class"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedMainClass); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["mainClass"] = transformedMainClass
	}

	transformedPythonScriptFile, err := expandDataplexTaskSparkPythonScriptFile(original["python_script_file"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedPythonScriptFile); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["pythonScriptFile"] = transformedPythonScriptFile
	}

	transformedSqlScriptFile, err := expandDataplexTaskSparkSqlScriptFile(original["sql_script_file"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedSqlScriptFile); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["sqlScriptFile"] = transformedSqlScriptFile
	}

	transformedSqlScript, err := expandDataplexTaskSparkSqlScript(original["sql_script"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedSqlScript); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["sqlScript"] = transformedSqlScript
	}

	return transformed, nil
}

func expandDataplexTaskSparkFileUris(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexTaskSparkArchiveUris(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexTaskSparkInfrastructureSpec(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedBatch, err := expandDataplexTaskSparkInfrastructureSpecBatch(original["batch"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedBatch); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["batch"] = transformedBatch
	}

	transformedContainerImage, err := expandDataplexTaskSparkInfrastructureSpecContainerImage(original["container_image"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedContainerImage); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["containerImage"] = transformedContainerImage
	}

	transformedVpcNetwork, err := expandDataplexTaskSparkInfrastructureSpecVpcNetwork(original["vpc_network"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedVpcNetwork); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["vpcNetwork"] = transformedVpcNetwork
	}

	return transformed, nil
}

func expandDataplexTaskSparkInfrastructureSpecBatch(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedExecutorsCount, err := expandDataplexTaskSparkInfrastructureSpecBatchExecutorsCount(original["executors_count"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedExecutorsCount); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["executorsCount"] = transformedExecutorsCount
	}

	transformedMaxExecutorsCount, err := expandDataplexTaskSparkInfrastructureSpecBatchMaxExecutorsCount(original["max_executors_count"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedMaxExecutorsCount); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["maxExecutorsCount"] = transformedMaxExecutorsCount
	}

	return transformed, nil
}

func expandDataplexTaskSparkInfrastructureSpecBatchExecutorsCount(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexTaskSparkInfrastructureSpecBatchMaxExecutorsCount(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexTaskSparkInfrastructureSpecContainerImage(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedImage, err := expandDataplexTaskSparkInfrastructureSpecContainerImageImage(original["image"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedImage); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["image"] = transformedImage
	}

	transformedJavaJars, err := expandDataplexTaskSparkInfrastructureSpecContainerImageJavaJars(original["java_jars"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedJavaJars); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["javaJars"] = transformedJavaJars
	}

	transformedPythonPackages, err := expandDataplexTaskSparkInfrastructureSpecContainerImagePythonPackages(original["python_packages"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedPythonPackages); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["pythonPackages"] = transformedPythonPackages
	}

	transformedProperties, err := expandDataplexTaskSparkInfrastructureSpecContainerImageProperties(original["properties"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedProperties); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["properties"] = transformedProperties
	}

	return transformed, nil
}

func expandDataplexTaskSparkInfrastructureSpecContainerImageImage(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexTaskSparkInfrastructureSpecContainerImageJavaJars(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexTaskSparkInfrastructureSpecContainerImagePythonPackages(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexTaskSparkInfrastructureSpecContainerImageProperties(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (map[string]string, error) {
	if v == nil {
		return map[string]string{}, nil
	}
	m := make(map[string]string)
	for k, val := range v.(map[string]interface{}) {
		m[k] = val.(string)
	}
	return m, nil
}

func expandDataplexTaskSparkInfrastructureSpecVpcNetwork(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedNetworkTags, err := expandDataplexTaskSparkInfrastructureSpecVpcNetworkNetworkTags(original["network_tags"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedNetworkTags); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["networkTags"] = transformedNetworkTags
	}

	transformedNetwork, err := expandDataplexTaskSparkInfrastructureSpecVpcNetworkNetwork(original["network"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedNetwork); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["network"] = transformedNetwork
	}

	transformedSubNetwork, err := expandDataplexTaskSparkInfrastructureSpecVpcNetworkSubNetwork(original["sub_network"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedSubNetwork); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["subNetwork"] = transformedSubNetwork
	}

	return transformed, nil
}

func expandDataplexTaskSparkInfrastructureSpecVpcNetworkNetworkTags(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexTaskSparkInfrastructureSpecVpcNetworkNetwork(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexTaskSparkInfrastructureSpecVpcNetworkSubNetwork(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexTaskSparkMainJarFileUri(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexTaskSparkMainClass(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexTaskSparkPythonScriptFile(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexTaskSparkSqlScriptFile(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexTaskSparkSqlScript(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexTaskNotebook(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedNotebook, err := expandDataplexTaskNotebookNotebook(original["notebook"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedNotebook); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["notebook"] = transformedNotebook
	}

	transformedInfrastructureSpec, err := expandDataplexTaskNotebookInfrastructureSpec(original["infrastructure_spec"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedInfrastructureSpec); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["infrastructureSpec"] = transformedInfrastructureSpec
	}

	transformedFileUris, err := expandDataplexTaskNotebookFileUris(original["file_uris"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedFileUris); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["fileUris"] = transformedFileUris
	}

	transformedArchiveUris, err := expandDataplexTaskNotebookArchiveUris(original["archive_uris"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedArchiveUris); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["archiveUris"] = transformedArchiveUris
	}

	return transformed, nil
}

func expandDataplexTaskNotebookNotebook(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexTaskNotebookInfrastructureSpec(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedBatch, err := expandDataplexTaskNotebookInfrastructureSpecBatch(original["batch"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedBatch); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["batch"] = transformedBatch
	}

	transformedContainerImage, err := expandDataplexTaskNotebookInfrastructureSpecContainerImage(original["container_image"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedContainerImage); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["containerImage"] = transformedContainerImage
	}

	transformedVpcNetwork, err := expandDataplexTaskNotebookInfrastructureSpecVpcNetwork(original["vpc_network"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedVpcNetwork); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["vpcNetwork"] = transformedVpcNetwork
	}

	return transformed, nil
}

func expandDataplexTaskNotebookInfrastructureSpecBatch(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedExecutorsCount, err := expandDataplexTaskNotebookInfrastructureSpecBatchExecutorsCount(original["executors_count"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedExecutorsCount); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["executorsCount"] = transformedExecutorsCount
	}

	transformedMaxExecutorsCount, err := expandDataplexTaskNotebookInfrastructureSpecBatchMaxExecutorsCount(original["max_executors_count"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedMaxExecutorsCount); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["maxExecutorsCount"] = transformedMaxExecutorsCount
	}

	return transformed, nil
}

func expandDataplexTaskNotebookInfrastructureSpecBatchExecutorsCount(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexTaskNotebookInfrastructureSpecBatchMaxExecutorsCount(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexTaskNotebookInfrastructureSpecContainerImage(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedImage, err := expandDataplexTaskNotebookInfrastructureSpecContainerImageImage(original["image"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedImage); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["image"] = transformedImage
	}

	transformedJavaJars, err := expandDataplexTaskNotebookInfrastructureSpecContainerImageJavaJars(original["java_jars"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedJavaJars); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["javaJars"] = transformedJavaJars
	}

	transformedPythonPackages, err := expandDataplexTaskNotebookInfrastructureSpecContainerImagePythonPackages(original["python_packages"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedPythonPackages); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["pythonPackages"] = transformedPythonPackages
	}

	transformedProperties, err := expandDataplexTaskNotebookInfrastructureSpecContainerImageProperties(original["properties"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedProperties); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["properties"] = transformedProperties
	}

	return transformed, nil
}

func expandDataplexTaskNotebookInfrastructureSpecContainerImageImage(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexTaskNotebookInfrastructureSpecContainerImageJavaJars(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexTaskNotebookInfrastructureSpecContainerImagePythonPackages(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexTaskNotebookInfrastructureSpecContainerImageProperties(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (map[string]string, error) {
	if v == nil {
		return map[string]string{}, nil
	}
	m := make(map[string]string)
	for k, val := range v.(map[string]interface{}) {
		m[k] = val.(string)
	}
	return m, nil
}

func expandDataplexTaskNotebookInfrastructureSpecVpcNetwork(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedNetworkTags, err := expandDataplexTaskNotebookInfrastructureSpecVpcNetworkNetworkTags(original["network_tags"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedNetworkTags); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["networkTags"] = transformedNetworkTags
	}

	transformedNetwork, err := expandDataplexTaskNotebookInfrastructureSpecVpcNetworkNetwork(original["network"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedNetwork); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["network"] = transformedNetwork
	}

	transformedSubNetwork, err := expandDataplexTaskNotebookInfrastructureSpecVpcNetworkSubNetwork(original["sub_network"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedSubNetwork); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["subNetwork"] = transformedSubNetwork
	}

	return transformed, nil
}

func expandDataplexTaskNotebookInfrastructureSpecVpcNetworkNetworkTags(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexTaskNotebookInfrastructureSpecVpcNetworkNetwork(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexTaskNotebookInfrastructureSpecVpcNetworkSubNetwork(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexTaskNotebookFileUris(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexTaskNotebookArchiveUris(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexTaskEffectiveLabels(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (map[string]string, error) {
	if v == nil {
		return map[string]string{}, nil
	}
	m := make(map[string]string)
	for k, val := range v.(map[string]interface{}) {
		m[k] = val.(string)
	}
	return m, nil
}
