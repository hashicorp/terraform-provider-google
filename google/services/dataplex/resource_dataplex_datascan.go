// Copyright (c) HashiCorp, Inc.
// SPDX-License-Identifier: MPL-2.0

// ----------------------------------------------------------------------------
//
//     ***     AUTO GENERATED CODE    ***    Type: MMv1     ***
//
// ----------------------------------------------------------------------------
//
//     This file is automatically generated by Magic Modules and manual
//     changes will be clobbered when the file is regenerated.
//
//     Please read more about how to change this file in
//     .github/CONTRIBUTING.md.
//
// ----------------------------------------------------------------------------

package dataplex

import (
	"fmt"
	"log"
	"reflect"
	"strings"
	"time"

	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/schema"

	"github.com/hashicorp/terraform-provider-google/google/tpgresource"
	transport_tpg "github.com/hashicorp/terraform-provider-google/google/transport"
	"github.com/hashicorp/terraform-provider-google/google/verify"
)

func ResourceDataplexDatascan() *schema.Resource {
	return &schema.Resource{
		Create: resourceDataplexDatascanCreate,
		Read:   resourceDataplexDatascanRead,
		Update: resourceDataplexDatascanUpdate,
		Delete: resourceDataplexDatascanDelete,

		Importer: &schema.ResourceImporter{
			State: resourceDataplexDatascanImport,
		},

		Timeouts: &schema.ResourceTimeout{
			Create: schema.DefaultTimeout(5 * time.Minute),
			Update: schema.DefaultTimeout(5 * time.Minute),
			Delete: schema.DefaultTimeout(5 * time.Minute),
		},

		Schema: map[string]*schema.Schema{
			"data": {
				Type:        schema.TypeList,
				Required:    true,
				ForceNew:    true,
				Description: `The data source for DataScan.`,
				MaxItems:    1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"entity": {
							Type:         schema.TypeString,
							Optional:     true,
							ForceNew:     true,
							Description:  `The Dataplex entity that represents the data source(e.g. BigQuery table) for Datascan.`,
							ExactlyOneOf: []string{"data.0.entity", "data.0.resource"},
						},
						"resource": {
							Type:     schema.TypeString,
							Optional: true,
							ForceNew: true,
							Description: `The service-qualified full resource name of the cloud resource for a DataScan job to scan against. The field could be:
(Cloud Storage bucket for DataDiscoveryScan)BigQuery table of type "TABLE" for DataProfileScan/DataQualityScan.`,
							ExactlyOneOf: []string{"data.0.entity", "data.0.resource"},
						},
					},
				},
			},
			"data_scan_id": {
				Type:        schema.TypeString,
				Required:    true,
				ForceNew:    true,
				Description: `DataScan identifier. Must contain only lowercase letters, numbers and hyphens. Must start with a letter. Must end with a number or a letter.`,
			},
			"execution_spec": {
				Type:        schema.TypeList,
				Required:    true,
				Description: `DataScan execution settings.`,
				MaxItems:    1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"trigger": {
							Type:        schema.TypeList,
							Required:    true,
							Description: `Spec related to how often and when a scan should be triggered.`,
							MaxItems:    1,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"on_demand": {
										Type:        schema.TypeList,
										Optional:    true,
										Description: `The scan runs once via dataScans.run API.`,
										MaxItems:    1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{},
										},
										ExactlyOneOf: []string{"execution_spec.0.trigger.0.on_demand", "execution_spec.0.trigger.0.schedule"},
									},
									"schedule": {
										Type:        schema.TypeList,
										Optional:    true,
										Description: `The scan is scheduled to run periodically.`,
										MaxItems:    1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"cron": {
													Type:        schema.TypeString,
													Required:    true,
													Description: `Cron schedule for running scans periodically. This field is required for Schedule scans.`,
												},
											},
										},
										ExactlyOneOf: []string{"execution_spec.0.trigger.0.on_demand", "execution_spec.0.trigger.0.schedule"},
									},
								},
							},
						},
						"field": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: `The unnested field (of type Date or Timestamp) that contains values which monotonically increase over time. If not specified, a data scan will run for all data in the table.`,
						},
					},
				},
			},
			"location": {
				Type:        schema.TypeString,
				Required:    true,
				ForceNew:    true,
				Description: `The location where the data scan should reside.`,
			},
			"data_profile_spec": {
				Type:        schema.TypeList,
				Optional:    true,
				Description: `DataProfileScan related setting.`,
				MaxItems:    1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"row_filter": {
							Type:        schema.TypeString,
							Optional:    true,
							Description: `A filter applied to all rows in a single DataScan job. The filter needs to be a valid SQL expression for a WHERE clause in BigQuery standard SQL syntax. Example: col1 >= 0 AND col2 < 10`,
						},
						"sampling_percent": {
							Type:        schema.TypeFloat,
							Optional:    true,
							Description: `The percentage of the records to be selected from the dataset for DataScan.`,
						},
					},
				},
				ExactlyOneOf: []string{"data_quality_spec", "data_profile_spec"},
			},
			"data_quality_spec": {
				Type:        schema.TypeList,
				Optional:    true,
				Description: `DataQualityScan related setting.`,
				MaxItems:    1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"row_filter": {
							Type:        schema.TypeString,
							Optional:    true,
							Description: `A filter applied to all rows in a single DataScan job. The filter needs to be a valid SQL expression for a WHERE clause in BigQuery standard SQL syntax. Example: col1 >= 0 AND col2 < 10`,
						},
						"rules": {
							Type:        schema.TypeList,
							Optional:    true,
							Description: `The list of rules to evaluate against a data source. At least one rule is required.`,
							MinItems:    1,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"dimension": {
										Type:        schema.TypeString,
										Required:    true,
										Description: `The dimension a rule belongs to. Results are also aggregated at the dimension level. Supported dimensions are ["COMPLETENESS", "ACCURACY", "CONSISTENCY", "VALIDITY", "UNIQUENESS", "INTEGRITY"]`,
									},
									"column": {
										Type:        schema.TypeString,
										Optional:    true,
										Description: `The unnested column which this rule is evaluated against.`,
									},
									"ignore_null": {
										Type:        schema.TypeBool,
										Optional:    true,
										Description: `Rows with null values will automatically fail a rule, unless ignoreNull is true. In that case, such null rows are trivially considered passing. Only applicable to ColumnMap rules.`,
									},
									"non_null_expectation": {
										Type:        schema.TypeList,
										Optional:    true,
										Description: `ColumnMap rule which evaluates whether each column value is null.`,
										MaxItems:    1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{},
										},
									},
									"range_expectation": {
										Type:        schema.TypeList,
										Optional:    true,
										Description: `ColumnMap rule which evaluates whether each column value lies between a specified range.`,
										MaxItems:    1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"max_value": {
													Type:        schema.TypeString,
													Optional:    true,
													Description: `The maximum column value allowed for a row to pass this validation. At least one of minValue and maxValue need to be provided.`,
												},
												"min_value": {
													Type:        schema.TypeString,
													Optional:    true,
													Description: `The minimum column value allowed for a row to pass this validation. At least one of minValue and maxValue need to be provided.`,
												},
												"strict_max_enabled": {
													Type:     schema.TypeBool,
													Optional: true,
													Description: `Whether each value needs to be strictly lesser than ('<') the maximum, or if equality is allowed.
Only relevant if a maxValue has been defined. Default = false.`,
													Default: false,
												},
												"strict_min_enabled": {
													Type:     schema.TypeBool,
													Optional: true,
													Description: `Whether each value needs to be strictly greater than ('>') the minimum, or if equality is allowed.
Only relevant if a minValue has been defined. Default = false.`,
													Default: false,
												},
											},
										},
									},
									"regex_expectation": {
										Type:        schema.TypeList,
										Optional:    true,
										Description: `ColumnMap rule which evaluates whether each column value matches a specified regex.`,
										MaxItems:    1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"regex": {
													Type:        schema.TypeString,
													Required:    true,
													Description: `A regular expression the column value is expected to match.`,
												},
											},
										},
									},
									"row_condition_expectation": {
										Type:        schema.TypeList,
										Optional:    true,
										Description: `Table rule which evaluates whether each row passes the specified condition.`,
										MaxItems:    1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"sql_expression": {
													Type:        schema.TypeString,
													Required:    true,
													Description: `The SQL expression.`,
												},
											},
										},
									},
									"set_expectation": {
										Type:        schema.TypeList,
										Optional:    true,
										Description: `ColumnMap rule which evaluates whether each column value is contained by a specified set.`,
										MaxItems:    1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"values": {
													Type:        schema.TypeList,
													Required:    true,
													Description: `Expected values for the column value.`,
													Elem: &schema.Schema{
														Type: schema.TypeString,
													},
												},
											},
										},
									},
									"statistic_range_expectation": {
										Type:        schema.TypeList,
										Optional:    true,
										Description: `ColumnAggregate rule which evaluates whether the column aggregate statistic lies between a specified range.`,
										MaxItems:    1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"statistic": {
													Type:         schema.TypeString,
													Required:     true,
													ValidateFunc: verify.ValidateEnum([]string{"STATISTIC_UNDEFINED", "MEAN", "MIN", "MAX"}),
													Description:  `column statistics. Possible values: ["STATISTIC_UNDEFINED", "MEAN", "MIN", "MAX"]`,
												},
												"max_value": {
													Type:     schema.TypeString,
													Optional: true,
													Description: `The maximum column statistic value allowed for a row to pass this validation.
At least one of minValue and maxValue need to be provided.`,
												},
												"min_value": {
													Type:     schema.TypeString,
													Optional: true,
													Description: `The minimum column statistic value allowed for a row to pass this validation.
At least one of minValue and maxValue need to be provided.`,
												},
												"strict_max_enabled": {
													Type:     schema.TypeBool,
													Optional: true,
													Description: `Whether column statistic needs to be strictly lesser than ('<') the maximum, or if equality is allowed.
Only relevant if a maxValue has been defined. Default = false.`,
													Default: false,
												},
												"strict_min_enabled": {
													Type:     schema.TypeBool,
													Optional: true,
													Description: `Whether column statistic needs to be strictly greater than ('>') the minimum, or if equality is allowed.
Only relevant if a minValue has been defined. Default = false.`,
													Default: false,
												},
											},
										},
									},
									"table_condition_expectation": {
										Type:        schema.TypeList,
										Optional:    true,
										Description: `Table rule which evaluates whether the provided expression is true.`,
										MaxItems:    1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"sql_expression": {
													Type:        schema.TypeString,
													Required:    true,
													Description: `The SQL expression.`,
												},
											},
										},
									},
									"threshold": {
										Type:        schema.TypeFloat,
										Optional:    true,
										Description: `The minimum ratio of passing_rows / total_rows required to pass this rule, with a range of [0.0, 1.0]. 0 indicates default value (i.e. 1.0).`,
									},
									"uniqueness_expectation": {
										Type:        schema.TypeList,
										Optional:    true,
										Description: `ColumnAggregate rule which evaluates whether the column has duplicates.`,
										MaxItems:    1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{},
										},
									},
								},
							},
						},
						"sampling_percent": {
							Type:        schema.TypeFloat,
							Optional:    true,
							Description: `The percentage of the records to be selected from the dataset for DataScan.`,
						},
					},
				},
				ExactlyOneOf: []string{"data_quality_spec", "data_profile_spec"},
			},
			"description": {
				Type:        schema.TypeString,
				Optional:    true,
				Description: `Description of the scan.`,
			},
			"display_name": {
				Type:        schema.TypeString,
				Optional:    true,
				Description: `User friendly display name.`,
			},
			"labels": {
				Type:        schema.TypeMap,
				Optional:    true,
				Description: `User-defined labels for the scan. A list of key->value pairs.`,
				Elem:        &schema.Schema{Type: schema.TypeString},
			},
			"create_time": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `The time when the scan was created.`,
			},
			"data_profile_result": {
				Type:        schema.TypeList,
				Computed:    true,
				Description: `The result of the data profile scan.`,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"row_count": {
							Type:        schema.TypeString,
							Optional:    true,
							Description: `The count of rows scanned.`,
						},
						"profile": {
							Type:        schema.TypeList,
							Computed:    true,
							Description: `The profile information per field.`,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"fields": {
										Type:        schema.TypeList,
										Optional:    true,
										Description: `List of fields with structural and profile information for each field.`,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"mode": {
													Type:     schema.TypeString,
													Optional: true,
													Description: `The mode of the field. Possible values include:
1. REQUIRED, if it is a required field.
2. NULLABLE, if it is an optional field.
3. REPEATED, if it is a repeated field.`,
												},
												"name": {
													Type:        schema.TypeString,
													Optional:    true,
													Description: `The name of the field.`,
												},
												"profile": {
													Type:        schema.TypeList,
													Optional:    true,
													Description: `Profile information for the corresponding field.`,
													MaxItems:    1,
													Elem: &schema.Resource{
														Schema: map[string]*schema.Schema{
															"distinct_ratio": {
																Type:        schema.TypeInt,
																Optional:    true,
																Description: `Ratio of rows with distinct values against total scanned rows. Not available for complex non-groupable field type RECORD and fields with REPEATABLE mode.`,
															},
															"top_n_values": {
																Type:        schema.TypeList,
																Optional:    true,
																Description: `The list of top N non-null values and number of times they occur in the scanned data. N is 10 or equal to the number of distinct values in the field, whichever is smaller. Not available for complex non-groupable field type RECORD and fields with REPEATABLE mode.`,
																MaxItems:    1,
																Elem: &schema.Resource{
																	Schema: map[string]*schema.Schema{
																		"count": {
																			Type:        schema.TypeString,
																			Optional:    true,
																			Description: `Count of the corresponding value in the scanned data.`,
																		},
																		"value": {
																			Type:        schema.TypeString,
																			Optional:    true,
																			Description: `String value of a top N non-null value.`,
																		},
																	},
																},
															},
															"double_profile": {
																Type:        schema.TypeList,
																Computed:    true,
																Description: `Double type field information.`,
																Elem: &schema.Resource{
																	Schema: map[string]*schema.Schema{
																		"average": {
																			Type:        schema.TypeInt,
																			Optional:    true,
																			Description: `Average of non-null values in the scanned data. NaN, if the field has a NaN.`,
																		},
																		"max": {
																			Type:        schema.TypeString,
																			Optional:    true,
																			Description: `Maximum of non-null values in the scanned data. NaN, if the field has a NaN.`,
																		},
																		"min": {
																			Type:        schema.TypeString,
																			Optional:    true,
																			Description: `Minimum of non-null values in the scanned data. NaN, if the field has a NaN.`,
																		},
																		"quartiles": {
																			Type:        schema.TypeString,
																			Optional:    true,
																			Description: `A quartile divides the number of data points into four parts, or quarters, of more-or-less equal size. Three main quartiles used are: The first quartile (Q1) splits off the lowest 25% of data from the highest 75%. It is also known as the lower or 25th empirical quartile, as 25% of the data is below this point. The second quartile (Q2) is the median of a data set. So, 50% of the data lies below this point. The third quartile (Q3) splits off the highest 25% of data from the lowest 75%. It is known as the upper or 75th empirical quartile, as 75% of the data lies below this point. Here, the quartiles is provided as an ordered list of quartile values for the scanned data, occurring in order Q1, median, Q3.`,
																		},
																		"standard_deviation": {
																			Type:        schema.TypeInt,
																			Optional:    true,
																			Description: `Standard deviation of non-null values in the scanned data. NaN, if the field has a NaN.`,
																		},
																	},
																},
															},
															"integer_profile": {
																Type:        schema.TypeList,
																Computed:    true,
																Description: `Integer type field information.`,
																Elem: &schema.Resource{
																	Schema: map[string]*schema.Schema{
																		"average": {
																			Type:        schema.TypeInt,
																			Optional:    true,
																			Description: `Average of non-null values in the scanned data. NaN, if the field has a NaN.`,
																		},
																		"max": {
																			Type:        schema.TypeString,
																			Optional:    true,
																			Description: `Maximum of non-null values in the scanned data. NaN, if the field has a NaN.`,
																		},
																		"min": {
																			Type:        schema.TypeString,
																			Optional:    true,
																			Description: `Minimum of non-null values in the scanned data. NaN, if the field has a NaN.`,
																		},
																		"quartiles": {
																			Type:        schema.TypeString,
																			Optional:    true,
																			Description: `A quartile divides the number of data points into four parts, or quarters, of more-or-less equal size. Three main quartiles used are: The first quartile (Q1) splits off the lowest 25% of data from the highest 75%. It is also known as the lower or 25th empirical quartile, as 25% of the data is below this point. The second quartile (Q2) is the median of a data set. So, 50% of the data lies below this point. The third quartile (Q3) splits off the highest 25% of data from the lowest 75%. It is known as the upper or 75th empirical quartile, as 75% of the data lies below this point. Here, the quartiles is provided as an ordered list of quartile values for the scanned data, occurring in order Q1, median, Q3.`,
																		},
																		"standard_deviation": {
																			Type:        schema.TypeInt,
																			Optional:    true,
																			Description: `Standard deviation of non-null values in the scanned data. NaN, if the field has a NaN.`,
																		},
																	},
																},
															},
															"null_ratio": {
																Type:        schema.TypeInt,
																Computed:    true,
																Description: `Ratio of rows with null value against total scanned rows.`,
															},
															"string_profile": {
																Type:        schema.TypeList,
																Computed:    true,
																Description: `String type field information.`,
																Elem: &schema.Resource{
																	Schema: map[string]*schema.Schema{
																		"average_length": {
																			Type:        schema.TypeInt,
																			Optional:    true,
																			Description: `Average length of non-null values in the scanned data.`,
																		},
																		"max_length": {
																			Type:        schema.TypeString,
																			Optional:    true,
																			Description: `Maximum length of non-null values in the scanned data.`,
																		},
																		"min_length": {
																			Type:        schema.TypeString,
																			Optional:    true,
																			Description: `Minimum length of non-null values in the scanned data.`,
																		},
																	},
																},
															},
														},
													},
												},
												"type": {
													Type:        schema.TypeString,
													Optional:    true,
													Description: `The field data type.`,
												},
											},
										},
									},
								},
							},
						},
						"scanned_data": {
							Type:        schema.TypeList,
							Computed:    true,
							Description: `The data scanned for this result.`,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"incremental_field": {
										Type:        schema.TypeList,
										Optional:    true,
										Description: `The range denoted by values of an incremental field`,
										MaxItems:    1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"end": {
													Type:        schema.TypeString,
													Optional:    true,
													Description: `Value that marks the end of the range.`,
												},
												"field": {
													Type:        schema.TypeString,
													Optional:    true,
													Description: `The field that contains values which monotonically increases over time (e.g. a timestamp column).`,
												},
												"start": {
													Type:        schema.TypeString,
													Optional:    true,
													Description: `Value that marks the start of the range.`,
												},
											},
										},
									},
								},
							},
						},
					},
				},
			},
			"data_quality_result": {
				Type:        schema.TypeList,
				Computed:    true,
				Description: `The result of the data quality scan.`,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"dimensions": {
							Type:        schema.TypeList,
							Optional:    true,
							Description: `A list of results at the dimension level.`,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"passed": {
										Type:        schema.TypeBool,
										Optional:    true,
										Description: `Whether the dimension passed or failed.`,
									},
								},
							},
						},
						"passed": {
							Type:        schema.TypeBool,
							Computed:    true,
							Description: `Overall data quality result -- true if all rules passed.`,
						},
						"row_count": {
							Type:        schema.TypeString,
							Computed:    true,
							Description: `The count of rows processed.`,
						},
						"rules": {
							Type:        schema.TypeList,
							Computed:    true,
							Description: `A list of all the rules in a job, and their results.`,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"evaluated_count": {
										Type:     schema.TypeString,
										Computed: true,
										Description: `The number of rows a rule was evaluated against. This field is only valid for ColumnMap type rules.
Evaluated count can be configured to either
1. include all rows (default) - with null rows automatically failing rule evaluation, or
2. exclude null rows from the evaluatedCount, by setting ignore_nulls = true.`,
									},
									"failing_rows_query": {
										Type:        schema.TypeString,
										Computed:    true,
										Description: `The query to find rows that did not pass this rule. Only applies to ColumnMap and RowCondition rules.`,
									},
									"null_count": {
										Type:        schema.TypeString,
										Computed:    true,
										Description: `The number of rows with null values in the specified column.`,
									},
									"pass_ratio": {
										Type:        schema.TypeInt,
										Computed:    true,
										Description: `The ratio of passedCount / evaluatedCount. This field is only valid for ColumnMap type rules.`,
									},
									"passed": {
										Type:        schema.TypeBool,
										Computed:    true,
										Description: `Whether the rule passed or failed.`,
									},
									"passed_count": {
										Type:        schema.TypeString,
										Computed:    true,
										Description: `The number of rows which passed a rule evaluation. This field is only valid for ColumnMap type rules.`,
									},
									"rule": {
										Type:        schema.TypeList,
										Computed:    true,
										Description: `The rule specified in the DataQualitySpec, as is.`,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"column": {
													Type:        schema.TypeString,
													Optional:    true,
													Description: `The unnested column which this rule is evaluated against.`,
												},
												"dimension": {
													Type:        schema.TypeString,
													Optional:    true,
													Description: `The dimension a rule belongs to. Results are also aggregated at the dimension level. Supported dimensions are ["COMPLETENESS", "ACCURACY", "CONSISTENCY", "VALIDITY", "UNIQUENESS", "INTEGRITY"]`,
												},
												"ignore_null": {
													Type:        schema.TypeBool,
													Optional:    true,
													Description: `Rows with null values will automatically fail a rule, unless ignoreNull is true. In that case, such null rows are trivially considered passing. Only applicable to ColumnMap rules.`,
												},
												"threshold": {
													Type:        schema.TypeInt,
													Optional:    true,
													Description: `The minimum ratio of passing_rows / total_rows required to pass this rule, with a range of [0.0, 1.0]. 0 indicates default value (i.e. 1.0).`,
												},
												"non_null_expectation": {
													Type:        schema.TypeList,
													Computed:    true,
													Description: `ColumnMap rule which evaluates whether each column value is null.`,
													Elem: &schema.Resource{
														Schema: map[string]*schema.Schema{},
													},
												},
												"range_expectation": {
													Type:        schema.TypeList,
													Computed:    true,
													Description: `ColumnMap rule which evaluates whether each column value lies between a specified range.`,
													Elem: &schema.Resource{
														Schema: map[string]*schema.Schema{
															"max_value": {
																Type:        schema.TypeString,
																Optional:    true,
																Description: `The maximum column value allowed for a row to pass this validation. At least one of minValue and maxValue need to be provided.`,
															},
															"min_value": {
																Type:        schema.TypeString,
																Optional:    true,
																Description: `The minimum column value allowed for a row to pass this validation. At least one of minValue and maxValue need to be provided.`,
															},
															"strict_max_enabled": {
																Type:     schema.TypeBool,
																Optional: true,
																Description: `Whether each value needs to be strictly lesser than ('<') the maximum, or if equality is allowed.
Only relevant if a maxValue has been defined. Default = false.`,
																Default: false,
															},
															"strict_min_enabled": {
																Type:     schema.TypeBool,
																Optional: true,
																Description: `Whether each value needs to be strictly greater than ('>') the minimum, or if equality is allowed.
Only relevant if a minValue has been defined. Default = false.`,
																Default: false,
															},
														},
													},
												},
												"regex_expectation": {
													Type:        schema.TypeList,
													Computed:    true,
													Description: `ColumnMap rule which evaluates whether each column value matches a specified regex.`,
													Elem: &schema.Resource{
														Schema: map[string]*schema.Schema{
															"regex": {
																Type:        schema.TypeString,
																Optional:    true,
																Description: `A regular expression the column value is expected to match.`,
															},
														},
													},
												},
												"row_condition_expectation": {
													Type:        schema.TypeList,
													Computed:    true,
													Description: `Table rule which evaluates whether each row passes the specified condition.`,
													Elem: &schema.Resource{
														Schema: map[string]*schema.Schema{
															"sql_expression": {
																Type:        schema.TypeString,
																Optional:    true,
																Description: `The SQL expression.`,
															},
														},
													},
												},
												"set_expectation": {
													Type:        schema.TypeList,
													Computed:    true,
													Description: `ColumnMap rule which evaluates whether each column value is contained by a specified set.`,
													Elem: &schema.Resource{
														Schema: map[string]*schema.Schema{
															"values": {
																Type:        schema.TypeList,
																Optional:    true,
																Description: `Expected values for the column value.`,
																Elem: &schema.Schema{
																	Type: schema.TypeString,
																},
															},
														},
													},
												},
												"statistic_range_expectation": {
													Type:        schema.TypeList,
													Computed:    true,
													Description: `ColumnAggregate rule which evaluates whether the column aggregate statistic lies between a specified range.`,
													Elem: &schema.Resource{
														Schema: map[string]*schema.Schema{
															"max_value": {
																Type:     schema.TypeString,
																Optional: true,
																Description: `The maximum column statistic value allowed for a row to pass this validation.
At least one of minValue and maxValue need to be provided.`,
															},
															"min_value": {
																Type:     schema.TypeString,
																Optional: true,
																Description: `The minimum column statistic value allowed for a row to pass this validation.
At least one of minValue and maxValue need to be provided.`,
															},
															"statistic": {
																Type:         schema.TypeString,
																Optional:     true,
																ValidateFunc: verify.ValidateEnum([]string{"STATISTIC_UNDEFINED", "MEAN", "MIN", "MAX", ""}),
																Description:  `column statistics. Possible values: ["STATISTIC_UNDEFINED", "MEAN", "MIN", "MAX"]`,
															},
															"strict_max_enabled": {
																Type:     schema.TypeBool,
																Optional: true,
																Description: `Whether column statistic needs to be strictly lesser than ('<') the maximum, or if equality is allowed.
Only relevant if a maxValue has been defined. Default = false.`,
															},
															"strict_min_enabled": {
																Type:     schema.TypeBool,
																Optional: true,
																Description: `Whether column statistic needs to be strictly greater than ('>') the minimum, or if equality is allowed.
Only relevant if a minValue has been defined. Default = false.`,
															},
														},
													},
												},
												"table_condition_expectation": {
													Type:        schema.TypeList,
													Computed:    true,
													Description: `Table rule which evaluates whether the provided expression is true.`,
													Elem: &schema.Resource{
														Schema: map[string]*schema.Schema{
															"sql_expression": {
																Type:        schema.TypeString,
																Optional:    true,
																Description: `The SQL expression.`,
															},
														},
													},
												},
												"uniqueness_expectation": {
													Type:        schema.TypeList,
													Computed:    true,
													Description: `ColumnAggregate rule which evaluates whether the column has duplicates.`,
													Elem: &schema.Resource{
														Schema: map[string]*schema.Schema{},
													},
												},
											},
										},
									},
								},
							},
						},
						"scanned_data": {
							Type:        schema.TypeList,
							Computed:    true,
							Description: `The data scanned for this result.`,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"incremental_field": {
										Type:        schema.TypeList,
										Optional:    true,
										Description: `The range denoted by values of an incremental field`,
										MaxItems:    1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"end": {
													Type:        schema.TypeString,
													Optional:    true,
													Description: `Value that marks the end of the range.`,
												},
												"field": {
													Type:        schema.TypeString,
													Optional:    true,
													Description: `The field that contains values which monotonically increases over time (e.g. a timestamp column).`,
												},
												"start": {
													Type:        schema.TypeString,
													Optional:    true,
													Description: `Value that marks the start of the range.`,
												},
											},
										},
									},
								},
							},
						},
					},
				},
			},
			"execution_status": {
				Type:        schema.TypeList,
				Computed:    true,
				Description: `Status of the data scan execution.`,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"latest_job_end_time": {
							Type:        schema.TypeString,
							Computed:    true,
							Description: `The time when the latest DataScanJob started.`,
						},
						"latest_job_start_time": {
							Type:        schema.TypeString,
							Computed:    true,
							Description: `The time when the latest DataScanJob ended.`,
						},
					},
				},
			},
			"name": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `The relative resource name of the scan, of the form: projects/{project}/locations/{locationId}/dataScans/{datascan_id}, where project refers to a project_id or project_number and locationId refers to a GCP region.`,
			},
			"state": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `Current state of the DataScan.`,
			},
			"type": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `The type of DataScan.`,
			},
			"uid": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `System generated globally unique ID for the scan. This ID will be different if the scan is deleted and re-created with the same name.`,
			},
			"update_time": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `The time when the scan was last updated.`,
			},
			"project": {
				Type:     schema.TypeString,
				Optional: true,
				Computed: true,
				ForceNew: true,
			},
		},
		UseJSONNumber: true,
	}
}

func resourceDataplexDatascanCreate(d *schema.ResourceData, meta interface{}) error {
	config := meta.(*transport_tpg.Config)
	userAgent, err := tpgresource.GenerateUserAgentString(d, config.UserAgent)
	if err != nil {
		return err
	}

	obj := make(map[string]interface{})
	descriptionProp, err := expandDataplexDatascanDescription(d.Get("description"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("description"); !tpgresource.IsEmptyValue(reflect.ValueOf(descriptionProp)) && (ok || !reflect.DeepEqual(v, descriptionProp)) {
		obj["description"] = descriptionProp
	}
	displayNameProp, err := expandDataplexDatascanDisplayName(d.Get("display_name"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("display_name"); !tpgresource.IsEmptyValue(reflect.ValueOf(displayNameProp)) && (ok || !reflect.DeepEqual(v, displayNameProp)) {
		obj["displayName"] = displayNameProp
	}
	labelsProp, err := expandDataplexDatascanLabels(d.Get("labels"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("labels"); !tpgresource.IsEmptyValue(reflect.ValueOf(labelsProp)) && (ok || !reflect.DeepEqual(v, labelsProp)) {
		obj["labels"] = labelsProp
	}
	dataProp, err := expandDataplexDatascanData(d.Get("data"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("data"); !tpgresource.IsEmptyValue(reflect.ValueOf(dataProp)) && (ok || !reflect.DeepEqual(v, dataProp)) {
		obj["data"] = dataProp
	}
	executionSpecProp, err := expandDataplexDatascanExecutionSpec(d.Get("execution_spec"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("execution_spec"); !tpgresource.IsEmptyValue(reflect.ValueOf(executionSpecProp)) && (ok || !reflect.DeepEqual(v, executionSpecProp)) {
		obj["executionSpec"] = executionSpecProp
	}
	dataQualitySpecProp, err := expandDataplexDatascanDataQualitySpec(d.Get("data_quality_spec"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("data_quality_spec"); !tpgresource.IsEmptyValue(reflect.ValueOf(dataQualitySpecProp)) && (ok || !reflect.DeepEqual(v, dataQualitySpecProp)) {
		obj["dataQualitySpec"] = dataQualitySpecProp
	}
	dataProfileSpecProp, err := expandDataplexDatascanDataProfileSpec(d.Get("data_profile_spec"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("data_profile_spec"); ok || !reflect.DeepEqual(v, dataProfileSpecProp) {
		obj["dataProfileSpec"] = dataProfileSpecProp
	}

	url, err := tpgresource.ReplaceVars(d, config, "{{DataplexBasePath}}projects/{{project}}/locations/{{location}}/dataScans?dataScanId={{data_scan_id}}")
	if err != nil {
		return err
	}

	log.Printf("[DEBUG] Creating new Datascan: %#v", obj)
	billingProject := ""

	project, err := tpgresource.GetProject(d, config)
	if err != nil {
		return fmt.Errorf("Error fetching project for Datascan: %s", err)
	}
	billingProject = project

	// err == nil indicates that the billing_project value was found
	if bp, err := tpgresource.GetBillingProject(d, config); err == nil {
		billingProject = bp
	}

	res, err := transport_tpg.SendRequest(transport_tpg.SendRequestOptions{
		Config:    config,
		Method:    "POST",
		Project:   billingProject,
		RawURL:    url,
		UserAgent: userAgent,
		Body:      obj,
		Timeout:   d.Timeout(schema.TimeoutCreate),
	})
	if err != nil {
		return fmt.Errorf("Error creating Datascan: %s", err)
	}

	// Store the ID now
	id, err := tpgresource.ReplaceVars(d, config, "projects/{{project}}/locations/{{location}}/dataScans/{{data_scan_id}}")
	if err != nil {
		return fmt.Errorf("Error constructing id: %s", err)
	}
	d.SetId(id)

	err = DataplexOperationWaitTime(
		config, res, project, "Creating Datascan", userAgent,
		d.Timeout(schema.TimeoutCreate))

	if err != nil {
		// The resource didn't actually create
		d.SetId("")
		return fmt.Errorf("Error waiting to create Datascan: %s", err)
	}

	log.Printf("[DEBUG] Finished creating Datascan %q: %#v", d.Id(), res)

	return resourceDataplexDatascanRead(d, meta)
}

func resourceDataplexDatascanRead(d *schema.ResourceData, meta interface{}) error {
	config := meta.(*transport_tpg.Config)
	userAgent, err := tpgresource.GenerateUserAgentString(d, config.UserAgent)
	if err != nil {
		return err
	}

	url, err := tpgresource.ReplaceVars(d, config, "{{DataplexBasePath}}projects/{{project}}/locations/{{location}}/dataScans/{{data_scan_id}}?view=FULL")
	if err != nil {
		return err
	}

	billingProject := ""

	project, err := tpgresource.GetProject(d, config)
	if err != nil {
		return fmt.Errorf("Error fetching project for Datascan: %s", err)
	}
	billingProject = project

	// err == nil indicates that the billing_project value was found
	if bp, err := tpgresource.GetBillingProject(d, config); err == nil {
		billingProject = bp
	}

	res, err := transport_tpg.SendRequest(transport_tpg.SendRequestOptions{
		Config:    config,
		Method:    "GET",
		Project:   billingProject,
		RawURL:    url,
		UserAgent: userAgent,
	})
	if err != nil {
		return transport_tpg.HandleNotFoundError(err, d, fmt.Sprintf("DataplexDatascan %q", d.Id()))
	}

	if err := d.Set("project", project); err != nil {
		return fmt.Errorf("Error reading Datascan: %s", err)
	}

	if err := d.Set("name", flattenDataplexDatascanName(res["name"], d, config)); err != nil {
		return fmt.Errorf("Error reading Datascan: %s", err)
	}
	if err := d.Set("uid", flattenDataplexDatascanUid(res["uid"], d, config)); err != nil {
		return fmt.Errorf("Error reading Datascan: %s", err)
	}
	if err := d.Set("description", flattenDataplexDatascanDescription(res["description"], d, config)); err != nil {
		return fmt.Errorf("Error reading Datascan: %s", err)
	}
	if err := d.Set("display_name", flattenDataplexDatascanDisplayName(res["displayName"], d, config)); err != nil {
		return fmt.Errorf("Error reading Datascan: %s", err)
	}
	if err := d.Set("labels", flattenDataplexDatascanLabels(res["labels"], d, config)); err != nil {
		return fmt.Errorf("Error reading Datascan: %s", err)
	}
	if err := d.Set("state", flattenDataplexDatascanState(res["state"], d, config)); err != nil {
		return fmt.Errorf("Error reading Datascan: %s", err)
	}
	if err := d.Set("create_time", flattenDataplexDatascanCreateTime(res["createTime"], d, config)); err != nil {
		return fmt.Errorf("Error reading Datascan: %s", err)
	}
	if err := d.Set("update_time", flattenDataplexDatascanUpdateTime(res["updateTime"], d, config)); err != nil {
		return fmt.Errorf("Error reading Datascan: %s", err)
	}
	if err := d.Set("data", flattenDataplexDatascanData(res["data"], d, config)); err != nil {
		return fmt.Errorf("Error reading Datascan: %s", err)
	}
	if err := d.Set("execution_spec", flattenDataplexDatascanExecutionSpec(res["executionSpec"], d, config)); err != nil {
		return fmt.Errorf("Error reading Datascan: %s", err)
	}
	if err := d.Set("execution_status", flattenDataplexDatascanExecutionStatus(res["executionStatus"], d, config)); err != nil {
		return fmt.Errorf("Error reading Datascan: %s", err)
	}
	if err := d.Set("type", flattenDataplexDatascanType(res["type"], d, config)); err != nil {
		return fmt.Errorf("Error reading Datascan: %s", err)
	}
	if err := d.Set("data_quality_spec", flattenDataplexDatascanDataQualitySpec(res["dataQualitySpec"], d, config)); err != nil {
		return fmt.Errorf("Error reading Datascan: %s", err)
	}
	if err := d.Set("data_profile_spec", flattenDataplexDatascanDataProfileSpec(res["dataProfileSpec"], d, config)); err != nil {
		return fmt.Errorf("Error reading Datascan: %s", err)
	}
	if err := d.Set("data_quality_result", flattenDataplexDatascanDataQualityResult(res["dataQualityResult"], d, config)); err != nil {
		return fmt.Errorf("Error reading Datascan: %s", err)
	}
	if err := d.Set("data_profile_result", flattenDataplexDatascanDataProfileResult(res["dataProfileResult"], d, config)); err != nil {
		return fmt.Errorf("Error reading Datascan: %s", err)
	}

	return nil
}

func resourceDataplexDatascanUpdate(d *schema.ResourceData, meta interface{}) error {
	config := meta.(*transport_tpg.Config)
	userAgent, err := tpgresource.GenerateUserAgentString(d, config.UserAgent)
	if err != nil {
		return err
	}

	billingProject := ""

	project, err := tpgresource.GetProject(d, config)
	if err != nil {
		return fmt.Errorf("Error fetching project for Datascan: %s", err)
	}
	billingProject = project

	obj := make(map[string]interface{})
	descriptionProp, err := expandDataplexDatascanDescription(d.Get("description"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("description"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, descriptionProp)) {
		obj["description"] = descriptionProp
	}
	displayNameProp, err := expandDataplexDatascanDisplayName(d.Get("display_name"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("display_name"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, displayNameProp)) {
		obj["displayName"] = displayNameProp
	}
	labelsProp, err := expandDataplexDatascanLabels(d.Get("labels"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("labels"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, labelsProp)) {
		obj["labels"] = labelsProp
	}
	executionSpecProp, err := expandDataplexDatascanExecutionSpec(d.Get("execution_spec"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("execution_spec"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, executionSpecProp)) {
		obj["executionSpec"] = executionSpecProp
	}
	dataQualitySpecProp, err := expandDataplexDatascanDataQualitySpec(d.Get("data_quality_spec"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("data_quality_spec"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, dataQualitySpecProp)) {
		obj["dataQualitySpec"] = dataQualitySpecProp
	}
	dataProfileSpecProp, err := expandDataplexDatascanDataProfileSpec(d.Get("data_profile_spec"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("data_profile_spec"); ok || !reflect.DeepEqual(v, dataProfileSpecProp) {
		obj["dataProfileSpec"] = dataProfileSpecProp
	}

	url, err := tpgresource.ReplaceVars(d, config, "{{DataplexBasePath}}projects/{{project}}/locations/{{location}}/dataScans/{{data_scan_id}}")
	if err != nil {
		return err
	}

	log.Printf("[DEBUG] Updating Datascan %q: %#v", d.Id(), obj)
	updateMask := []string{}

	if d.HasChange("description") {
		updateMask = append(updateMask, "description")
	}

	if d.HasChange("display_name") {
		updateMask = append(updateMask, "displayName")
	}

	if d.HasChange("labels") {
		updateMask = append(updateMask, "labels")
	}

	if d.HasChange("execution_spec") {
		updateMask = append(updateMask, "executionSpec")
	}

	if d.HasChange("data_quality_spec") {
		updateMask = append(updateMask, "dataQualitySpec")
	}

	if d.HasChange("data_profile_spec") {
		updateMask = append(updateMask, "dataProfileSpec")
	}
	// updateMask is a URL parameter but not present in the schema, so ReplaceVars
	// won't set it
	url, err = transport_tpg.AddQueryParams(url, map[string]string{"updateMask": strings.Join(updateMask, ",")})
	if err != nil {
		return err
	}

	// err == nil indicates that the billing_project value was found
	if bp, err := tpgresource.GetBillingProject(d, config); err == nil {
		billingProject = bp
	}

	res, err := transport_tpg.SendRequest(transport_tpg.SendRequestOptions{
		Config:    config,
		Method:    "PATCH",
		Project:   billingProject,
		RawURL:    url,
		UserAgent: userAgent,
		Body:      obj,
		Timeout:   d.Timeout(schema.TimeoutUpdate),
	})

	if err != nil {
		return fmt.Errorf("Error updating Datascan %q: %s", d.Id(), err)
	} else {
		log.Printf("[DEBUG] Finished updating Datascan %q: %#v", d.Id(), res)
	}

	err = DataplexOperationWaitTime(
		config, res, project, "Updating Datascan", userAgent,
		d.Timeout(schema.TimeoutUpdate))

	if err != nil {
		return err
	}

	return resourceDataplexDatascanRead(d, meta)
}

func resourceDataplexDatascanDelete(d *schema.ResourceData, meta interface{}) error {
	config := meta.(*transport_tpg.Config)
	userAgent, err := tpgresource.GenerateUserAgentString(d, config.UserAgent)
	if err != nil {
		return err
	}

	billingProject := ""

	project, err := tpgresource.GetProject(d, config)
	if err != nil {
		return fmt.Errorf("Error fetching project for Datascan: %s", err)
	}
	billingProject = project

	url, err := tpgresource.ReplaceVars(d, config, "{{DataplexBasePath}}projects/{{project}}/locations/{{location}}/dataScans/{{data_scan_id}}")
	if err != nil {
		return err
	}

	var obj map[string]interface{}
	log.Printf("[DEBUG] Deleting Datascan %q", d.Id())

	// err == nil indicates that the billing_project value was found
	if bp, err := tpgresource.GetBillingProject(d, config); err == nil {
		billingProject = bp
	}

	res, err := transport_tpg.SendRequest(transport_tpg.SendRequestOptions{
		Config:    config,
		Method:    "DELETE",
		Project:   billingProject,
		RawURL:    url,
		UserAgent: userAgent,
		Body:      obj,
		Timeout:   d.Timeout(schema.TimeoutDelete),
	})
	if err != nil {
		return transport_tpg.HandleNotFoundError(err, d, "Datascan")
	}

	err = DataplexOperationWaitTime(
		config, res, project, "Deleting Datascan", userAgent,
		d.Timeout(schema.TimeoutDelete))

	if err != nil {
		return err
	}

	log.Printf("[DEBUG] Finished deleting Datascan %q: %#v", d.Id(), res)
	return nil
}

func resourceDataplexDatascanImport(d *schema.ResourceData, meta interface{}) ([]*schema.ResourceData, error) {
	config := meta.(*transport_tpg.Config)
	if err := tpgresource.ParseImportId([]string{
		"projects/(?P<project>[^/]+)/locations/(?P<location>[^/]+)/dataScans/(?P<data_scan_id>[^/]+)",
		"(?P<project>[^/]+)/(?P<location>[^/]+)/(?P<data_scan_id>[^/]+)",
		"(?P<location>[^/]+)/(?P<data_scan_id>[^/]+)",
		"(?P<data_scan_id>[^/]+)",
	}, d, config); err != nil {
		return nil, err
	}

	// Replace import id for the resource id
	id, err := tpgresource.ReplaceVars(d, config, "projects/{{project}}/locations/{{location}}/dataScans/{{data_scan_id}}")
	if err != nil {
		return nil, fmt.Errorf("Error constructing id: %s", err)
	}
	d.SetId(id)

	return []*schema.ResourceData{d}, nil
}

func flattenDataplexDatascanName(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanUid(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDescription(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDisplayName(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanLabels(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanState(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanCreateTime(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanUpdateTime(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanData(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["entity"] =
		flattenDataplexDatascanDataEntity(original["entity"], d, config)
	transformed["resource"] =
		flattenDataplexDatascanDataResource(original["resource"], d, config)
	return []interface{}{transformed}
}
func flattenDataplexDatascanDataEntity(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataResource(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanExecutionSpec(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["trigger"] =
		flattenDataplexDatascanExecutionSpecTrigger(original["trigger"], d, config)
	transformed["field"] =
		flattenDataplexDatascanExecutionSpecField(original["field"], d, config)
	return []interface{}{transformed}
}
func flattenDataplexDatascanExecutionSpecTrigger(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["on_demand"] =
		flattenDataplexDatascanExecutionSpecTriggerOnDemand(original["onDemand"], d, config)
	transformed["schedule"] =
		flattenDataplexDatascanExecutionSpecTriggerSchedule(original["schedule"], d, config)
	return []interface{}{transformed}
}
func flattenDataplexDatascanExecutionSpecTriggerOnDemand(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	transformed := make(map[string]interface{})
	return []interface{}{transformed}
}

func flattenDataplexDatascanExecutionSpecTriggerSchedule(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["cron"] =
		flattenDataplexDatascanExecutionSpecTriggerScheduleCron(original["cron"], d, config)
	return []interface{}{transformed}
}
func flattenDataplexDatascanExecutionSpecTriggerScheduleCron(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanExecutionSpecField(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanExecutionStatus(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["latest_job_end_time"] =
		flattenDataplexDatascanExecutionStatusLatestJobEndTime(original["latestJobEndTime"], d, config)
	transformed["latest_job_start_time"] =
		flattenDataplexDatascanExecutionStatusLatestJobStartTime(original["latestJobStartTime"], d, config)
	return []interface{}{transformed}
}
func flattenDataplexDatascanExecutionStatusLatestJobEndTime(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanExecutionStatusLatestJobStartTime(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanType(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataQualitySpec(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["sampling_percent"] =
		flattenDataplexDatascanDataQualitySpecSamplingPercent(original["samplingPercent"], d, config)
	transformed["row_filter"] =
		flattenDataplexDatascanDataQualitySpecRowFilter(original["rowFilter"], d, config)
	transformed["rules"] =
		flattenDataplexDatascanDataQualitySpecRules(original["rules"], d, config)
	return []interface{}{transformed}
}
func flattenDataplexDatascanDataQualitySpecSamplingPercent(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataQualitySpecRowFilter(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataQualitySpecRules(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}
	l := v.([]interface{})
	transformed := make([]interface{}, 0, len(l))
	for _, raw := range l {
		original := raw.(map[string]interface{})
		if len(original) < 1 {
			// Do not include empty json objects coming back from the api
			continue
		}
		transformed = append(transformed, map[string]interface{}{
			"column":                      flattenDataplexDatascanDataQualitySpecRulesColumn(original["column"], d, config),
			"ignore_null":                 flattenDataplexDatascanDataQualitySpecRulesIgnoreNull(original["ignoreNull"], d, config),
			"dimension":                   flattenDataplexDatascanDataQualitySpecRulesDimension(original["dimension"], d, config),
			"threshold":                   flattenDataplexDatascanDataQualitySpecRulesThreshold(original["threshold"], d, config),
			"range_expectation":           flattenDataplexDatascanDataQualitySpecRulesRangeExpectation(original["rangeExpectation"], d, config),
			"non_null_expectation":        flattenDataplexDatascanDataQualitySpecRulesNonNullExpectation(original["nonNullExpectation"], d, config),
			"set_expectation":             flattenDataplexDatascanDataQualitySpecRulesSetExpectation(original["setExpectation"], d, config),
			"regex_expectation":           flattenDataplexDatascanDataQualitySpecRulesRegexExpectation(original["regexExpectation"], d, config),
			"uniqueness_expectation":      flattenDataplexDatascanDataQualitySpecRulesUniquenessExpectation(original["uniquenessExpectation"], d, config),
			"statistic_range_expectation": flattenDataplexDatascanDataQualitySpecRulesStatisticRangeExpectation(original["statisticRangeExpectation"], d, config),
			"row_condition_expectation":   flattenDataplexDatascanDataQualitySpecRulesRowConditionExpectation(original["rowConditionExpectation"], d, config),
			"table_condition_expectation": flattenDataplexDatascanDataQualitySpecRulesTableConditionExpectation(original["tableConditionExpectation"], d, config),
		})
	}
	return transformed
}
func flattenDataplexDatascanDataQualitySpecRulesColumn(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataQualitySpecRulesIgnoreNull(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataQualitySpecRulesDimension(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataQualitySpecRulesThreshold(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataQualitySpecRulesRangeExpectation(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["min_value"] =
		flattenDataplexDatascanDataQualitySpecRulesRangeExpectationMinValue(original["minValue"], d, config)
	transformed["max_value"] =
		flattenDataplexDatascanDataQualitySpecRulesRangeExpectationMaxValue(original["maxValue"], d, config)
	transformed["strict_min_enabled"] =
		flattenDataplexDatascanDataQualitySpecRulesRangeExpectationStrictMinEnabled(original["strictMinEnabled"], d, config)
	transformed["strict_max_enabled"] =
		flattenDataplexDatascanDataQualitySpecRulesRangeExpectationStrictMaxEnabled(original["strictMaxEnabled"], d, config)
	return []interface{}{transformed}
}
func flattenDataplexDatascanDataQualitySpecRulesRangeExpectationMinValue(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataQualitySpecRulesRangeExpectationMaxValue(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataQualitySpecRulesRangeExpectationStrictMinEnabled(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataQualitySpecRulesRangeExpectationStrictMaxEnabled(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataQualitySpecRulesNonNullExpectation(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	transformed := make(map[string]interface{})
	return []interface{}{transformed}
}

func flattenDataplexDatascanDataQualitySpecRulesSetExpectation(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["values"] =
		flattenDataplexDatascanDataQualitySpecRulesSetExpectationValues(original["values"], d, config)
	return []interface{}{transformed}
}
func flattenDataplexDatascanDataQualitySpecRulesSetExpectationValues(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataQualitySpecRulesRegexExpectation(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["regex"] =
		flattenDataplexDatascanDataQualitySpecRulesRegexExpectationRegex(original["regex"], d, config)
	return []interface{}{transformed}
}
func flattenDataplexDatascanDataQualitySpecRulesRegexExpectationRegex(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataQualitySpecRulesUniquenessExpectation(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	transformed := make(map[string]interface{})
	return []interface{}{transformed}
}

func flattenDataplexDatascanDataQualitySpecRulesStatisticRangeExpectation(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["statistic"] =
		flattenDataplexDatascanDataQualitySpecRulesStatisticRangeExpectationStatistic(original["statistic"], d, config)
	transformed["min_value"] =
		flattenDataplexDatascanDataQualitySpecRulesStatisticRangeExpectationMinValue(original["minValue"], d, config)
	transformed["max_value"] =
		flattenDataplexDatascanDataQualitySpecRulesStatisticRangeExpectationMaxValue(original["maxValue"], d, config)
	transformed["strict_min_enabled"] =
		flattenDataplexDatascanDataQualitySpecRulesStatisticRangeExpectationStrictMinEnabled(original["strictMinEnabled"], d, config)
	transformed["strict_max_enabled"] =
		flattenDataplexDatascanDataQualitySpecRulesStatisticRangeExpectationStrictMaxEnabled(original["strictMaxEnabled"], d, config)
	return []interface{}{transformed}
}
func flattenDataplexDatascanDataQualitySpecRulesStatisticRangeExpectationStatistic(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataQualitySpecRulesStatisticRangeExpectationMinValue(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataQualitySpecRulesStatisticRangeExpectationMaxValue(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataQualitySpecRulesStatisticRangeExpectationStrictMinEnabled(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataQualitySpecRulesStatisticRangeExpectationStrictMaxEnabled(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataQualitySpecRulesRowConditionExpectation(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["sql_expression"] =
		flattenDataplexDatascanDataQualitySpecRulesRowConditionExpectationSqlExpression(original["sqlExpression"], d, config)
	return []interface{}{transformed}
}
func flattenDataplexDatascanDataQualitySpecRulesRowConditionExpectationSqlExpression(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataQualitySpecRulesTableConditionExpectation(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["sql_expression"] =
		flattenDataplexDatascanDataQualitySpecRulesTableConditionExpectationSqlExpression(original["sqlExpression"], d, config)
	return []interface{}{transformed}
}
func flattenDataplexDatascanDataQualitySpecRulesTableConditionExpectationSqlExpression(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataProfileSpec(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	transformed := make(map[string]interface{})
	transformed["sampling_percent"] =
		flattenDataplexDatascanDataProfileSpecSamplingPercent(original["samplingPercent"], d, config)
	transformed["row_filter"] =
		flattenDataplexDatascanDataProfileSpecRowFilter(original["rowFilter"], d, config)
	return []interface{}{transformed}
}
func flattenDataplexDatascanDataProfileSpecSamplingPercent(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataProfileSpecRowFilter(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataQualityResult(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["passed"] =
		flattenDataplexDatascanDataQualityResultPassed(original["passed"], d, config)
	transformed["dimensions"] =
		flattenDataplexDatascanDataQualityResultDimensions(original["dimensions"], d, config)
	transformed["rules"] =
		flattenDataplexDatascanDataQualityResultRules(original["rules"], d, config)
	transformed["row_count"] =
		flattenDataplexDatascanDataQualityResultRowCount(original["rowCount"], d, config)
	transformed["scanned_data"] =
		flattenDataplexDatascanDataQualityResultScannedData(original["scannedData"], d, config)
	return []interface{}{transformed}
}
func flattenDataplexDatascanDataQualityResultPassed(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataQualityResultDimensions(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}
	l := v.([]interface{})
	transformed := make([]interface{}, 0, len(l))
	for _, raw := range l {
		original := raw.(map[string]interface{})
		if len(original) < 1 {
			// Do not include empty json objects coming back from the api
			continue
		}
		transformed = append(transformed, map[string]interface{}{
			"passed": flattenDataplexDatascanDataQualityResultDimensionsPassed(original["passed"], d, config),
		})
	}
	return transformed
}
func flattenDataplexDatascanDataQualityResultDimensionsPassed(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataQualityResultRules(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}
	l := v.([]interface{})
	transformed := make([]interface{}, 0, len(l))
	for _, raw := range l {
		original := raw.(map[string]interface{})
		if len(original) < 1 {
			// Do not include empty json objects coming back from the api
			continue
		}
		transformed = append(transformed, map[string]interface{}{
			"rule":               flattenDataplexDatascanDataQualityResultRulesRule(original["rule"], d, config),
			"passed":             flattenDataplexDatascanDataQualityResultRulesPassed(original["passed"], d, config),
			"evaluated_count":    flattenDataplexDatascanDataQualityResultRulesEvaluatedCount(original["evaluatedCount"], d, config),
			"passed_count":       flattenDataplexDatascanDataQualityResultRulesPassedCount(original["passedCount"], d, config),
			"null_count":         flattenDataplexDatascanDataQualityResultRulesNullCount(original["nullCount"], d, config),
			"pass_ratio":         flattenDataplexDatascanDataQualityResultRulesPassRatio(original["passRatio"], d, config),
			"failing_rows_query": flattenDataplexDatascanDataQualityResultRulesFailingRowsQuery(original["failingRowsQuery"], d, config),
		})
	}
	return transformed
}
func flattenDataplexDatascanDataQualityResultRulesRule(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["column"] =
		flattenDataplexDatascanDataQualityResultRulesRuleColumn(original["column"], d, config)
	transformed["ignore_null"] =
		flattenDataplexDatascanDataQualityResultRulesRuleIgnoreNull(original["ignoreNull"], d, config)
	transformed["dimension"] =
		flattenDataplexDatascanDataQualityResultRulesRuleDimension(original["dimension"], d, config)
	transformed["threshold"] =
		flattenDataplexDatascanDataQualityResultRulesRuleThreshold(original["threshold"], d, config)
	transformed["range_expectation"] =
		flattenDataplexDatascanDataQualityResultRulesRuleRangeExpectation(original["rangeExpectation"], d, config)
	transformed["non_null_expectation"] =
		flattenDataplexDatascanDataQualityResultRulesRuleNonNullExpectation(original["nonNullExpectation"], d, config)
	transformed["set_expectation"] =
		flattenDataplexDatascanDataQualityResultRulesRuleSetExpectation(original["setExpectation"], d, config)
	transformed["regex_expectation"] =
		flattenDataplexDatascanDataQualityResultRulesRuleRegexExpectation(original["regexExpectation"], d, config)
	transformed["uniqueness_expectation"] =
		flattenDataplexDatascanDataQualityResultRulesRuleUniquenessExpectation(original["uniquenessExpectation"], d, config)
	transformed["statistic_range_expectation"] =
		flattenDataplexDatascanDataQualityResultRulesRuleStatisticRangeExpectation(original["statisticRangeExpectation"], d, config)
	transformed["row_condition_expectation"] =
		flattenDataplexDatascanDataQualityResultRulesRuleRowConditionExpectation(original["rowConditionExpectation"], d, config)
	transformed["table_condition_expectation"] =
		flattenDataplexDatascanDataQualityResultRulesRuleTableConditionExpectation(original["tableConditionExpectation"], d, config)
	return []interface{}{transformed}
}
func flattenDataplexDatascanDataQualityResultRulesRuleColumn(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataQualityResultRulesRuleIgnoreNull(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataQualityResultRulesRuleDimension(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataQualityResultRulesRuleThreshold(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenDataplexDatascanDataQualityResultRulesRuleRangeExpectation(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["min_value"] =
		flattenDataplexDatascanDataQualityResultRulesRuleRangeExpectationMinValue(original["minValue"], d, config)
	transformed["max_value"] =
		flattenDataplexDatascanDataQualityResultRulesRuleRangeExpectationMaxValue(original["maxValue"], d, config)
	transformed["strict_min_enabled"] =
		flattenDataplexDatascanDataQualityResultRulesRuleRangeExpectationStrictMinEnabled(original["strictMinEnabled"], d, config)
	transformed["strict_max_enabled"] =
		flattenDataplexDatascanDataQualityResultRulesRuleRangeExpectationStrictMaxEnabled(original["strictMaxEnabled"], d, config)
	return []interface{}{transformed}
}
func flattenDataplexDatascanDataQualityResultRulesRuleRangeExpectationMinValue(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataQualityResultRulesRuleRangeExpectationMaxValue(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataQualityResultRulesRuleRangeExpectationStrictMinEnabled(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataQualityResultRulesRuleRangeExpectationStrictMaxEnabled(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataQualityResultRulesRuleNonNullExpectation(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	transformed := make(map[string]interface{})
	return []interface{}{transformed}
}

func flattenDataplexDatascanDataQualityResultRulesRuleSetExpectation(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["values"] =
		flattenDataplexDatascanDataQualityResultRulesRuleSetExpectationValues(original["values"], d, config)
	return []interface{}{transformed}
}
func flattenDataplexDatascanDataQualityResultRulesRuleSetExpectationValues(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataQualityResultRulesRuleRegexExpectation(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["regex"] =
		flattenDataplexDatascanDataQualityResultRulesRuleRegexExpectationRegex(original["regex"], d, config)
	return []interface{}{transformed}
}
func flattenDataplexDatascanDataQualityResultRulesRuleRegexExpectationRegex(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataQualityResultRulesRuleUniquenessExpectation(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	transformed := make(map[string]interface{})
	return []interface{}{transformed}
}

func flattenDataplexDatascanDataQualityResultRulesRuleStatisticRangeExpectation(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["statistic"] =
		flattenDataplexDatascanDataQualityResultRulesRuleStatisticRangeExpectationStatistic(original["statistic"], d, config)
	transformed["min_value"] =
		flattenDataplexDatascanDataQualityResultRulesRuleStatisticRangeExpectationMinValue(original["minValue"], d, config)
	transformed["max_value"] =
		flattenDataplexDatascanDataQualityResultRulesRuleStatisticRangeExpectationMaxValue(original["maxValue"], d, config)
	transformed["strict_min_enabled"] =
		flattenDataplexDatascanDataQualityResultRulesRuleStatisticRangeExpectationStrictMinEnabled(original["strictMinEnabled"], d, config)
	transformed["strict_max_enabled"] =
		flattenDataplexDatascanDataQualityResultRulesRuleStatisticRangeExpectationStrictMaxEnabled(original["strictMaxEnabled"], d, config)
	return []interface{}{transformed}
}
func flattenDataplexDatascanDataQualityResultRulesRuleStatisticRangeExpectationStatistic(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataQualityResultRulesRuleStatisticRangeExpectationMinValue(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataQualityResultRulesRuleStatisticRangeExpectationMaxValue(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataQualityResultRulesRuleStatisticRangeExpectationStrictMinEnabled(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataQualityResultRulesRuleStatisticRangeExpectationStrictMaxEnabled(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataQualityResultRulesRuleRowConditionExpectation(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["sql_expression"] =
		flattenDataplexDatascanDataQualityResultRulesRuleRowConditionExpectationSqlExpression(original["sqlExpression"], d, config)
	return []interface{}{transformed}
}
func flattenDataplexDatascanDataQualityResultRulesRuleRowConditionExpectationSqlExpression(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataQualityResultRulesRuleTableConditionExpectation(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["sql_expression"] =
		flattenDataplexDatascanDataQualityResultRulesRuleTableConditionExpectationSqlExpression(original["sqlExpression"], d, config)
	return []interface{}{transformed}
}
func flattenDataplexDatascanDataQualityResultRulesRuleTableConditionExpectationSqlExpression(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataQualityResultRulesPassed(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataQualityResultRulesEvaluatedCount(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataQualityResultRulesPassedCount(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataQualityResultRulesNullCount(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataQualityResultRulesPassRatio(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenDataplexDatascanDataQualityResultRulesFailingRowsQuery(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataQualityResultRowCount(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataQualityResultScannedData(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["incremental_field"] =
		flattenDataplexDatascanDataQualityResultScannedDataIncrementalField(original["incrementalField"], d, config)
	return []interface{}{transformed}
}
func flattenDataplexDatascanDataQualityResultScannedDataIncrementalField(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["field"] =
		flattenDataplexDatascanDataQualityResultScannedDataIncrementalFieldField(original["field"], d, config)
	transformed["start"] =
		flattenDataplexDatascanDataQualityResultScannedDataIncrementalFieldStart(original["start"], d, config)
	transformed["end"] =
		flattenDataplexDatascanDataQualityResultScannedDataIncrementalFieldEnd(original["end"], d, config)
	return []interface{}{transformed}
}
func flattenDataplexDatascanDataQualityResultScannedDataIncrementalFieldField(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataQualityResultScannedDataIncrementalFieldStart(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataQualityResultScannedDataIncrementalFieldEnd(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataProfileResult(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["row_count"] =
		flattenDataplexDatascanDataProfileResultRowCount(original["rowCount"], d, config)
	transformed["profile"] =
		flattenDataplexDatascanDataProfileResultProfile(original["profile"], d, config)
	transformed["scanned_data"] =
		flattenDataplexDatascanDataProfileResultScannedData(original["scannedData"], d, config)
	return []interface{}{transformed}
}
func flattenDataplexDatascanDataProfileResultRowCount(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataProfileResultProfile(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["fields"] =
		flattenDataplexDatascanDataProfileResultProfileFields(original["fields"], d, config)
	return []interface{}{transformed}
}
func flattenDataplexDatascanDataProfileResultProfileFields(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}
	l := v.([]interface{})
	transformed := make([]interface{}, 0, len(l))
	for _, raw := range l {
		original := raw.(map[string]interface{})
		if len(original) < 1 {
			// Do not include empty json objects coming back from the api
			continue
		}
		transformed = append(transformed, map[string]interface{}{
			"name":    flattenDataplexDatascanDataProfileResultProfileFieldsName(original["name"], d, config),
			"type":    flattenDataplexDatascanDataProfileResultProfileFieldsType(original["type"], d, config),
			"mode":    flattenDataplexDatascanDataProfileResultProfileFieldsMode(original["mode"], d, config),
			"profile": flattenDataplexDatascanDataProfileResultProfileFieldsProfile(original["profile"], d, config),
		})
	}
	return transformed
}
func flattenDataplexDatascanDataProfileResultProfileFieldsName(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataProfileResultProfileFieldsType(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataProfileResultProfileFieldsMode(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataProfileResultProfileFieldsProfile(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["null_ratio"] =
		flattenDataplexDatascanDataProfileResultProfileFieldsProfileNullRatio(original["nullRatio"], d, config)
	transformed["distinct_ratio"] =
		flattenDataplexDatascanDataProfileResultProfileFieldsProfileDistinctRatio(original["distinctRatio"], d, config)
	transformed["top_n_values"] =
		flattenDataplexDatascanDataProfileResultProfileFieldsProfileTopNValues(original["topNValues"], d, config)
	transformed["string_profile"] =
		flattenDataplexDatascanDataProfileResultProfileFieldsProfileStringProfile(original["stringProfile"], d, config)
	transformed["integer_profile"] =
		flattenDataplexDatascanDataProfileResultProfileFieldsProfileIntegerProfile(original["integerProfile"], d, config)
	transformed["double_profile"] =
		flattenDataplexDatascanDataProfileResultProfileFieldsProfileDoubleProfile(original["doubleProfile"], d, config)
	return []interface{}{transformed}
}
func flattenDataplexDatascanDataProfileResultProfileFieldsProfileNullRatio(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenDataplexDatascanDataProfileResultProfileFieldsProfileDistinctRatio(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenDataplexDatascanDataProfileResultProfileFieldsProfileTopNValues(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["value"] =
		flattenDataplexDatascanDataProfileResultProfileFieldsProfileTopNValuesValue(original["value"], d, config)
	transformed["count"] =
		flattenDataplexDatascanDataProfileResultProfileFieldsProfileTopNValuesCount(original["count"], d, config)
	return []interface{}{transformed}
}
func flattenDataplexDatascanDataProfileResultProfileFieldsProfileTopNValuesValue(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataProfileResultProfileFieldsProfileTopNValuesCount(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataProfileResultProfileFieldsProfileStringProfile(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["min_length"] =
		flattenDataplexDatascanDataProfileResultProfileFieldsProfileStringProfileMinLength(original["minLength"], d, config)
	transformed["max_length"] =
		flattenDataplexDatascanDataProfileResultProfileFieldsProfileStringProfileMaxLength(original["maxLength"], d, config)
	transformed["average_length"] =
		flattenDataplexDatascanDataProfileResultProfileFieldsProfileStringProfileAverageLength(original["averageLength"], d, config)
	return []interface{}{transformed}
}
func flattenDataplexDatascanDataProfileResultProfileFieldsProfileStringProfileMinLength(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataProfileResultProfileFieldsProfileStringProfileMaxLength(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataProfileResultProfileFieldsProfileStringProfileAverageLength(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenDataplexDatascanDataProfileResultProfileFieldsProfileIntegerProfile(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["average"] =
		flattenDataplexDatascanDataProfileResultProfileFieldsProfileIntegerProfileAverage(original["average"], d, config)
	transformed["standard_deviation"] =
		flattenDataplexDatascanDataProfileResultProfileFieldsProfileIntegerProfileStandardDeviation(original["standardDeviation"], d, config)
	transformed["min"] =
		flattenDataplexDatascanDataProfileResultProfileFieldsProfileIntegerProfileMin(original["min"], d, config)
	transformed["quartiles"] =
		flattenDataplexDatascanDataProfileResultProfileFieldsProfileIntegerProfileQuartiles(original["quartiles"], d, config)
	transformed["max"] =
		flattenDataplexDatascanDataProfileResultProfileFieldsProfileIntegerProfileMax(original["max"], d, config)
	return []interface{}{transformed}
}
func flattenDataplexDatascanDataProfileResultProfileFieldsProfileIntegerProfileAverage(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenDataplexDatascanDataProfileResultProfileFieldsProfileIntegerProfileStandardDeviation(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenDataplexDatascanDataProfileResultProfileFieldsProfileIntegerProfileMin(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataProfileResultProfileFieldsProfileIntegerProfileQuartiles(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataProfileResultProfileFieldsProfileIntegerProfileMax(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataProfileResultProfileFieldsProfileDoubleProfile(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["average"] =
		flattenDataplexDatascanDataProfileResultProfileFieldsProfileDoubleProfileAverage(original["average"], d, config)
	transformed["standard_deviation"] =
		flattenDataplexDatascanDataProfileResultProfileFieldsProfileDoubleProfileStandardDeviation(original["standardDeviation"], d, config)
	transformed["min"] =
		flattenDataplexDatascanDataProfileResultProfileFieldsProfileDoubleProfileMin(original["min"], d, config)
	transformed["quartiles"] =
		flattenDataplexDatascanDataProfileResultProfileFieldsProfileDoubleProfileQuartiles(original["quartiles"], d, config)
	transformed["max"] =
		flattenDataplexDatascanDataProfileResultProfileFieldsProfileDoubleProfileMax(original["max"], d, config)
	return []interface{}{transformed}
}
func flattenDataplexDatascanDataProfileResultProfileFieldsProfileDoubleProfileAverage(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenDataplexDatascanDataProfileResultProfileFieldsProfileDoubleProfileStandardDeviation(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenDataplexDatascanDataProfileResultProfileFieldsProfileDoubleProfileMin(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataProfileResultProfileFieldsProfileDoubleProfileQuartiles(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataProfileResultProfileFieldsProfileDoubleProfileMax(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataProfileResultScannedData(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["incremental_field"] =
		flattenDataplexDatascanDataProfileResultScannedDataIncrementalField(original["incrementalField"], d, config)
	return []interface{}{transformed}
}
func flattenDataplexDatascanDataProfileResultScannedDataIncrementalField(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["field"] =
		flattenDataplexDatascanDataProfileResultScannedDataIncrementalFieldField(original["field"], d, config)
	transformed["start"] =
		flattenDataplexDatascanDataProfileResultScannedDataIncrementalFieldStart(original["start"], d, config)
	transformed["end"] =
		flattenDataplexDatascanDataProfileResultScannedDataIncrementalFieldEnd(original["end"], d, config)
	return []interface{}{transformed}
}
func flattenDataplexDatascanDataProfileResultScannedDataIncrementalFieldField(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataProfileResultScannedDataIncrementalFieldStart(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenDataplexDatascanDataProfileResultScannedDataIncrementalFieldEnd(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func expandDataplexDatascanDescription(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexDatascanDisplayName(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexDatascanLabels(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (map[string]string, error) {
	if v == nil {
		return map[string]string{}, nil
	}
	m := make(map[string]string)
	for k, val := range v.(map[string]interface{}) {
		m[k] = val.(string)
	}
	return m, nil
}

func expandDataplexDatascanData(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedEntity, err := expandDataplexDatascanDataEntity(original["entity"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedEntity); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["entity"] = transformedEntity
	}

	transformedResource, err := expandDataplexDatascanDataResource(original["resource"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedResource); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["resource"] = transformedResource
	}

	return transformed, nil
}

func expandDataplexDatascanDataEntity(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexDatascanDataResource(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexDatascanExecutionSpec(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedTrigger, err := expandDataplexDatascanExecutionSpecTrigger(original["trigger"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedTrigger); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["trigger"] = transformedTrigger
	}

	transformedField, err := expandDataplexDatascanExecutionSpecField(original["field"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedField); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["field"] = transformedField
	}

	return transformed, nil
}

func expandDataplexDatascanExecutionSpecTrigger(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedOnDemand, err := expandDataplexDatascanExecutionSpecTriggerOnDemand(original["on_demand"], d, config)
	if err != nil {
		return nil, err
	} else {
		transformed["onDemand"] = transformedOnDemand
	}

	transformedSchedule, err := expandDataplexDatascanExecutionSpecTriggerSchedule(original["schedule"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedSchedule); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["schedule"] = transformedSchedule
	}

	return transformed, nil
}

func expandDataplexDatascanExecutionSpecTriggerOnDemand(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 {
		return nil, nil
	}

	if l[0] == nil {
		transformed := make(map[string]interface{})
		return transformed, nil
	}
	transformed := make(map[string]interface{})

	return transformed, nil
}

func expandDataplexDatascanExecutionSpecTriggerSchedule(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedCron, err := expandDataplexDatascanExecutionSpecTriggerScheduleCron(original["cron"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedCron); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["cron"] = transformedCron
	}

	return transformed, nil
}

func expandDataplexDatascanExecutionSpecTriggerScheduleCron(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexDatascanExecutionSpecField(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexDatascanDataQualitySpec(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedSamplingPercent, err := expandDataplexDatascanDataQualitySpecSamplingPercent(original["sampling_percent"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedSamplingPercent); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["samplingPercent"] = transformedSamplingPercent
	}

	transformedRowFilter, err := expandDataplexDatascanDataQualitySpecRowFilter(original["row_filter"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedRowFilter); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["rowFilter"] = transformedRowFilter
	}

	transformedRules, err := expandDataplexDatascanDataQualitySpecRules(original["rules"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedRules); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["rules"] = transformedRules
	}

	return transformed, nil
}

func expandDataplexDatascanDataQualitySpecSamplingPercent(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexDatascanDataQualitySpecRowFilter(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexDatascanDataQualitySpecRules(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	req := make([]interface{}, 0, len(l))
	for _, raw := range l {
		if raw == nil {
			continue
		}
		original := raw.(map[string]interface{})
		transformed := make(map[string]interface{})

		transformedColumn, err := expandDataplexDatascanDataQualitySpecRulesColumn(original["column"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedColumn); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["column"] = transformedColumn
		}

		transformedIgnoreNull, err := expandDataplexDatascanDataQualitySpecRulesIgnoreNull(original["ignore_null"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedIgnoreNull); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["ignoreNull"] = transformedIgnoreNull
		}

		transformedDimension, err := expandDataplexDatascanDataQualitySpecRulesDimension(original["dimension"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedDimension); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["dimension"] = transformedDimension
		}

		transformedThreshold, err := expandDataplexDatascanDataQualitySpecRulesThreshold(original["threshold"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedThreshold); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["threshold"] = transformedThreshold
		}

		transformedRangeExpectation, err := expandDataplexDatascanDataQualitySpecRulesRangeExpectation(original["range_expectation"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedRangeExpectation); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["rangeExpectation"] = transformedRangeExpectation
		}

		transformedNonNullExpectation, err := expandDataplexDatascanDataQualitySpecRulesNonNullExpectation(original["non_null_expectation"], d, config)
		if err != nil {
			return nil, err
		} else {
			transformed["nonNullExpectation"] = transformedNonNullExpectation
		}

		transformedSetExpectation, err := expandDataplexDatascanDataQualitySpecRulesSetExpectation(original["set_expectation"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedSetExpectation); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["setExpectation"] = transformedSetExpectation
		}

		transformedRegexExpectation, err := expandDataplexDatascanDataQualitySpecRulesRegexExpectation(original["regex_expectation"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedRegexExpectation); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["regexExpectation"] = transformedRegexExpectation
		}

		transformedUniquenessExpectation, err := expandDataplexDatascanDataQualitySpecRulesUniquenessExpectation(original["uniqueness_expectation"], d, config)
		if err != nil {
			return nil, err
		} else {
			transformed["uniquenessExpectation"] = transformedUniquenessExpectation
		}

		transformedStatisticRangeExpectation, err := expandDataplexDatascanDataQualitySpecRulesStatisticRangeExpectation(original["statistic_range_expectation"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedStatisticRangeExpectation); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["statisticRangeExpectation"] = transformedStatisticRangeExpectation
		}

		transformedRowConditionExpectation, err := expandDataplexDatascanDataQualitySpecRulesRowConditionExpectation(original["row_condition_expectation"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedRowConditionExpectation); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["rowConditionExpectation"] = transformedRowConditionExpectation
		}

		transformedTableConditionExpectation, err := expandDataplexDatascanDataQualitySpecRulesTableConditionExpectation(original["table_condition_expectation"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedTableConditionExpectation); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["tableConditionExpectation"] = transformedTableConditionExpectation
		}

		req = append(req, transformed)
	}
	return req, nil
}

func expandDataplexDatascanDataQualitySpecRulesColumn(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexDatascanDataQualitySpecRulesIgnoreNull(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexDatascanDataQualitySpecRulesDimension(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexDatascanDataQualitySpecRulesThreshold(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexDatascanDataQualitySpecRulesRangeExpectation(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedMinValue, err := expandDataplexDatascanDataQualitySpecRulesRangeExpectationMinValue(original["min_value"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedMinValue); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["minValue"] = transformedMinValue
	}

	transformedMaxValue, err := expandDataplexDatascanDataQualitySpecRulesRangeExpectationMaxValue(original["max_value"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedMaxValue); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["maxValue"] = transformedMaxValue
	}

	transformedStrictMinEnabled, err := expandDataplexDatascanDataQualitySpecRulesRangeExpectationStrictMinEnabled(original["strict_min_enabled"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedStrictMinEnabled); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["strictMinEnabled"] = transformedStrictMinEnabled
	}

	transformedStrictMaxEnabled, err := expandDataplexDatascanDataQualitySpecRulesRangeExpectationStrictMaxEnabled(original["strict_max_enabled"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedStrictMaxEnabled); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["strictMaxEnabled"] = transformedStrictMaxEnabled
	}

	return transformed, nil
}

func expandDataplexDatascanDataQualitySpecRulesRangeExpectationMinValue(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexDatascanDataQualitySpecRulesRangeExpectationMaxValue(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexDatascanDataQualitySpecRulesRangeExpectationStrictMinEnabled(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexDatascanDataQualitySpecRulesRangeExpectationStrictMaxEnabled(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexDatascanDataQualitySpecRulesNonNullExpectation(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 {
		return nil, nil
	}

	if l[0] == nil {
		transformed := make(map[string]interface{})
		return transformed, nil
	}
	transformed := make(map[string]interface{})

	return transformed, nil
}

func expandDataplexDatascanDataQualitySpecRulesSetExpectation(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedValues, err := expandDataplexDatascanDataQualitySpecRulesSetExpectationValues(original["values"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedValues); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["values"] = transformedValues
	}

	return transformed, nil
}

func expandDataplexDatascanDataQualitySpecRulesSetExpectationValues(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexDatascanDataQualitySpecRulesRegexExpectation(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedRegex, err := expandDataplexDatascanDataQualitySpecRulesRegexExpectationRegex(original["regex"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedRegex); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["regex"] = transformedRegex
	}

	return transformed, nil
}

func expandDataplexDatascanDataQualitySpecRulesRegexExpectationRegex(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexDatascanDataQualitySpecRulesUniquenessExpectation(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 {
		return nil, nil
	}

	if l[0] == nil {
		transformed := make(map[string]interface{})
		return transformed, nil
	}
	transformed := make(map[string]interface{})

	return transformed, nil
}

func expandDataplexDatascanDataQualitySpecRulesStatisticRangeExpectation(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedStatistic, err := expandDataplexDatascanDataQualitySpecRulesStatisticRangeExpectationStatistic(original["statistic"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedStatistic); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["statistic"] = transformedStatistic
	}

	transformedMinValue, err := expandDataplexDatascanDataQualitySpecRulesStatisticRangeExpectationMinValue(original["min_value"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedMinValue); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["minValue"] = transformedMinValue
	}

	transformedMaxValue, err := expandDataplexDatascanDataQualitySpecRulesStatisticRangeExpectationMaxValue(original["max_value"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedMaxValue); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["maxValue"] = transformedMaxValue
	}

	transformedStrictMinEnabled, err := expandDataplexDatascanDataQualitySpecRulesStatisticRangeExpectationStrictMinEnabled(original["strict_min_enabled"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedStrictMinEnabled); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["strictMinEnabled"] = transformedStrictMinEnabled
	}

	transformedStrictMaxEnabled, err := expandDataplexDatascanDataQualitySpecRulesStatisticRangeExpectationStrictMaxEnabled(original["strict_max_enabled"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedStrictMaxEnabled); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["strictMaxEnabled"] = transformedStrictMaxEnabled
	}

	return transformed, nil
}

func expandDataplexDatascanDataQualitySpecRulesStatisticRangeExpectationStatistic(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexDatascanDataQualitySpecRulesStatisticRangeExpectationMinValue(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexDatascanDataQualitySpecRulesStatisticRangeExpectationMaxValue(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexDatascanDataQualitySpecRulesStatisticRangeExpectationStrictMinEnabled(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexDatascanDataQualitySpecRulesStatisticRangeExpectationStrictMaxEnabled(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexDatascanDataQualitySpecRulesRowConditionExpectation(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedSqlExpression, err := expandDataplexDatascanDataQualitySpecRulesRowConditionExpectationSqlExpression(original["sql_expression"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedSqlExpression); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["sqlExpression"] = transformedSqlExpression
	}

	return transformed, nil
}

func expandDataplexDatascanDataQualitySpecRulesRowConditionExpectationSqlExpression(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexDatascanDataQualitySpecRulesTableConditionExpectation(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedSqlExpression, err := expandDataplexDatascanDataQualitySpecRulesTableConditionExpectationSqlExpression(original["sql_expression"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedSqlExpression); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["sqlExpression"] = transformedSqlExpression
	}

	return transformed, nil
}

func expandDataplexDatascanDataQualitySpecRulesTableConditionExpectationSqlExpression(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexDatascanDataProfileSpec(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 {
		return nil, nil
	}

	if l[0] == nil {
		transformed := make(map[string]interface{})
		return transformed, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedSamplingPercent, err := expandDataplexDatascanDataProfileSpecSamplingPercent(original["sampling_percent"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedSamplingPercent); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["samplingPercent"] = transformedSamplingPercent
	}

	transformedRowFilter, err := expandDataplexDatascanDataProfileSpecRowFilter(original["row_filter"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedRowFilter); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["rowFilter"] = transformedRowFilter
	}

	return transformed, nil
}

func expandDataplexDatascanDataProfileSpecSamplingPercent(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandDataplexDatascanDataProfileSpecRowFilter(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}
