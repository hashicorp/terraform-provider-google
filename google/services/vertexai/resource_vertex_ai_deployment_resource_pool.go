// Copyright (c) HashiCorp, Inc.
// SPDX-License-Identifier: MPL-2.0

// ----------------------------------------------------------------------------
//
//     ***     AUTO GENERATED CODE    ***    Type: MMv1     ***
//
// ----------------------------------------------------------------------------
//
//     This code is generated by Magic Modules using the following:
//
//     Configuration: https://github.com/GoogleCloudPlatform/magic-modules/tree/main/mmv1/products/vertexai/DeploymentResourcePool.yaml
//     Template:      https://github.com/GoogleCloudPlatform/magic-modules/tree/main/mmv1/templates/terraform/resource.go.tmpl
//
//     DO NOT EDIT this file directly. Any changes made to this file will be
//     overwritten during the next generation cycle.
//
// ----------------------------------------------------------------------------

package vertexai

import (
	"fmt"
	"log"
	"net/http"
	"reflect"
	"time"

	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/customdiff"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/schema"

	"github.com/hashicorp/terraform-provider-google/google/tpgresource"
	transport_tpg "github.com/hashicorp/terraform-provider-google/google/transport"
)

func ResourceVertexAIDeploymentResourcePool() *schema.Resource {
	return &schema.Resource{
		Create: resourceVertexAIDeploymentResourcePoolCreate,
		Read:   resourceVertexAIDeploymentResourcePoolRead,
		Delete: resourceVertexAIDeploymentResourcePoolDelete,

		Importer: &schema.ResourceImporter{
			State: resourceVertexAIDeploymentResourcePoolImport,
		},

		Timeouts: &schema.ResourceTimeout{
			Create: schema.DefaultTimeout(20 * time.Minute),
			Delete: schema.DefaultTimeout(20 * time.Minute),
		},

		CustomizeDiff: customdiff.All(
			tpgresource.DefaultProviderProject,
		),

		Schema: map[string]*schema.Schema{
			"name": {
				Type:        schema.TypeString,
				Required:    true,
				ForceNew:    true,
				Description: `The resource name of deployment resource pool. The maximum length is 63 characters, and valid characters are '/^[a-z]([a-z0-9-]{0,61}[a-z0-9])?$/'.`,
			},
			"dedicated_resources": {
				Type:        schema.TypeList,
				Optional:    true,
				ForceNew:    true,
				Description: `The underlying dedicated resources that the deployment resource pool uses.`,
				MaxItems:    1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"machine_spec": {
							Type:        schema.TypeList,
							Required:    true,
							ForceNew:    true,
							Description: `The specification of a single machine used by the prediction`,
							MaxItems:    1,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"accelerator_count": {
										Type:        schema.TypeInt,
										Optional:    true,
										ForceNew:    true,
										Description: `The number of accelerators to attach to the machine.`,
									},
									"accelerator_type": {
										Type:        schema.TypeString,
										Optional:    true,
										ForceNew:    true,
										Description: `The type of accelerator(s) that may be attached to the machine as per accelerator_count. See possible values [here](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec#AcceleratorType).`,
									},
									"machine_type": {
										Type:        schema.TypeString,
										Optional:    true,
										ForceNew:    true,
										Description: `The type of the machine. See the [list of machine types supported for prediction](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types).`,
									},
								},
							},
						},
						"min_replica_count": {
							Type:        schema.TypeInt,
							Required:    true,
							ForceNew:    true,
							Description: `The minimum number of machine replicas this DeployedModel will be always deployed on. This value must be greater than or equal to 1. If traffic against the DeployedModel increases, it may dynamically be deployed onto more replicas, and as traffic decreases, some of these extra replicas may be freed.`,
						},
						"autoscaling_metric_specs": {
							Type:        schema.TypeList,
							Optional:    true,
							ForceNew:    true,
							Description: `A list of the metric specifications that overrides a resource utilization metric.`,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"metric_name": {
										Type:        schema.TypeString,
										Required:    true,
										ForceNew:    true,
										Description: `The resource metric name. Supported metrics: For Online Prediction: * 'aiplatform.googleapis.com/prediction/online/accelerator/duty_cycle' * 'aiplatform.googleapis.com/prediction/online/cpu/utilization'`,
									},
									"target": {
										Type:        schema.TypeInt,
										Optional:    true,
										ForceNew:    true,
										Description: `The target resource utilization in percentage (1% - 100%) for the given metric; once the real usage deviates from the target by a certain percentage, the machine replicas change. The default value is 60 (representing 60%) if not provided.`,
									},
								},
							},
						},
						"max_replica_count": {
							Type:        schema.TypeInt,
							Optional:    true,
							ForceNew:    true,
							Description: `The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases. If the requested value is too large, the deployment will error, but if deployment succeeds then the ability to scale the model to that many replicas is guaranteed (barring service outages). If traffic against the DeployedModel increases beyond what its replicas at maximum may handle, a portion of the traffic will be dropped. If this value is not provided, will use min_replica_count as the default value. The value of this field impacts the charge against Vertex CPU and GPU quotas. Specifically, you will be charged for max_replica_count * number of cores in the selected machine type) and (max_replica_count * number of GPUs per replica in the selected machine type).`,
						},
					},
				},
			},
			"region": {
				Type:        schema.TypeString,
				Optional:    true,
				ForceNew:    true,
				Description: `The region of deployment resource pool. eg us-central1`,
			},
			"create_time": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits.`,
			},
			"project": {
				Type:     schema.TypeString,
				Optional: true,
				Computed: true,
				ForceNew: true,
			},
		},
		UseJSONNumber: true,
	}
}

func resourceVertexAIDeploymentResourcePoolCreate(d *schema.ResourceData, meta interface{}) error {
	config := meta.(*transport_tpg.Config)
	userAgent, err := tpgresource.GenerateUserAgentString(d, config.UserAgent)
	if err != nil {
		return err
	}

	obj := make(map[string]interface{})
	nameProp, err := expandVertexAIDeploymentResourcePoolName(d.Get("name"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("name"); !tpgresource.IsEmptyValue(reflect.ValueOf(nameProp)) && (ok || !reflect.DeepEqual(v, nameProp)) {
		obj["name"] = nameProp
	}
	dedicatedResourcesProp, err := expandVertexAIDeploymentResourcePoolDedicatedResources(d.Get("dedicated_resources"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("dedicated_resources"); !tpgresource.IsEmptyValue(reflect.ValueOf(dedicatedResourcesProp)) && (ok || !reflect.DeepEqual(v, dedicatedResourcesProp)) {
		obj["dedicatedResources"] = dedicatedResourcesProp
	}

	obj, err = resourceVertexAIDeploymentResourcePoolEncoder(d, meta, obj)
	if err != nil {
		return err
	}

	url, err := tpgresource.ReplaceVars(d, config, "{{VertexAIBasePath}}projects/{{project}}/locations/{{region}}/deploymentResourcePools")
	if err != nil {
		return err
	}

	log.Printf("[DEBUG] Creating new DeploymentResourcePool: %#v", obj)
	billingProject := ""

	project, err := tpgresource.GetProject(d, config)
	if err != nil {
		return fmt.Errorf("Error fetching project for DeploymentResourcePool: %s", err)
	}
	billingProject = project

	// err == nil indicates that the billing_project value was found
	if bp, err := tpgresource.GetBillingProject(d, config); err == nil {
		billingProject = bp
	}

	headers := make(http.Header)
	res, err := transport_tpg.SendRequest(transport_tpg.SendRequestOptions{
		Config:    config,
		Method:    "POST",
		Project:   billingProject,
		RawURL:    url,
		UserAgent: userAgent,
		Body:      obj,
		Timeout:   d.Timeout(schema.TimeoutCreate),
		Headers:   headers,
	})
	if err != nil {
		return fmt.Errorf("Error creating DeploymentResourcePool: %s", err)
	}

	// Store the ID now
	id, err := tpgresource.ReplaceVars(d, config, "projects/{{project}}/locations/{{region}}/deploymentResourcePools/{{name}}")
	if err != nil {
		return fmt.Errorf("Error constructing id: %s", err)
	}
	d.SetId(id)

	// Use the resource in the operation response to populate
	// identity fields and d.Id() before read
	var opRes map[string]interface{}
	err = VertexAIOperationWaitTimeWithResponse(
		config, res, &opRes, project, "Creating DeploymentResourcePool", userAgent,
		d.Timeout(schema.TimeoutCreate))
	if err != nil {
		// The resource didn't actually create
		d.SetId("")

		return fmt.Errorf("Error waiting to create DeploymentResourcePool: %s", err)
	}

	if err := d.Set("name", flattenVertexAIDeploymentResourcePoolName(opRes["name"], d, config)); err != nil {
		return err
	}

	// This may have caused the ID to update - update it if so.
	id, err = tpgresource.ReplaceVars(d, config, "projects/{{project}}/locations/{{region}}/deploymentResourcePools/{{name}}")
	if err != nil {
		return fmt.Errorf("Error constructing id: %s", err)
	}
	d.SetId(id)

	log.Printf("[DEBUG] Finished creating DeploymentResourcePool %q: %#v", d.Id(), res)

	return resourceVertexAIDeploymentResourcePoolRead(d, meta)
}

func resourceVertexAIDeploymentResourcePoolRead(d *schema.ResourceData, meta interface{}) error {
	config := meta.(*transport_tpg.Config)
	userAgent, err := tpgresource.GenerateUserAgentString(d, config.UserAgent)
	if err != nil {
		return err
	}

	url, err := tpgresource.ReplaceVars(d, config, "{{VertexAIBasePath}}projects/{{project}}/locations/{{region}}/deploymentResourcePools/{{name}}")
	if err != nil {
		return err
	}

	billingProject := ""

	project, err := tpgresource.GetProject(d, config)
	if err != nil {
		return fmt.Errorf("Error fetching project for DeploymentResourcePool: %s", err)
	}
	billingProject = project

	// err == nil indicates that the billing_project value was found
	if bp, err := tpgresource.GetBillingProject(d, config); err == nil {
		billingProject = bp
	}

	headers := make(http.Header)
	res, err := transport_tpg.SendRequest(transport_tpg.SendRequestOptions{
		Config:    config,
		Method:    "GET",
		Project:   billingProject,
		RawURL:    url,
		UserAgent: userAgent,
		Headers:   headers,
	})
	if err != nil {
		return transport_tpg.HandleNotFoundError(err, d, fmt.Sprintf("VertexAIDeploymentResourcePool %q", d.Id()))
	}

	if err := d.Set("project", project); err != nil {
		return fmt.Errorf("Error reading DeploymentResourcePool: %s", err)
	}

	if err := d.Set("name", flattenVertexAIDeploymentResourcePoolName(res["name"], d, config)); err != nil {
		return fmt.Errorf("Error reading DeploymentResourcePool: %s", err)
	}
	if err := d.Set("dedicated_resources", flattenVertexAIDeploymentResourcePoolDedicatedResources(res["dedicatedResources"], d, config)); err != nil {
		return fmt.Errorf("Error reading DeploymentResourcePool: %s", err)
	}
	if err := d.Set("create_time", flattenVertexAIDeploymentResourcePoolCreateTime(res["createTime"], d, config)); err != nil {
		return fmt.Errorf("Error reading DeploymentResourcePool: %s", err)
	}

	return nil
}

func resourceVertexAIDeploymentResourcePoolDelete(d *schema.ResourceData, meta interface{}) error {
	config := meta.(*transport_tpg.Config)
	userAgent, err := tpgresource.GenerateUserAgentString(d, config.UserAgent)
	if err != nil {
		return err
	}

	billingProject := ""

	project, err := tpgresource.GetProject(d, config)
	if err != nil {
		return fmt.Errorf("Error fetching project for DeploymentResourcePool: %s", err)
	}
	billingProject = project

	url, err := tpgresource.ReplaceVars(d, config, "{{VertexAIBasePath}}projects/{{project}}/locations/{{region}}/deploymentResourcePools/{{name}}")
	if err != nil {
		return err
	}

	var obj map[string]interface{}

	// err == nil indicates that the billing_project value was found
	if bp, err := tpgresource.GetBillingProject(d, config); err == nil {
		billingProject = bp
	}

	headers := make(http.Header)

	log.Printf("[DEBUG] Deleting DeploymentResourcePool %q", d.Id())
	res, err := transport_tpg.SendRequest(transport_tpg.SendRequestOptions{
		Config:    config,
		Method:    "DELETE",
		Project:   billingProject,
		RawURL:    url,
		UserAgent: userAgent,
		Body:      obj,
		Timeout:   d.Timeout(schema.TimeoutDelete),
		Headers:   headers,
	})
	if err != nil {
		return transport_tpg.HandleNotFoundError(err, d, "DeploymentResourcePool")
	}

	err = VertexAIOperationWaitTime(
		config, res, project, "Deleting DeploymentResourcePool", userAgent,
		d.Timeout(schema.TimeoutDelete))

	if err != nil {
		return err
	}

	log.Printf("[DEBUG] Finished deleting DeploymentResourcePool %q: %#v", d.Id(), res)
	return nil
}

func resourceVertexAIDeploymentResourcePoolImport(d *schema.ResourceData, meta interface{}) ([]*schema.ResourceData, error) {
	config := meta.(*transport_tpg.Config)
	if err := tpgresource.ParseImportId([]string{
		"^projects/(?P<project>[^/]+)/locations/(?P<region>[^/]+)/deploymentResourcePools/(?P<name>[^/]+)$",
		"^(?P<project>[^/]+)/(?P<region>[^/]+)/(?P<name>[^/]+)$",
		"^(?P<region>[^/]+)/(?P<name>[^/]+)$",
		"^(?P<name>[^/]+)$",
	}, d, config); err != nil {
		return nil, err
	}

	// Replace import id for the resource id
	id, err := tpgresource.ReplaceVars(d, config, "projects/{{project}}/locations/{{region}}/deploymentResourcePools/{{name}}")
	if err != nil {
		return nil, fmt.Errorf("Error constructing id: %s", err)
	}
	d.SetId(id)

	return []*schema.ResourceData{d}, nil
}

func flattenVertexAIDeploymentResourcePoolName(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}
	return tpgresource.GetResourceNameFromSelfLink(v.(string))
}

func flattenVertexAIDeploymentResourcePoolDedicatedResources(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["machine_spec"] =
		flattenVertexAIDeploymentResourcePoolDedicatedResourcesMachineSpec(original["machineSpec"], d, config)
	transformed["min_replica_count"] =
		flattenVertexAIDeploymentResourcePoolDedicatedResourcesMinReplicaCount(original["minReplicaCount"], d, config)
	transformed["max_replica_count"] =
		flattenVertexAIDeploymentResourcePoolDedicatedResourcesMaxReplicaCount(original["maxReplicaCount"], d, config)
	transformed["autoscaling_metric_specs"] =
		flattenVertexAIDeploymentResourcePoolDedicatedResourcesAutoscalingMetricSpecs(original["autoscalingMetricSpecs"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIDeploymentResourcePoolDedicatedResourcesMachineSpec(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["machine_type"] =
		flattenVertexAIDeploymentResourcePoolDedicatedResourcesMachineSpecMachineType(original["machineType"], d, config)
	transformed["accelerator_type"] =
		flattenVertexAIDeploymentResourcePoolDedicatedResourcesMachineSpecAcceleratorType(original["acceleratorType"], d, config)
	transformed["accelerator_count"] =
		flattenVertexAIDeploymentResourcePoolDedicatedResourcesMachineSpecAcceleratorCount(original["acceleratorCount"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIDeploymentResourcePoolDedicatedResourcesMachineSpecMachineType(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIDeploymentResourcePoolDedicatedResourcesMachineSpecAcceleratorType(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIDeploymentResourcePoolDedicatedResourcesMachineSpecAcceleratorCount(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIDeploymentResourcePoolDedicatedResourcesMinReplicaCount(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIDeploymentResourcePoolDedicatedResourcesMaxReplicaCount(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIDeploymentResourcePoolDedicatedResourcesAutoscalingMetricSpecs(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}
	l := v.([]interface{})
	transformed := make([]interface{}, 0, len(l))
	for _, raw := range l {
		original := raw.(map[string]interface{})
		if len(original) < 1 {
			// Do not include empty json objects coming back from the api
			continue
		}
		transformed = append(transformed, map[string]interface{}{
			"metric_name": flattenVertexAIDeploymentResourcePoolDedicatedResourcesAutoscalingMetricSpecsMetricName(original["metricName"], d, config),
			"target":      flattenVertexAIDeploymentResourcePoolDedicatedResourcesAutoscalingMetricSpecsTarget(original["target"], d, config),
		})
	}
	return transformed
}
func flattenVertexAIDeploymentResourcePoolDedicatedResourcesAutoscalingMetricSpecsMetricName(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIDeploymentResourcePoolDedicatedResourcesAutoscalingMetricSpecsTarget(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIDeploymentResourcePoolCreateTime(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func expandVertexAIDeploymentResourcePoolName(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIDeploymentResourcePoolDedicatedResources(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedMachineSpec, err := expandVertexAIDeploymentResourcePoolDedicatedResourcesMachineSpec(original["machine_spec"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedMachineSpec); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["machineSpec"] = transformedMachineSpec
	}

	transformedMinReplicaCount, err := expandVertexAIDeploymentResourcePoolDedicatedResourcesMinReplicaCount(original["min_replica_count"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedMinReplicaCount); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["minReplicaCount"] = transformedMinReplicaCount
	}

	transformedMaxReplicaCount, err := expandVertexAIDeploymentResourcePoolDedicatedResourcesMaxReplicaCount(original["max_replica_count"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedMaxReplicaCount); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["maxReplicaCount"] = transformedMaxReplicaCount
	}

	transformedAutoscalingMetricSpecs, err := expandVertexAIDeploymentResourcePoolDedicatedResourcesAutoscalingMetricSpecs(original["autoscaling_metric_specs"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedAutoscalingMetricSpecs); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["autoscalingMetricSpecs"] = transformedAutoscalingMetricSpecs
	}

	return transformed, nil
}

func expandVertexAIDeploymentResourcePoolDedicatedResourcesMachineSpec(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedMachineType, err := expandVertexAIDeploymentResourcePoolDedicatedResourcesMachineSpecMachineType(original["machine_type"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedMachineType); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["machineType"] = transformedMachineType
	}

	transformedAcceleratorType, err := expandVertexAIDeploymentResourcePoolDedicatedResourcesMachineSpecAcceleratorType(original["accelerator_type"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedAcceleratorType); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["acceleratorType"] = transformedAcceleratorType
	}

	transformedAcceleratorCount, err := expandVertexAIDeploymentResourcePoolDedicatedResourcesMachineSpecAcceleratorCount(original["accelerator_count"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedAcceleratorCount); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["acceleratorCount"] = transformedAcceleratorCount
	}

	return transformed, nil
}

func expandVertexAIDeploymentResourcePoolDedicatedResourcesMachineSpecMachineType(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIDeploymentResourcePoolDedicatedResourcesMachineSpecAcceleratorType(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIDeploymentResourcePoolDedicatedResourcesMachineSpecAcceleratorCount(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIDeploymentResourcePoolDedicatedResourcesMinReplicaCount(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIDeploymentResourcePoolDedicatedResourcesMaxReplicaCount(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIDeploymentResourcePoolDedicatedResourcesAutoscalingMetricSpecs(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	req := make([]interface{}, 0, len(l))
	for _, raw := range l {
		if raw == nil {
			continue
		}
		original := raw.(map[string]interface{})
		transformed := make(map[string]interface{})

		transformedMetricName, err := expandVertexAIDeploymentResourcePoolDedicatedResourcesAutoscalingMetricSpecsMetricName(original["metric_name"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedMetricName); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["metricName"] = transformedMetricName
		}

		transformedTarget, err := expandVertexAIDeploymentResourcePoolDedicatedResourcesAutoscalingMetricSpecsTarget(original["target"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedTarget); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["target"] = transformedTarget
		}

		req = append(req, transformed)
	}
	return req, nil
}

func expandVertexAIDeploymentResourcePoolDedicatedResourcesAutoscalingMetricSpecsMetricName(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIDeploymentResourcePoolDedicatedResourcesAutoscalingMetricSpecsTarget(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func resourceVertexAIDeploymentResourcePoolEncoder(d *schema.ResourceData, meta interface{}, obj map[string]interface{}) (map[string]interface{}, error) {
	newObj := make(map[string]interface{})
	newObj["deploymentResourcePool"] = obj
	nameProp, ok := d.GetOk("name")
	if ok && nameProp != nil {
		newObj["deploymentResourcePoolId"] = nameProp
	}
	return newObj, nil
}
