// Copyright (c) HashiCorp, Inc.
// SPDX-License-Identifier: MPL-2.0

// ----------------------------------------------------------------------------
//
//     ***     AUTO GENERATED CODE    ***    Type: MMv1     ***
//
// ----------------------------------------------------------------------------
//
//     This code is generated by Magic Modules using the following:
//
//     Configuration: https://github.com/GoogleCloudPlatform/magic-modules/tree/main/mmv1/products/vertexai/EndpointWithModelGardenDeployment.yaml
//     Template:      https://github.com/GoogleCloudPlatform/magic-modules/tree/main/mmv1/templates/terraform/resource.go.tmpl
//
//     DO NOT EDIT this file directly. Any changes made to this file will be
//     overwritten during the next generation cycle.
//
// ----------------------------------------------------------------------------

package vertexai

import (
	"fmt"
	"log"
	"net/http"
	"reflect"
	"strings"
	"time"

	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/customdiff"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/schema"

	"github.com/hashicorp/terraform-provider-google/google/tpgresource"
	transport_tpg "github.com/hashicorp/terraform-provider-google/google/transport"
)

func ResourceVertexAIEndpointWithModelGardenDeployment() *schema.Resource {
	return &schema.Resource{
		Create: resourceVertexAIEndpointWithModelGardenDeploymentCreate,
		Read:   resourceVertexAIEndpointWithModelGardenDeploymentRead,
		Delete: resourceVertexAIEndpointWithModelGardenDeploymentDelete,

		Timeouts: &schema.ResourceTimeout{
			Create: schema.DefaultTimeout(180 * time.Minute),
			Delete: schema.DefaultTimeout(0 * time.Minute),
		},

		CustomizeDiff: customdiff.All(
			tpgresource.DefaultProviderProject,
		),

		Schema: map[string]*schema.Schema{
			"location": {
				Type:        schema.TypeString,
				Required:    true,
				ForceNew:    true,
				Description: `Resource ID segment making up resource 'location'. It identifies the resource within its parent collection as described in https://google.aip.dev/122.`,
			},
			"deploy_config": {
				Type:        schema.TypeList,
				Optional:    true,
				ForceNew:    true,
				Description: `The deploy config to use for the deployment.`,
				MaxItems:    1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"dedicated_resources": {
							Type:     schema.TypeList,
							Optional: true,
							ForceNew: true,
							Description: `A description of resources that are dedicated to a DeployedModel or
DeployedIndex, and that need a higher degree of manual configuration.`,
							MaxItems: 1,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"machine_spec": {
										Type:        schema.TypeList,
										Required:    true,
										ForceNew:    true,
										Description: `Specification of a single machine.`,
										MaxItems:    1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"accelerator_count": {
													Type:        schema.TypeInt,
													Optional:    true,
													ForceNew:    true,
													Description: `The number of accelerators to attach to the machine.`,
												},
												"accelerator_type": {
													Type:     schema.TypeString,
													Optional: true,
													ForceNew: true,
													Description: `Possible values:
ACCELERATOR_TYPE_UNSPECIFIED
NVIDIA_TESLA_K80
NVIDIA_TESLA_P100
NVIDIA_TESLA_V100
NVIDIA_TESLA_P4
NVIDIA_TESLA_T4
NVIDIA_TESLA_A100
NVIDIA_A100_80GB
NVIDIA_L4
NVIDIA_H100_80GB
NVIDIA_H100_MEGA_80GB
NVIDIA_H200_141GB
NVIDIA_B200
TPU_V2
TPU_V3
TPU_V4_POD
TPU_V5_LITEPOD`,
												},
												"machine_type": {
													Type:     schema.TypeString,
													Optional: true,
													ForceNew: true,
													Description: `The type of the machine.

See the [list of machine types supported for
prediction](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types)

See the [list of machine types supported for custom
training](https://cloud.google.com/vertex-ai/docs/training/configure-compute#machine-types).

For DeployedModel this field is optional, and the default
value is 'n1-standard-2'. For BatchPredictionJob or as part of
WorkerPoolSpec this field is required.`,
												},
												"multihost_gpu_node_count": {
													Type:        schema.TypeInt,
													Optional:    true,
													ForceNew:    true,
													Description: `The number of nodes per replica for multihost GPU deployments.`,
												},
												"reservation_affinity": {
													Type:     schema.TypeList,
													Optional: true,
													ForceNew: true,
													Description: `A ReservationAffinity can be used to configure a Vertex AI resource (e.g., a
DeployedModel) to draw its Compute Engine resources from a Shared
Reservation, or exclusively from on-demand capacity.`,
													MaxItems: 1,
													Elem: &schema.Resource{
														Schema: map[string]*schema.Schema{
															"reservation_affinity_type": {
																Type:     schema.TypeString,
																Required: true,
																ForceNew: true,
																Description: `Specifies the reservation affinity type.
Possible values:
TYPE_UNSPECIFIED
NO_RESERVATION
ANY_RESERVATION
SPECIFIC_RESERVATION`,
															},
															"key": {
																Type:     schema.TypeString,
																Optional: true,
																ForceNew: true,
																Description: `Corresponds to the label key of a reservation resource. To target a
SPECIFIC_RESERVATION by name, use 'compute.googleapis.com/reservation-name'
as the key and specify the name of your reservation as its value.`,
															},
															"values": {
																Type:     schema.TypeList,
																Optional: true,
																ForceNew: true,
																Description: `Corresponds to the label values of a reservation resource. This must be the
full resource name of the reservation or reservation block.`,
																Elem: &schema.Schema{
																	Type: schema.TypeString,
																},
															},
														},
													},
												},
												"tpu_topology": {
													Type:     schema.TypeString,
													Optional: true,
													ForceNew: true,
													Description: `The topology of the TPUs. Corresponds to the TPU topologies available from
GKE. (Example: tpu_topology: "2x2x1").`,
												},
											},
										},
									},
									"min_replica_count": {
										Type:     schema.TypeInt,
										Required: true,
										ForceNew: true,
										Description: `The minimum number of machine replicas that will be always deployed on.
This value must be greater than or equal to 1.

If traffic increases, it may dynamically be deployed onto more replicas,
and as traffic decreases, some of these extra replicas may be freed.`,
									},
									"autoscaling_metric_specs": {
										Type:     schema.TypeList,
										Optional: true,
										ForceNew: true,
										Description: `The metric specifications that overrides a resource
utilization metric (CPU utilization, accelerator's duty cycle, and so on)
target value (default to 60 if not set). At most one entry is allowed per
metric.

If machine_spec.accelerator_count is
above 0, the autoscaling will be based on both CPU utilization and
accelerator's duty cycle metrics and scale up when either metrics exceeds
its target value while scale down if both metrics are under their target
value. The default target value is 60 for both metrics.

If machine_spec.accelerator_count is
0, the autoscaling will be based on CPU utilization metric only with
default target value 60 if not explicitly set.

For example, in the case of Online Prediction, if you want to override
target CPU utilization to 80, you should set
autoscaling_metric_specs.metric_name
to 'aiplatform.googleapis.com/prediction/online/cpu/utilization' and
autoscaling_metric_specs.target to '80'.`,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"metric_name": {
													Type:     schema.TypeString,
													Required: true,
													ForceNew: true,
													Description: `The resource metric name.
Supported metrics:

* For Online Prediction:
* 'aiplatform.googleapis.com/prediction/online/accelerator/duty_cycle'
* 'aiplatform.googleapis.com/prediction/online/cpu/utilization'`,
												},
												"target": {
													Type:     schema.TypeInt,
													Optional: true,
													ForceNew: true,
													Description: `The target resource utilization in percentage (1% - 100%) for the given
metric; once the real usage deviates from the target by a certain
percentage, the machine replicas change. The default value is 60
(representing 60%) if not provided.`,
												},
											},
										},
									},
									"max_replica_count": {
										Type:     schema.TypeInt,
										Optional: true,
										ForceNew: true,
										Description: `The maximum number of replicas that may be deployed on when the traffic
against it increases. If the requested value is too large, the deployment
will error, but if deployment succeeds then the ability to scale to that
many replicas is guaranteed (barring service outages). If traffic increases
beyond what its replicas at maximum may handle, a portion of the traffic
will be dropped. If this value is not provided, will use
min_replica_count as the default value.

The value of this field impacts the charge against Vertex CPU and GPU
quotas. Specifically, you will be charged for (max_replica_count *
number of cores in the selected machine type) and (max_replica_count *
number of GPUs per replica in the selected machine type).`,
									},
									"required_replica_count": {
										Type:     schema.TypeInt,
										Optional: true,
										ForceNew: true,
										Description: `Number of required available replicas for the deployment to succeed.
This field is only needed when partial deployment/mutation is
desired. If set, the deploy/mutate operation will succeed once
available_replica_count reaches required_replica_count, and the rest of
the replicas will be retried. If not set, the default
required_replica_count will be min_replica_count.`,
									},
									"spot": {
										Type:     schema.TypeBool,
										Optional: true,
										ForceNew: true,
										Description: `If true, schedule the deployment workload on [spot
VMs](https://cloud.google.com/kubernetes-engine/docs/concepts/spot-vms).`,
									},
								},
							},
						},
						"fast_tryout_enabled": {
							Type:        schema.TypeBool,
							Optional:    true,
							ForceNew:    true,
							Description: `If true, enable the QMT fast tryout feature for this model if possible.`,
						},
						"system_labels": {
							Type:     schema.TypeMap,
							Optional: true,
							ForceNew: true,
							Description: `System labels for Model Garden deployments.
These labels are managed by Google and for tracking purposes only.`,
							Elem: &schema.Schema{Type: schema.TypeString},
						},
					},
				},
			},
			"endpoint_config": {
				Type:        schema.TypeList,
				Optional:    true,
				ForceNew:    true,
				Description: `The endpoint config to use for the deployment.`,
				MaxItems:    1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"dedicated_endpoint_enabled": {
							Type:     schema.TypeBool,
							Optional: true,
							ForceNew: true,
							Description: `If true, the endpoint will be exposed through a dedicated
DNS [Endpoint.dedicated_endpoint_dns]. Your request to the dedicated DNS
will be isolated from other users' traffic and will have better
performance and reliability. Note: Once you enabled dedicated endpoint,
you won't be able to send request to the shared DNS
{region}-aiplatform.googleapis.com. The limitations will be removed soon.`,
						},
						"endpoint_display_name": {
							Type:     schema.TypeString,
							Optional: true,
							ForceNew: true,
							Description: `The user-specified display name of the endpoint. If not set, a
default name will be used.`,
						},
					},
				},
			},
			"hugging_face_model_id": {
				Type:     schema.TypeString,
				Optional: true,
				ForceNew: true,
				Description: `The Hugging Face model to deploy.
Format: Hugging Face model ID like 'google/gemma-2-2b-it'.`,
				ExactlyOneOf: []string{"publisher_model_name", "hugging_face_model_id"},
			},
			"model_config": {
				Type:        schema.TypeList,
				Optional:    true,
				ForceNew:    true,
				Description: `The model config to use for the deployment.`,
				MaxItems:    1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"accept_eula": {
							Type:     schema.TypeBool,
							Optional: true,
							ForceNew: true,
							Description: `Whether the user accepts the End User License Agreement (EULA)
for the model.`,
						},
						"container_spec": {
							Type:     schema.TypeList,
							Optional: true,
							ForceNew: true,
							Description: `Specification of a container for serving predictions. Some fields in this
message correspond to fields in the [Kubernetes Container v1 core
specification](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).`,
							MaxItems: 1,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"image_uri": {
										Type:     schema.TypeString,
										Required: true,
										ForceNew: true,
										Description: `URI of the Docker image to be used as the custom container for serving
predictions. This URI must identify an image in Artifact Registry or
Container Registry. Learn more about the [container publishing
requirements](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#publishing),
including permissions requirements for the Vertex AI Service Agent.

The container image is ingested upon ModelService.UploadModel, stored
internally, and this original path is afterwards not used.

To learn about the requirements for the Docker image itself, see
[Custom container
requirements](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#).

You can use the URI to one of Vertex AI's [pre-built container images for
prediction](https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers)
in this field.`,
									},
									"args": {
										Type:     schema.TypeList,
										Optional: true,
										ForceNew: true,
										Description: `Specifies arguments for the command that runs when the container starts.
This overrides the container's
['CMD'](https://docs.docker.com/engine/reference/builder/#cmd). Specify
this field as an array of executable and arguments, similar to a Docker
'CMD''s "default parameters" form.

If you don't specify this field but do specify the
command field, then the command from the
'command' field runs without any additional arguments. See the
[Kubernetes documentation about how the
'command' and 'args' fields interact with a container's 'ENTRYPOINT' and
'CMD'](https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#notes).

If you don't specify this field and don't specify the 'command' field,
then the container's
['ENTRYPOINT'](https://docs.docker.com/engine/reference/builder/#cmd) and
'CMD' determine what runs based on their default behavior. See the Docker
documentation about [how 'CMD' and 'ENTRYPOINT'
interact](https://docs.docker.com/engine/reference/builder/#understand-how-cmd-and-entrypoint-interact).

In this field, you can reference [environment variables
set by Vertex
AI](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables)
and environment variables set in the env field.
You cannot reference environment variables set in the Docker image. In
order for environment variables to be expanded, reference them by using the
following syntax:$(VARIABLE_NAME)
Note that this differs from Bash variable expansion, which does not use
parentheses. If a variable cannot be resolved, the reference in the input
string is used unchanged. To avoid variable expansion, you can escape this
syntax with '$$'; for example:$$(VARIABLE_NAME)
This field corresponds to the 'args' field of the Kubernetes Containers
[v1 core
API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).`,
										Elem: &schema.Schema{
											Type: schema.TypeString,
										},
									},
									"command": {
										Type:     schema.TypeList,
										Optional: true,
										ForceNew: true,
										Description: `Specifies the command that runs when the container starts. This overrides
the container's
[ENTRYPOINT](https://docs.docker.com/engine/reference/builder/#entrypoint).
Specify this field as an array of executable and arguments, similar to a
Docker 'ENTRYPOINT''s "exec" form, not its "shell" form.

If you do not specify this field, then the container's 'ENTRYPOINT' runs,
in conjunction with the args field or the
container's ['CMD'](https://docs.docker.com/engine/reference/builder/#cmd),
if either exists. If this field is not specified and the container does not
have an 'ENTRYPOINT', then refer to the Docker documentation about [how
'CMD' and 'ENTRYPOINT'
interact](https://docs.docker.com/engine/reference/builder/#understand-how-cmd-and-entrypoint-interact).

If you specify this field, then you can also specify the 'args' field to
provide additional arguments for this command. However, if you specify this
field, then the container's 'CMD' is ignored. See the
[Kubernetes documentation about how the
'command' and 'args' fields interact with a container's 'ENTRYPOINT' and
'CMD'](https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#notes).

In this field, you can reference [environment variables set by Vertex
AI](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables)
and environment variables set in the env field.
You cannot reference environment variables set in the Docker image. In
order for environment variables to be expanded, reference them by using the
following syntax:$(VARIABLE_NAME)
Note that this differs from Bash variable expansion, which does not use
parentheses. If a variable cannot be resolved, the reference in the input
string is used unchanged. To avoid variable expansion, you can escape this
syntax with '$$'; for example:$$(VARIABLE_NAME)
This field corresponds to the 'command' field of the Kubernetes Containers
[v1 core
API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).`,
										Elem: &schema.Schema{
											Type: schema.TypeString,
										},
									},
									"deployment_timeout": {
										Type:     schema.TypeString,
										Optional: true,
										ForceNew: true,
										Description: `Deployment timeout.
Limit for deployment timeout is 2 hours.`,
									},
									"env": {
										Type:     schema.TypeList,
										Optional: true,
										ForceNew: true,
										Description: `List of environment variables to set in the container. After the container
starts running, code running in the container can read these environment
variables.

Additionally, the command and
args fields can reference these variables. Later
entries in this list can also reference earlier entries. For example, the
following example sets the variable 'VAR_2' to have the value 'foo bar':

'''json
[
{
"name": "VAR_1",
"value": "foo"
},
{
"name": "VAR_2",
"value": "$(VAR_1) bar"
}
]
'''

If you switch the order of the variables in the example, then the expansion
does not occur.

This field corresponds to the 'env' field of the Kubernetes Containers
[v1 core
API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).`,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"name": {
													Type:        schema.TypeString,
													Required:    true,
													ForceNew:    true,
													Description: `Name of the environment variable. Must be a valid C identifier.`,
												},
												"value": {
													Type:     schema.TypeString,
													Required: true,
													ForceNew: true,
													Description: `Variables that reference a $(VAR_NAME) are expanded
using the previous defined environment variables in the container and
any service environment variables. If a variable cannot be resolved,
the reference in the input string will be unchanged. The $(VAR_NAME)
syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped
references will never be expanded, regardless of whether the variable
exists or not.`,
												},
											},
										},
									},
									"grpc_ports": {
										Type:     schema.TypeList,
										Optional: true,
										ForceNew: true,
										Description: `List of ports to expose from the container. Vertex AI sends gRPC
prediction requests that it receives to the first port on this list. Vertex
AI also sends liveness and health checks to this port.

If you do not specify this field, gRPC requests to the container will be
disabled.

Vertex AI does not use ports other than the first one listed. This field
corresponds to the 'ports' field of the Kubernetes Containers v1 core API.`,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"container_port": {
													Type:     schema.TypeInt,
													Optional: true,
													ForceNew: true,
													Description: `The number of the port to expose on the pod's IP address.
Must be a valid port number, between 1 and 65535 inclusive.`,
												},
											},
										},
									},
									"health_probe": {
										Type:     schema.TypeList,
										Optional: true,
										ForceNew: true,
										Description: `Probe describes a health check to be performed against a container to
determine whether it is alive or ready to receive traffic.`,
										MaxItems: 1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"exec": {
													Type:        schema.TypeList,
													Optional:    true,
													ForceNew:    true,
													Description: `ExecAction specifies a command to execute.`,
													MaxItems:    1,
													Elem: &schema.Resource{
														Schema: map[string]*schema.Schema{
															"command": {
																Type:     schema.TypeList,
																Optional: true,
																ForceNew: true,
																Description: `Command is the command line to execute inside the container, the working
directory for the command is root ('/') in the container's filesystem.
The command is simply exec'd, it is not run inside a shell, so
traditional shell instructions ('|', etc) won't work. To use a shell, you
need to explicitly call out to that shell. Exit status of 0 is treated as
live/healthy and non-zero is unhealthy.`,
																Elem: &schema.Schema{
																	Type: schema.TypeString,
																},
															},
														},
													},
												},
												"failure_threshold": {
													Type:     schema.TypeInt,
													Optional: true,
													ForceNew: true,
													Description: `Number of consecutive failures before the probe is considered failed.
Defaults to 3. Minimum value is 1.

Maps to Kubernetes probe argument 'failureThreshold'.`,
												},
												"grpc": {
													Type:        schema.TypeList,
													Optional:    true,
													ForceNew:    true,
													Description: `GrpcAction checks the health of a container using a gRPC service.`,
													MaxItems:    1,
													Elem: &schema.Resource{
														Schema: map[string]*schema.Schema{
															"port": {
																Type:        schema.TypeInt,
																Optional:    true,
																ForceNew:    true,
																Description: `Port number of the gRPC service. Number must be in the range 1 to 65535.`,
															},
															"service": {
																Type:     schema.TypeString,
																Optional: true,
																ForceNew: true,
																Description: `Service is the name of the service to place in the gRPC
HealthCheckRequest. See
https://github.com/grpc/grpc/blob/master/doc/health-checking.md.

If this is not specified, the default behavior is defined by gRPC.`,
															},
														},
													},
												},
												"http_get": {
													Type:        schema.TypeList,
													Optional:    true,
													ForceNew:    true,
													Description: `HttpGetAction describes an action based on HTTP Get requests.`,
													MaxItems:    1,
													Elem: &schema.Resource{
														Schema: map[string]*schema.Schema{
															"host": {
																Type:     schema.TypeString,
																Optional: true,
																ForceNew: true,
																Description: `Host name to connect to, defaults to the model serving container's IP.
You probably want to set "Host" in httpHeaders instead.`,
															},
															"http_headers": {
																Type:        schema.TypeList,
																Optional:    true,
																ForceNew:    true,
																Description: `Custom headers to set in the request. HTTP allows repeated headers.`,
																Elem: &schema.Resource{
																	Schema: map[string]*schema.Schema{
																		"name": {
																			Type:     schema.TypeString,
																			Optional: true,
																			ForceNew: true,
																			Description: `The header field name.
This will be canonicalized upon output, so case-variant names will be
understood as the same header.`,
																		},
																		"value": {
																			Type:        schema.TypeString,
																			Optional:    true,
																			ForceNew:    true,
																			Description: `The header field value`,
																		},
																	},
																},
															},
															"path": {
																Type:        schema.TypeString,
																Optional:    true,
																ForceNew:    true,
																Description: `Path to access on the HTTP server.`,
															},
															"port": {
																Type:     schema.TypeInt,
																Optional: true,
																ForceNew: true,
																Description: `Number of the port to access on the container.
Number must be in the range 1 to 65535.`,
															},
															"scheme": {
																Type:     schema.TypeString,
																Optional: true,
																ForceNew: true,
																Description: `Scheme to use for connecting to the host.
Defaults to HTTP. Acceptable values are "HTTP" or "HTTPS".`,
															},
														},
													},
												},
												"initial_delay_seconds": {
													Type:     schema.TypeInt,
													Optional: true,
													ForceNew: true,
													Description: `Number of seconds to wait before starting the probe. Defaults to 0.
Minimum value is 0.

Maps to Kubernetes probe argument 'initialDelaySeconds'.`,
												},
												"period_seconds": {
													Type:     schema.TypeInt,
													Optional: true,
													ForceNew: true,
													Description: `How often (in seconds) to perform the probe. Default to 10 seconds.
Minimum value is 1. Must be less than timeout_seconds.

Maps to Kubernetes probe argument 'periodSeconds'.`,
												},
												"success_threshold": {
													Type:     schema.TypeInt,
													Optional: true,
													ForceNew: true,
													Description: `Number of consecutive successes before the probe is considered successful.
Defaults to 1. Minimum value is 1.

Maps to Kubernetes probe argument 'successThreshold'.`,
												},
												"tcp_socket": {
													Type:     schema.TypeList,
													Optional: true,
													ForceNew: true,
													Description: `TcpSocketAction probes the health of a container by opening a TCP socket
connection.`,
													MaxItems: 1,
													Elem: &schema.Resource{
														Schema: map[string]*schema.Schema{
															"host": {
																Type:     schema.TypeString,
																Optional: true,
																ForceNew: true,
																Description: `Optional: Host name to connect to, defaults to the model serving
container's IP.`,
															},
															"port": {
																Type:     schema.TypeInt,
																Optional: true,
																ForceNew: true,
																Description: `Number of the port to access on the container.
Number must be in the range 1 to 65535.`,
															},
														},
													},
												},
												"timeout_seconds": {
													Type:     schema.TypeInt,
													Optional: true,
													ForceNew: true,
													Description: `Number of seconds after which the probe times out. Defaults to 1 second.
Minimum value is 1. Must be greater or equal to period_seconds.

Maps to Kubernetes probe argument 'timeoutSeconds'.`,
												},
											},
										},
									},
									"health_route": {
										Type:     schema.TypeString,
										Optional: true,
										ForceNew: true,
										Description: `HTTP path on the container to send health checks to. Vertex AI
intermittently sends GET requests to this path on the container's IP
address and port to check that the container is healthy. Read more about
[health
checks](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#health).

For example, if you set this field to '/bar', then Vertex AI
intermittently sends a GET request to the '/bar' path on the port of your
container specified by the first value of this 'ModelContainerSpec''s
ports field.

If you don't specify this field, it defaults to the following value when
you deploy this Model to an Endpoint:/v1/endpoints/ENDPOINT/deployedModels/DEPLOYED_MODEL:predict
The placeholders in this value are replaced as follows:

* ENDPOINT: The last segment (following 'endpoints/')of the
Endpoint.name][] field of the Endpoint where this Model has been
deployed. (Vertex AI makes this value available to your container code
as the ['AIP_ENDPOINT_ID' environment
variable](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables).)

* DEPLOYED_MODEL: DeployedModel.id of the 'DeployedModel'.
(Vertex AI makes this value available to your container code as the
['AIP_DEPLOYED_MODEL_ID' environment
variable](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables).)`,
									},
									"liveness_probe": {
										Type:     schema.TypeList,
										Optional: true,
										ForceNew: true,
										Description: `Probe describes a health check to be performed against a container to
determine whether it is alive or ready to receive traffic.`,
										MaxItems: 1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"exec": {
													Type:        schema.TypeList,
													Optional:    true,
													ForceNew:    true,
													Description: `ExecAction specifies a command to execute.`,
													MaxItems:    1,
													Elem: &schema.Resource{
														Schema: map[string]*schema.Schema{
															"command": {
																Type:     schema.TypeList,
																Optional: true,
																ForceNew: true,
																Description: `Command is the command line to execute inside the container, the working
directory for the command is root ('/') in the container's filesystem.
The command is simply exec'd, it is not run inside a shell, so
traditional shell instructions ('|', etc) won't work. To use a shell, you
need to explicitly call out to that shell. Exit status of 0 is treated as
live/healthy and non-zero is unhealthy.`,
																Elem: &schema.Schema{
																	Type: schema.TypeString,
																},
															},
														},
													},
												},
												"failure_threshold": {
													Type:     schema.TypeInt,
													Optional: true,
													ForceNew: true,
													Description: `Number of consecutive failures before the probe is considered failed.
Defaults to 3. Minimum value is 1.

Maps to Kubernetes probe argument 'failureThreshold'.`,
												},
												"grpc": {
													Type:        schema.TypeList,
													Optional:    true,
													ForceNew:    true,
													Description: `GrpcAction checks the health of a container using a gRPC service.`,
													MaxItems:    1,
													Elem: &schema.Resource{
														Schema: map[string]*schema.Schema{
															"port": {
																Type:        schema.TypeInt,
																Optional:    true,
																ForceNew:    true,
																Description: `Port number of the gRPC service. Number must be in the range 1 to 65535.`,
															},
															"service": {
																Type:     schema.TypeString,
																Optional: true,
																ForceNew: true,
																Description: `Service is the name of the service to place in the gRPC
HealthCheckRequest. See
https://github.com/grpc/grpc/blob/master/doc/health-checking.md.

If this is not specified, the default behavior is defined by gRPC.`,
															},
														},
													},
												},
												"http_get": {
													Type:        schema.TypeList,
													Optional:    true,
													ForceNew:    true,
													Description: `HttpGetAction describes an action based on HTTP Get requests.`,
													MaxItems:    1,
													Elem: &schema.Resource{
														Schema: map[string]*schema.Schema{
															"host": {
																Type:     schema.TypeString,
																Optional: true,
																ForceNew: true,
																Description: `Host name to connect to, defaults to the model serving container's IP.
You probably want to set "Host" in httpHeaders instead.`,
															},
															"http_headers": {
																Type:        schema.TypeList,
																Optional:    true,
																ForceNew:    true,
																Description: `Custom headers to set in the request. HTTP allows repeated headers.`,
																Elem: &schema.Resource{
																	Schema: map[string]*schema.Schema{
																		"name": {
																			Type:     schema.TypeString,
																			Optional: true,
																			ForceNew: true,
																			Description: `The header field name.
This will be canonicalized upon output, so case-variant names will be
understood as the same header.`,
																		},
																		"value": {
																			Type:        schema.TypeString,
																			Optional:    true,
																			ForceNew:    true,
																			Description: `The header field value`,
																		},
																	},
																},
															},
															"path": {
																Type:        schema.TypeString,
																Optional:    true,
																ForceNew:    true,
																Description: `Path to access on the HTTP server.`,
															},
															"port": {
																Type:     schema.TypeInt,
																Optional: true,
																ForceNew: true,
																Description: `Number of the port to access on the container.
Number must be in the range 1 to 65535.`,
															},
															"scheme": {
																Type:     schema.TypeString,
																Optional: true,
																ForceNew: true,
																Description: `Scheme to use for connecting to the host.
Defaults to HTTP. Acceptable values are "HTTP" or "HTTPS".`,
															},
														},
													},
												},
												"initial_delay_seconds": {
													Type:     schema.TypeInt,
													Optional: true,
													ForceNew: true,
													Description: `Number of seconds to wait before starting the probe. Defaults to 0.
Minimum value is 0.

Maps to Kubernetes probe argument 'initialDelaySeconds'.`,
												},
												"period_seconds": {
													Type:     schema.TypeInt,
													Optional: true,
													ForceNew: true,
													Description: `How often (in seconds) to perform the probe. Default to 10 seconds.
Minimum value is 1. Must be less than timeout_seconds.

Maps to Kubernetes probe argument 'periodSeconds'.`,
												},
												"success_threshold": {
													Type:     schema.TypeInt,
													Optional: true,
													ForceNew: true,
													Description: `Number of consecutive successes before the probe is considered successful.
Defaults to 1. Minimum value is 1.

Maps to Kubernetes probe argument 'successThreshold'.`,
												},
												"tcp_socket": {
													Type:     schema.TypeList,
													Optional: true,
													ForceNew: true,
													Description: `TcpSocketAction probes the health of a container by opening a TCP socket
connection.`,
													MaxItems: 1,
													Elem: &schema.Resource{
														Schema: map[string]*schema.Schema{
															"host": {
																Type:     schema.TypeString,
																Optional: true,
																ForceNew: true,
																Description: `Optional: Host name to connect to, defaults to the model serving
container's IP.`,
															},
															"port": {
																Type:     schema.TypeInt,
																Optional: true,
																ForceNew: true,
																Description: `Number of the port to access on the container.
Number must be in the range 1 to 65535.`,
															},
														},
													},
												},
												"timeout_seconds": {
													Type:     schema.TypeInt,
													Optional: true,
													ForceNew: true,
													Description: `Number of seconds after which the probe times out. Defaults to 1 second.
Minimum value is 1. Must be greater or equal to period_seconds.

Maps to Kubernetes probe argument 'timeoutSeconds'.`,
												},
											},
										},
									},
									"ports": {
										Type:     schema.TypeList,
										Optional: true,
										ForceNew: true,
										Description: `List of ports to expose from the container. Vertex AI sends any
prediction requests that it receives to the first port on this list. Vertex
AI also sends
[liveness and health
checks](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#liveness)
to this port.

If you do not specify this field, it defaults to following value:

'''json
[
{
"containerPort": 8080
}
]
'''

Vertex AI does not use ports other than the first one listed. This field
corresponds to the 'ports' field of the Kubernetes Containers
[v1 core
API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).`,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"container_port": {
													Type:     schema.TypeInt,
													Optional: true,
													ForceNew: true,
													Description: `The number of the port to expose on the pod's IP address.
Must be a valid port number, between 1 and 65535 inclusive.`,
												},
											},
										},
									},
									"predict_route": {
										Type:     schema.TypeString,
										Optional: true,
										ForceNew: true,
										Description: `HTTP path on the container to send prediction requests to. Vertex AI
forwards requests sent using
projects.locations.endpoints.predict to this
path on the container's IP address and port. Vertex AI then returns the
container's response in the API response.

For example, if you set this field to '/foo', then when Vertex AI
receives a prediction request, it forwards the request body in a POST
request to the '/foo' path on the port of your container specified by the
first value of this 'ModelContainerSpec''s
ports field.

If you don't specify this field, it defaults to the following value when
you deploy this Model to an Endpoint:/v1/endpoints/ENDPOINT/deployedModels/DEPLOYED_MODEL:predict
The placeholders in this value are replaced as follows:

* ENDPOINT: The last segment (following 'endpoints/')of the
Endpoint.name][] field of the Endpoint where this Model has been
deployed. (Vertex AI makes this value available to your container code
as the ['AIP_ENDPOINT_ID' environment
variable](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables).)

* DEPLOYED_MODEL: DeployedModel.id of the 'DeployedModel'.
(Vertex AI makes this value available to your container code
as the ['AIP_DEPLOYED_MODEL_ID' environment
variable](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables).)`,
									},
									"shared_memory_size_mb": {
										Type:     schema.TypeString,
										Optional: true,
										ForceNew: true,
										Description: `The amount of the VM memory to reserve as the shared memory for the model
in megabytes.`,
									},
									"startup_probe": {
										Type:     schema.TypeList,
										Optional: true,
										ForceNew: true,
										Description: `Probe describes a health check to be performed against a container to
determine whether it is alive or ready to receive traffic.`,
										MaxItems: 1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"exec": {
													Type:        schema.TypeList,
													Optional:    true,
													ForceNew:    true,
													Description: `ExecAction specifies a command to execute.`,
													MaxItems:    1,
													Elem: &schema.Resource{
														Schema: map[string]*schema.Schema{
															"command": {
																Type:     schema.TypeList,
																Optional: true,
																ForceNew: true,
																Description: `Command is the command line to execute inside the container, the working
directory for the command is root ('/') in the container's filesystem.
The command is simply exec'd, it is not run inside a shell, so
traditional shell instructions ('|', etc) won't work. To use a shell, you
need to explicitly call out to that shell. Exit status of 0 is treated as
live/healthy and non-zero is unhealthy.`,
																Elem: &schema.Schema{
																	Type: schema.TypeString,
																},
															},
														},
													},
												},
												"failure_threshold": {
													Type:     schema.TypeInt,
													Optional: true,
													ForceNew: true,
													Description: `Number of consecutive failures before the probe is considered failed.
Defaults to 3. Minimum value is 1.

Maps to Kubernetes probe argument 'failureThreshold'.`,
												},
												"grpc": {
													Type:        schema.TypeList,
													Optional:    true,
													ForceNew:    true,
													Description: `GrpcAction checks the health of a container using a gRPC service.`,
													MaxItems:    1,
													Elem: &schema.Resource{
														Schema: map[string]*schema.Schema{
															"port": {
																Type:        schema.TypeInt,
																Optional:    true,
																ForceNew:    true,
																Description: `Port number of the gRPC service. Number must be in the range 1 to 65535.`,
															},
															"service": {
																Type:     schema.TypeString,
																Optional: true,
																ForceNew: true,
																Description: `Service is the name of the service to place in the gRPC
HealthCheckRequest. See
https://github.com/grpc/grpc/blob/master/doc/health-checking.md.

If this is not specified, the default behavior is defined by gRPC.`,
															},
														},
													},
												},
												"http_get": {
													Type:        schema.TypeList,
													Optional:    true,
													ForceNew:    true,
													Description: `HttpGetAction describes an action based on HTTP Get requests.`,
													MaxItems:    1,
													Elem: &schema.Resource{
														Schema: map[string]*schema.Schema{
															"host": {
																Type:     schema.TypeString,
																Optional: true,
																ForceNew: true,
																Description: `Host name to connect to, defaults to the model serving container's IP.
You probably want to set "Host" in httpHeaders instead.`,
															},
															"http_headers": {
																Type:        schema.TypeList,
																Optional:    true,
																ForceNew:    true,
																Description: `Custom headers to set in the request. HTTP allows repeated headers.`,
																Elem: &schema.Resource{
																	Schema: map[string]*schema.Schema{
																		"name": {
																			Type:     schema.TypeString,
																			Optional: true,
																			ForceNew: true,
																			Description: `The header field name.
This will be canonicalized upon output, so case-variant names will be
understood as the same header.`,
																		},
																		"value": {
																			Type:        schema.TypeString,
																			Optional:    true,
																			ForceNew:    true,
																			Description: `The header field value`,
																		},
																	},
																},
															},
															"path": {
																Type:        schema.TypeString,
																Optional:    true,
																ForceNew:    true,
																Description: `Path to access on the HTTP server.`,
															},
															"port": {
																Type:     schema.TypeInt,
																Optional: true,
																ForceNew: true,
																Description: `Number of the port to access on the container.
Number must be in the range 1 to 65535.`,
															},
															"scheme": {
																Type:     schema.TypeString,
																Optional: true,
																ForceNew: true,
																Description: `Scheme to use for connecting to the host.
Defaults to HTTP. Acceptable values are "HTTP" or "HTTPS".`,
															},
														},
													},
												},
												"initial_delay_seconds": {
													Type:     schema.TypeInt,
													Optional: true,
													ForceNew: true,
													Description: `Number of seconds to wait before starting the probe. Defaults to 0.
Minimum value is 0.

Maps to Kubernetes probe argument 'initialDelaySeconds'.`,
												},
												"period_seconds": {
													Type:     schema.TypeInt,
													Optional: true,
													ForceNew: true,
													Description: `How often (in seconds) to perform the probe. Default to 10 seconds.
Minimum value is 1. Must be less than timeout_seconds.

Maps to Kubernetes probe argument 'periodSeconds'.`,
												},
												"success_threshold": {
													Type:     schema.TypeInt,
													Optional: true,
													ForceNew: true,
													Description: `Number of consecutive successes before the probe is considered successful.
Defaults to 1. Minimum value is 1.

Maps to Kubernetes probe argument 'successThreshold'.`,
												},
												"tcp_socket": {
													Type:     schema.TypeList,
													Optional: true,
													ForceNew: true,
													Description: `TcpSocketAction probes the health of a container by opening a TCP socket
connection.`,
													MaxItems: 1,
													Elem: &schema.Resource{
														Schema: map[string]*schema.Schema{
															"host": {
																Type:     schema.TypeString,
																Optional: true,
																ForceNew: true,
																Description: `Optional: Host name to connect to, defaults to the model serving
container's IP.`,
															},
															"port": {
																Type:     schema.TypeInt,
																Optional: true,
																ForceNew: true,
																Description: `Number of the port to access on the container.
Number must be in the range 1 to 65535.`,
															},
														},
													},
												},
												"timeout_seconds": {
													Type:     schema.TypeInt,
													Optional: true,
													ForceNew: true,
													Description: `Number of seconds after which the probe times out. Defaults to 1 second.
Minimum value is 1. Must be greater or equal to period_seconds.

Maps to Kubernetes probe argument 'timeoutSeconds'.`,
												},
											},
										},
									},
								},
							},
						},
						"hugging_face_access_token": {
							Type:     schema.TypeString,
							Optional: true,
							ForceNew: true,
							Description: `The Hugging Face read access token used to access the model
artifacts of gated models.`,
						},
						"hugging_face_cache_enabled": {
							Type:     schema.TypeBool,
							Optional: true,
							ForceNew: true,
							Description: `If true, the model will deploy with a cached version instead of directly
downloading the model artifacts from Hugging Face. This is suitable for
VPC-SC users with limited internet access.`,
						},
						"model_display_name": {
							Type:     schema.TypeString,
							Optional: true,
							ForceNew: true,
							Description: `The user-specified display name of the uploaded model. If not
set, a default name will be used.`,
						},
					},
				},
			},
			"publisher_model_name": {
				Type:     schema.TypeString,
				Optional: true,
				ForceNew: true,
				Description: `The Model Garden model to deploy.
Format:
'publishers/{publisher}/models/{publisher_model}@{version_id}', or
'publishers/hf-{hugging-face-author}/models/{hugging-face-model-name}@001'.`,
				ExactlyOneOf: []string{"publisher_model_name", "hugging_face_model_id"},
			},
			"endpoint": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `Resource ID segment making up resource 'endpoint'. It identifies the resource within its parent collection as described in https://google.aip.dev/122.`,
			},
			"project": {
				Type:     schema.TypeString,
				Optional: true,
				Computed: true,
				ForceNew: true,
			},
		},
		UseJSONNumber: true,
	}
}

func resourceVertexAIEndpointWithModelGardenDeploymentCreate(d *schema.ResourceData, meta interface{}) error {
	config := meta.(*transport_tpg.Config)
	userAgent, err := tpgresource.GenerateUserAgentString(d, config.UserAgent)
	if err != nil {
		return err
	}

	obj := make(map[string]interface{})
	publisherModelNameProp, err := expandVertexAIEndpointWithModelGardenDeploymentPublisherModelName(d.Get("publisher_model_name"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("publisher_model_name"); !tpgresource.IsEmptyValue(reflect.ValueOf(publisherModelNameProp)) && (ok || !reflect.DeepEqual(v, publisherModelNameProp)) {
		obj["publisherModelName"] = publisherModelNameProp
	}
	huggingFaceModelIdProp, err := expandVertexAIEndpointWithModelGardenDeploymentHuggingFaceModelId(d.Get("hugging_face_model_id"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("hugging_face_model_id"); !tpgresource.IsEmptyValue(reflect.ValueOf(huggingFaceModelIdProp)) && (ok || !reflect.DeepEqual(v, huggingFaceModelIdProp)) {
		obj["huggingFaceModelId"] = huggingFaceModelIdProp
	}
	modelConfigProp, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfig(d.Get("model_config"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("model_config"); !tpgresource.IsEmptyValue(reflect.ValueOf(modelConfigProp)) && (ok || !reflect.DeepEqual(v, modelConfigProp)) {
		obj["modelConfig"] = modelConfigProp
	}
	endpointConfigProp, err := expandVertexAIEndpointWithModelGardenDeploymentEndpointConfig(d.Get("endpoint_config"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("endpoint_config"); !tpgresource.IsEmptyValue(reflect.ValueOf(endpointConfigProp)) && (ok || !reflect.DeepEqual(v, endpointConfigProp)) {
		obj["endpointConfig"] = endpointConfigProp
	}
	deployConfigProp, err := expandVertexAIEndpointWithModelGardenDeploymentDeployConfig(d.Get("deploy_config"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("deploy_config"); !tpgresource.IsEmptyValue(reflect.ValueOf(deployConfigProp)) && (ok || !reflect.DeepEqual(v, deployConfigProp)) {
		obj["deployConfig"] = deployConfigProp
	}

	url, err := tpgresource.ReplaceVars(d, config, "{{VertexAIBasePath}}projects/{{project}}/locations/{{location}}:deploy")
	if err != nil {
		return err
	}

	log.Printf("[DEBUG] Creating new EndpointWithModelGardenDeployment: %#v", obj)
	billingProject := ""

	project, err := tpgresource.GetProject(d, config)
	if err != nil {
		return fmt.Errorf("Error fetching project for EndpointWithModelGardenDeployment: %s", err)
	}
	billingProject = project

	// err == nil indicates that the billing_project value was found
	if bp, err := tpgresource.GetBillingProject(d, config); err == nil {
		billingProject = bp
	}

	headers := make(http.Header)
	res, err := transport_tpg.SendRequest(transport_tpg.SendRequestOptions{
		Config:    config,
		Method:    "POST",
		Project:   billingProject,
		RawURL:    url,
		UserAgent: userAgent,
		Body:      obj,
		Timeout:   d.Timeout(schema.TimeoutCreate),
		Headers:   headers,
	})
	if err != nil {
		return fmt.Errorf("Error creating EndpointWithModelGardenDeployment: %s", err)
	}

	// Store the ID now
	id, err := tpgresource.ReplaceVars(d, config, "projects/{{project}}/locations/{{location}}/endpoints/{{endpoint}}")
	if err != nil {
		return fmt.Errorf("Error constructing id: %s", err)
	}
	d.SetId(id)

	err = VertexAIOperationWaitTime(
		config, res, project, "Creating EndpointWithModelGardenDeployment", userAgent,
		d.Timeout(schema.TimeoutCreate))

	if err != nil {
		// The resource didn't actually create
		d.SetId("")
		return fmt.Errorf("Error waiting to create EndpointWithModelGardenDeployment: %s", err)
	}

	log.Printf("[DEBUG] Beginning post_create for Vertex AI Endpoint with Model Garden Deployment")

	// Log Terraform resource data
	log.Printf("[DEBUG] Terraform Resource Data (d): ID=%s", d.Id())
	for key, val := range d.State().Attributes {
		log.Printf("[DEBUG] d.State().Attributes[%s] = %s", key, val)
	}
	log.Printf("[DEBUG] d.Get(\"project\") = %v", d.Get("project"))
	log.Printf("[DEBUG] d.Get(\"location\") = %v", d.Get("location"))
	log.Printf("[DEBUG] d.Get(\"publisher_model_name\") = %v", d.Get("publisher_model_name"))

	// Log res structure
	log.Printf("[DEBUG] Top-level keys in res:")
	for k := range res {
		log.Printf("[DEBUG]   - %s", k)
	}

	// Declare and populate opRes
	var opRes map[string]interface{}
	err = VertexAIOperationWaitTimeWithResponse(
		config, res, &opRes, d.Get("project").(string), "Vertex AI deployModel operation", userAgent,
		d.Timeout(schema.TimeoutCreate),
	)
	if err != nil {
		d.SetId("")
		return fmt.Errorf("Error waiting for deploy operation: %s", err)
	}

	// Log keys in opRes
	log.Printf("[DEBUG] opRes successfully retrieved. Keys:")
	for k := range opRes {
		log.Printf("[DEBUG]   - %s", k)
	}

	// Extract full endpoint resource name
	endpointFull, ok := opRes["endpoint"].(string)
	if !ok || endpointFull == "" {
		log.Printf("[ERROR] 'endpoint' not found or empty in opRes. Full opRes: %#v", opRes)
		return fmt.Errorf("Create response didn't contain 'endpoint'. Create may not have succeeded.")
	}
	log.Printf("[DEBUG] Extracted full endpoint from opRes: %s", endpointFull)

	// Check format and extract endpoint name without strict project name match
	parts := strings.Split(endpointFull, "/")
	if len(parts) != 6 || parts[0] != "projects" || parts[2] != "locations" || parts[4] != "endpoints" {
		log.Printf("[ERROR] Unexpected endpoint format. Got: %s", endpointFull)
		return fmt.Errorf("unexpected format for endpoint: %s", endpointFull)
	}
	endpoint := parts[5]
	log.Printf("[DEBUG] Parsed endpoint ID: %s", endpoint)

	// Set Terraform fields
	if err := d.Set("endpoint", endpoint); err != nil {
		return fmt.Errorf("Error setting endpoint: %s", err)
	}
	d.SetId(endpointFull)
	log.Printf("[DEBUG] Set Terraform resource ID to: %s", endpointFull)

	return nil

	log.Printf("[DEBUG] Finished creating EndpointWithModelGardenDeployment %q: %#v", d.Id(), res)

	return resourceVertexAIEndpointWithModelGardenDeploymentRead(d, meta)
}

func resourceVertexAIEndpointWithModelGardenDeploymentRead(d *schema.ResourceData, meta interface{}) error {
	// This resource could not be read from the API.
	return nil
}

func resourceVertexAIEndpointWithModelGardenDeploymentDelete(d *schema.ResourceData, meta interface{}) error {
	log.Printf("[WARNING] VertexAI EndpointWithModelGardenDeployment resources"+
		" cannot be deleted from Google Cloud. The resource %s will be removed from Terraform"+
		" state, but will still be present on Google Cloud.", d.Id())
	d.SetId("")

	return nil
}

func flattenVertexAIEndpointWithModelGardenDeploymentPublisherModelName(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentHuggingFaceModelId(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfig(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["hugging_face_cache_enabled"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigHuggingFaceCacheEnabled(original["huggingFaceCacheEnabled"], d, config)
	transformed["model_display_name"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigModelDisplayName(original["modelDisplayName"], d, config)
	transformed["container_spec"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpec(original["containerSpec"], d, config)
	transformed["accept_eula"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigAcceptEula(original["acceptEula"], d, config)
	transformed["hugging_face_access_token"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigHuggingFaceAccessToken(original["huggingFaceAccessToken"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigHuggingFaceCacheEnabled(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigModelDisplayName(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpec(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["ports"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecPorts(original["ports"], d, config)
	transformed["predict_route"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecPredictRoute(original["predictRoute"], d, config)
	transformed["health_route"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthRoute(original["healthRoute"], d, config)
	transformed["deployment_timeout"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecDeploymentTimeout(original["deploymentTimeout"], d, config)
	transformed["startup_probe"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbe(original["startupProbe"], d, config)
	transformed["health_probe"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbe(original["healthProbe"], d, config)
	transformed["image_uri"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecImageUri(original["imageUri"], d, config)
	transformed["command"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecCommand(original["command"], d, config)
	transformed["args"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecArgs(original["args"], d, config)
	transformed["grpc_ports"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecGrpcPorts(original["grpcPorts"], d, config)
	transformed["shared_memory_size_mb"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecSharedMemorySizeMb(original["sharedMemorySizeMb"], d, config)
	transformed["liveness_probe"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbe(original["livenessProbe"], d, config)
	transformed["env"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecEnv(original["env"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecPorts(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}
	l := v.([]interface{})
	transformed := make([]interface{}, 0, len(l))
	for _, raw := range l {
		original := raw.(map[string]interface{})
		if len(original) < 1 {
			// Do not include empty json objects coming back from the api
			continue
		}
		transformed = append(transformed, map[string]interface{}{
			"container_port": flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecPortsContainerPort(original["containerPort"], d, config),
		})
	}
	return transformed
}
func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecPortsContainerPort(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecPredictRoute(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthRoute(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecDeploymentTimeout(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbe(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["exec"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeExec(original["exec"], d, config)
	transformed["http_get"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeHttpGet(original["httpGet"], d, config)
	transformed["grpc"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeGrpc(original["grpc"], d, config)
	transformed["tcp_socket"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeTcpSocket(original["tcpSocket"], d, config)
	transformed["timeout_seconds"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeTimeoutSeconds(original["timeoutSeconds"], d, config)
	transformed["success_threshold"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeSuccessThreshold(original["successThreshold"], d, config)
	transformed["initial_delay_seconds"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeInitialDelaySeconds(original["initialDelaySeconds"], d, config)
	transformed["period_seconds"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbePeriodSeconds(original["periodSeconds"], d, config)
	transformed["failure_threshold"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeFailureThreshold(original["failureThreshold"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeExec(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["command"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeExecCommand(original["command"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeExecCommand(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeHttpGet(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["path"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeHttpGetPath(original["path"], d, config)
	transformed["port"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeHttpGetPort(original["port"], d, config)
	transformed["host"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeHttpGetHost(original["host"], d, config)
	transformed["scheme"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeHttpGetScheme(original["scheme"], d, config)
	transformed["http_headers"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeHttpGetHttpHeaders(original["httpHeaders"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeHttpGetPath(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeHttpGetPort(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeHttpGetHost(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeHttpGetScheme(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeHttpGetHttpHeaders(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}
	l := v.([]interface{})
	transformed := make([]interface{}, 0, len(l))
	for _, raw := range l {
		original := raw.(map[string]interface{})
		if len(original) < 1 {
			// Do not include empty json objects coming back from the api
			continue
		}
		transformed = append(transformed, map[string]interface{}{
			"value": flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeHttpGetHttpHeadersValue(original["value"], d, config),
			"name":  flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeHttpGetHttpHeadersName(original["name"], d, config),
		})
	}
	return transformed
}
func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeHttpGetHttpHeadersValue(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeHttpGetHttpHeadersName(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeGrpc(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["port"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeGrpcPort(original["port"], d, config)
	transformed["service"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeGrpcService(original["service"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeGrpcPort(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeGrpcService(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeTcpSocket(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["port"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeTcpSocketPort(original["port"], d, config)
	transformed["host"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeTcpSocketHost(original["host"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeTcpSocketPort(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeTcpSocketHost(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeTimeoutSeconds(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeSuccessThreshold(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeInitialDelaySeconds(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbePeriodSeconds(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeFailureThreshold(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbe(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["exec"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeExec(original["exec"], d, config)
	transformed["http_get"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeHttpGet(original["httpGet"], d, config)
	transformed["grpc"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeGrpc(original["grpc"], d, config)
	transformed["tcp_socket"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeTcpSocket(original["tcpSocket"], d, config)
	transformed["timeout_seconds"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeTimeoutSeconds(original["timeoutSeconds"], d, config)
	transformed["success_threshold"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeSuccessThreshold(original["successThreshold"], d, config)
	transformed["initial_delay_seconds"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeInitialDelaySeconds(original["initialDelaySeconds"], d, config)
	transformed["period_seconds"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbePeriodSeconds(original["periodSeconds"], d, config)
	transformed["failure_threshold"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeFailureThreshold(original["failureThreshold"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeExec(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["command"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeExecCommand(original["command"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeExecCommand(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeHttpGet(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["path"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeHttpGetPath(original["path"], d, config)
	transformed["port"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeHttpGetPort(original["port"], d, config)
	transformed["host"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeHttpGetHost(original["host"], d, config)
	transformed["scheme"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeHttpGetScheme(original["scheme"], d, config)
	transformed["http_headers"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeHttpGetHttpHeaders(original["httpHeaders"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeHttpGetPath(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeHttpGetPort(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeHttpGetHost(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeHttpGetScheme(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeHttpGetHttpHeaders(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}
	l := v.([]interface{})
	transformed := make([]interface{}, 0, len(l))
	for _, raw := range l {
		original := raw.(map[string]interface{})
		if len(original) < 1 {
			// Do not include empty json objects coming back from the api
			continue
		}
		transformed = append(transformed, map[string]interface{}{
			"name":  flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeHttpGetHttpHeadersName(original["name"], d, config),
			"value": flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeHttpGetHttpHeadersValue(original["value"], d, config),
		})
	}
	return transformed
}
func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeHttpGetHttpHeadersName(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeHttpGetHttpHeadersValue(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeGrpc(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["port"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeGrpcPort(original["port"], d, config)
	transformed["service"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeGrpcService(original["service"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeGrpcPort(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeGrpcService(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeTcpSocket(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["port"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeTcpSocketPort(original["port"], d, config)
	transformed["host"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeTcpSocketHost(original["host"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeTcpSocketPort(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeTcpSocketHost(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeTimeoutSeconds(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeSuccessThreshold(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeInitialDelaySeconds(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbePeriodSeconds(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeFailureThreshold(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecImageUri(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecCommand(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecArgs(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecGrpcPorts(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}
	l := v.([]interface{})
	transformed := make([]interface{}, 0, len(l))
	for _, raw := range l {
		original := raw.(map[string]interface{})
		if len(original) < 1 {
			// Do not include empty json objects coming back from the api
			continue
		}
		transformed = append(transformed, map[string]interface{}{
			"container_port": flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecGrpcPortsContainerPort(original["containerPort"], d, config),
		})
	}
	return transformed
}
func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecGrpcPortsContainerPort(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecSharedMemorySizeMb(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbe(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["exec"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeExec(original["exec"], d, config)
	transformed["http_get"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeHttpGet(original["httpGet"], d, config)
	transformed["grpc"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeGrpc(original["grpc"], d, config)
	transformed["tcp_socket"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeTcpSocket(original["tcpSocket"], d, config)
	transformed["timeout_seconds"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeTimeoutSeconds(original["timeoutSeconds"], d, config)
	transformed["success_threshold"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeSuccessThreshold(original["successThreshold"], d, config)
	transformed["initial_delay_seconds"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeInitialDelaySeconds(original["initialDelaySeconds"], d, config)
	transformed["period_seconds"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbePeriodSeconds(original["periodSeconds"], d, config)
	transformed["failure_threshold"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeFailureThreshold(original["failureThreshold"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeExec(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["command"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeExecCommand(original["command"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeExecCommand(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeHttpGet(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["path"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeHttpGetPath(original["path"], d, config)
	transformed["port"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeHttpGetPort(original["port"], d, config)
	transformed["host"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeHttpGetHost(original["host"], d, config)
	transformed["scheme"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeHttpGetScheme(original["scheme"], d, config)
	transformed["http_headers"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeHttpGetHttpHeaders(original["httpHeaders"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeHttpGetPath(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeHttpGetPort(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeHttpGetHost(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeHttpGetScheme(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeHttpGetHttpHeaders(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}
	l := v.([]interface{})
	transformed := make([]interface{}, 0, len(l))
	for _, raw := range l {
		original := raw.(map[string]interface{})
		if len(original) < 1 {
			// Do not include empty json objects coming back from the api
			continue
		}
		transformed = append(transformed, map[string]interface{}{
			"name":  flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeHttpGetHttpHeadersName(original["name"], d, config),
			"value": flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeHttpGetHttpHeadersValue(original["value"], d, config),
		})
	}
	return transformed
}
func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeHttpGetHttpHeadersName(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeHttpGetHttpHeadersValue(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeGrpc(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["service"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeGrpcService(original["service"], d, config)
	transformed["port"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeGrpcPort(original["port"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeGrpcService(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeGrpcPort(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeTcpSocket(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["port"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeTcpSocketPort(original["port"], d, config)
	transformed["host"] =
		flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeTcpSocketHost(original["host"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeTcpSocketPort(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeTcpSocketHost(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeTimeoutSeconds(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeSuccessThreshold(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeInitialDelaySeconds(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbePeriodSeconds(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeFailureThreshold(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecEnv(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}
	l := v.([]interface{})
	transformed := make([]interface{}, 0, len(l))
	for _, raw := range l {
		original := raw.(map[string]interface{})
		if len(original) < 1 {
			// Do not include empty json objects coming back from the api
			continue
		}
		transformed = append(transformed, map[string]interface{}{
			"name":  flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecEnvName(original["name"], d, config),
			"value": flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecEnvValue(original["value"], d, config),
		})
	}
	return transformed
}
func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecEnvName(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecEnvValue(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigAcceptEula(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentModelConfigHuggingFaceAccessToken(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentEndpointConfig(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["endpoint_display_name"] =
		flattenVertexAIEndpointWithModelGardenDeploymentEndpointConfigEndpointDisplayName(original["endpointDisplayName"], d, config)
	transformed["dedicated_endpoint_enabled"] =
		flattenVertexAIEndpointWithModelGardenDeploymentEndpointConfigDedicatedEndpointEnabled(original["dedicatedEndpointEnabled"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIEndpointWithModelGardenDeploymentEndpointConfigEndpointDisplayName(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentEndpointConfigDedicatedEndpointEnabled(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentDeployConfig(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["system_labels"] =
		flattenVertexAIEndpointWithModelGardenDeploymentDeployConfigSystemLabels(original["systemLabels"], d, config)
	transformed["dedicated_resources"] =
		flattenVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResources(original["dedicatedResources"], d, config)
	transformed["fast_tryout_enabled"] =
		flattenVertexAIEndpointWithModelGardenDeploymentDeployConfigFastTryoutEnabled(original["fastTryoutEnabled"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIEndpointWithModelGardenDeploymentDeployConfigSystemLabels(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResources(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["machine_spec"] =
		flattenVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpec(original["machineSpec"], d, config)
	transformed["min_replica_count"] =
		flattenVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMinReplicaCount(original["minReplicaCount"], d, config)
	transformed["max_replica_count"] =
		flattenVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMaxReplicaCount(original["maxReplicaCount"], d, config)
	transformed["required_replica_count"] =
		flattenVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesRequiredReplicaCount(original["requiredReplicaCount"], d, config)
	transformed["autoscaling_metric_specs"] =
		flattenVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesAutoscalingMetricSpecs(original["autoscalingMetricSpecs"], d, config)
	transformed["spot"] =
		flattenVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesSpot(original["spot"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpec(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["reservation_affinity"] =
		flattenVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecReservationAffinity(original["reservationAffinity"], d, config)
	transformed["machine_type"] =
		flattenVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecMachineType(original["machineType"], d, config)
	transformed["accelerator_type"] =
		flattenVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecAcceleratorType(original["acceleratorType"], d, config)
	transformed["accelerator_count"] =
		flattenVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecAcceleratorCount(original["acceleratorCount"], d, config)
	transformed["tpu_topology"] =
		flattenVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecTpuTopology(original["tpuTopology"], d, config)
	transformed["multihost_gpu_node_count"] =
		flattenVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecMultihostGpuNodeCount(original["multihostGpuNodeCount"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecReservationAffinity(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["reservation_affinity_type"] =
		flattenVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecReservationAffinityReservationAffinityType(original["reservationAffinityType"], d, config)
	transformed["key"] =
		flattenVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecReservationAffinityKey(original["key"], d, config)
	transformed["values"] =
		flattenVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecReservationAffinityValues(original["values"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecReservationAffinityReservationAffinityType(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecReservationAffinityKey(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecReservationAffinityValues(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecMachineType(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecAcceleratorType(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecAcceleratorCount(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecTpuTopology(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecMultihostGpuNodeCount(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMinReplicaCount(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMaxReplicaCount(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesRequiredReplicaCount(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesAutoscalingMetricSpecs(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}
	l := v.([]interface{})
	transformed := make([]interface{}, 0, len(l))
	for _, raw := range l {
		original := raw.(map[string]interface{})
		if len(original) < 1 {
			// Do not include empty json objects coming back from the api
			continue
		}
		transformed = append(transformed, map[string]interface{}{
			"metric_name": flattenVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesAutoscalingMetricSpecsMetricName(original["metricName"], d, config),
			"target":      flattenVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesAutoscalingMetricSpecsTarget(original["target"], d, config),
		})
	}
	return transformed
}
func flattenVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesAutoscalingMetricSpecsMetricName(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesAutoscalingMetricSpecsTarget(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesSpot(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIEndpointWithModelGardenDeploymentDeployConfigFastTryoutEnabled(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func expandVertexAIEndpointWithModelGardenDeploymentPublisherModelName(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentHuggingFaceModelId(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfig(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedHuggingFaceCacheEnabled, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigHuggingFaceCacheEnabled(original["hugging_face_cache_enabled"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedHuggingFaceCacheEnabled); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["huggingFaceCacheEnabled"] = transformedHuggingFaceCacheEnabled
	}

	transformedModelDisplayName, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigModelDisplayName(original["model_display_name"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedModelDisplayName); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["modelDisplayName"] = transformedModelDisplayName
	}

	transformedContainerSpec, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpec(original["container_spec"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedContainerSpec); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["containerSpec"] = transformedContainerSpec
	}

	transformedAcceptEula, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigAcceptEula(original["accept_eula"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedAcceptEula); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["acceptEula"] = transformedAcceptEula
	}

	transformedHuggingFaceAccessToken, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigHuggingFaceAccessToken(original["hugging_face_access_token"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedHuggingFaceAccessToken); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["huggingFaceAccessToken"] = transformedHuggingFaceAccessToken
	}

	return transformed, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigHuggingFaceCacheEnabled(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigModelDisplayName(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpec(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedPorts, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecPorts(original["ports"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedPorts); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["ports"] = transformedPorts
	}

	transformedPredictRoute, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecPredictRoute(original["predict_route"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedPredictRoute); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["predictRoute"] = transformedPredictRoute
	}

	transformedHealthRoute, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthRoute(original["health_route"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedHealthRoute); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["healthRoute"] = transformedHealthRoute
	}

	transformedDeploymentTimeout, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecDeploymentTimeout(original["deployment_timeout"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedDeploymentTimeout); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["deploymentTimeout"] = transformedDeploymentTimeout
	}

	transformedStartupProbe, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbe(original["startup_probe"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedStartupProbe); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["startupProbe"] = transformedStartupProbe
	}

	transformedHealthProbe, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbe(original["health_probe"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedHealthProbe); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["healthProbe"] = transformedHealthProbe
	}

	transformedImageUri, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecImageUri(original["image_uri"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedImageUri); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["imageUri"] = transformedImageUri
	}

	transformedCommand, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecCommand(original["command"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedCommand); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["command"] = transformedCommand
	}

	transformedArgs, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecArgs(original["args"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedArgs); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["args"] = transformedArgs
	}

	transformedGrpcPorts, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecGrpcPorts(original["grpc_ports"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedGrpcPorts); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["grpcPorts"] = transformedGrpcPorts
	}

	transformedSharedMemorySizeMb, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecSharedMemorySizeMb(original["shared_memory_size_mb"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedSharedMemorySizeMb); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["sharedMemorySizeMb"] = transformedSharedMemorySizeMb
	}

	transformedLivenessProbe, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbe(original["liveness_probe"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedLivenessProbe); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["livenessProbe"] = transformedLivenessProbe
	}

	transformedEnv, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecEnv(original["env"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedEnv); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["env"] = transformedEnv
	}

	return transformed, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecPorts(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	req := make([]interface{}, 0, len(l))
	for _, raw := range l {
		if raw == nil {
			continue
		}
		original := raw.(map[string]interface{})
		transformed := make(map[string]interface{})

		transformedContainerPort, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecPortsContainerPort(original["container_port"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedContainerPort); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["containerPort"] = transformedContainerPort
		}

		req = append(req, transformed)
	}
	return req, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecPortsContainerPort(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecPredictRoute(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthRoute(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecDeploymentTimeout(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbe(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedExec, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeExec(original["exec"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedExec); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["exec"] = transformedExec
	}

	transformedHttpGet, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeHttpGet(original["http_get"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedHttpGet); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["httpGet"] = transformedHttpGet
	}

	transformedGrpc, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeGrpc(original["grpc"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedGrpc); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["grpc"] = transformedGrpc
	}

	transformedTcpSocket, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeTcpSocket(original["tcp_socket"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedTcpSocket); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["tcpSocket"] = transformedTcpSocket
	}

	transformedTimeoutSeconds, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeTimeoutSeconds(original["timeout_seconds"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedTimeoutSeconds); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["timeoutSeconds"] = transformedTimeoutSeconds
	}

	transformedSuccessThreshold, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeSuccessThreshold(original["success_threshold"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedSuccessThreshold); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["successThreshold"] = transformedSuccessThreshold
	}

	transformedInitialDelaySeconds, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeInitialDelaySeconds(original["initial_delay_seconds"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedInitialDelaySeconds); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["initialDelaySeconds"] = transformedInitialDelaySeconds
	}

	transformedPeriodSeconds, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbePeriodSeconds(original["period_seconds"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedPeriodSeconds); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["periodSeconds"] = transformedPeriodSeconds
	}

	transformedFailureThreshold, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeFailureThreshold(original["failure_threshold"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedFailureThreshold); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["failureThreshold"] = transformedFailureThreshold
	}

	return transformed, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeExec(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedCommand, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeExecCommand(original["command"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedCommand); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["command"] = transformedCommand
	}

	return transformed, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeExecCommand(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeHttpGet(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedPath, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeHttpGetPath(original["path"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedPath); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["path"] = transformedPath
	}

	transformedPort, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeHttpGetPort(original["port"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedPort); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["port"] = transformedPort
	}

	transformedHost, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeHttpGetHost(original["host"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedHost); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["host"] = transformedHost
	}

	transformedScheme, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeHttpGetScheme(original["scheme"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedScheme); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["scheme"] = transformedScheme
	}

	transformedHttpHeaders, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeHttpGetHttpHeaders(original["http_headers"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedHttpHeaders); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["httpHeaders"] = transformedHttpHeaders
	}

	return transformed, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeHttpGetPath(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeHttpGetPort(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeHttpGetHost(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeHttpGetScheme(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeHttpGetHttpHeaders(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	req := make([]interface{}, 0, len(l))
	for _, raw := range l {
		if raw == nil {
			continue
		}
		original := raw.(map[string]interface{})
		transformed := make(map[string]interface{})

		transformedValue, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeHttpGetHttpHeadersValue(original["value"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedValue); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["value"] = transformedValue
		}

		transformedName, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeHttpGetHttpHeadersName(original["name"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedName); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["name"] = transformedName
		}

		req = append(req, transformed)
	}
	return req, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeHttpGetHttpHeadersValue(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeHttpGetHttpHeadersName(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeGrpc(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedPort, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeGrpcPort(original["port"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedPort); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["port"] = transformedPort
	}

	transformedService, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeGrpcService(original["service"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedService); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["service"] = transformedService
	}

	return transformed, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeGrpcPort(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeGrpcService(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeTcpSocket(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedPort, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeTcpSocketPort(original["port"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedPort); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["port"] = transformedPort
	}

	transformedHost, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeTcpSocketHost(original["host"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedHost); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["host"] = transformedHost
	}

	return transformed, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeTcpSocketPort(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeTcpSocketHost(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeTimeoutSeconds(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeSuccessThreshold(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeInitialDelaySeconds(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbePeriodSeconds(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeFailureThreshold(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbe(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedExec, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeExec(original["exec"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedExec); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["exec"] = transformedExec
	}

	transformedHttpGet, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeHttpGet(original["http_get"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedHttpGet); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["httpGet"] = transformedHttpGet
	}

	transformedGrpc, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeGrpc(original["grpc"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedGrpc); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["grpc"] = transformedGrpc
	}

	transformedTcpSocket, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeTcpSocket(original["tcp_socket"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedTcpSocket); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["tcpSocket"] = transformedTcpSocket
	}

	transformedTimeoutSeconds, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeTimeoutSeconds(original["timeout_seconds"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedTimeoutSeconds); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["timeoutSeconds"] = transformedTimeoutSeconds
	}

	transformedSuccessThreshold, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeSuccessThreshold(original["success_threshold"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedSuccessThreshold); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["successThreshold"] = transformedSuccessThreshold
	}

	transformedInitialDelaySeconds, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeInitialDelaySeconds(original["initial_delay_seconds"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedInitialDelaySeconds); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["initialDelaySeconds"] = transformedInitialDelaySeconds
	}

	transformedPeriodSeconds, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbePeriodSeconds(original["period_seconds"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedPeriodSeconds); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["periodSeconds"] = transformedPeriodSeconds
	}

	transformedFailureThreshold, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeFailureThreshold(original["failure_threshold"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedFailureThreshold); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["failureThreshold"] = transformedFailureThreshold
	}

	return transformed, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeExec(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedCommand, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeExecCommand(original["command"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedCommand); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["command"] = transformedCommand
	}

	return transformed, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeExecCommand(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeHttpGet(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedPath, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeHttpGetPath(original["path"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedPath); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["path"] = transformedPath
	}

	transformedPort, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeHttpGetPort(original["port"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedPort); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["port"] = transformedPort
	}

	transformedHost, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeHttpGetHost(original["host"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedHost); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["host"] = transformedHost
	}

	transformedScheme, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeHttpGetScheme(original["scheme"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedScheme); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["scheme"] = transformedScheme
	}

	transformedHttpHeaders, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeHttpGetHttpHeaders(original["http_headers"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedHttpHeaders); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["httpHeaders"] = transformedHttpHeaders
	}

	return transformed, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeHttpGetPath(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeHttpGetPort(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeHttpGetHost(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeHttpGetScheme(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeHttpGetHttpHeaders(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	req := make([]interface{}, 0, len(l))
	for _, raw := range l {
		if raw == nil {
			continue
		}
		original := raw.(map[string]interface{})
		transformed := make(map[string]interface{})

		transformedName, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeHttpGetHttpHeadersName(original["name"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedName); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["name"] = transformedName
		}

		transformedValue, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeHttpGetHttpHeadersValue(original["value"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedValue); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["value"] = transformedValue
		}

		req = append(req, transformed)
	}
	return req, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeHttpGetHttpHeadersName(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeHttpGetHttpHeadersValue(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeGrpc(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedPort, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeGrpcPort(original["port"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedPort); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["port"] = transformedPort
	}

	transformedService, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeGrpcService(original["service"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedService); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["service"] = transformedService
	}

	return transformed, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeGrpcPort(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeGrpcService(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeTcpSocket(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedPort, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeTcpSocketPort(original["port"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedPort); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["port"] = transformedPort
	}

	transformedHost, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeTcpSocketHost(original["host"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedHost); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["host"] = transformedHost
	}

	return transformed, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeTcpSocketPort(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeTcpSocketHost(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeTimeoutSeconds(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeSuccessThreshold(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeInitialDelaySeconds(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbePeriodSeconds(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeFailureThreshold(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecImageUri(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecCommand(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecArgs(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecGrpcPorts(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	req := make([]interface{}, 0, len(l))
	for _, raw := range l {
		if raw == nil {
			continue
		}
		original := raw.(map[string]interface{})
		transformed := make(map[string]interface{})

		transformedContainerPort, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecGrpcPortsContainerPort(original["container_port"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedContainerPort); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["containerPort"] = transformedContainerPort
		}

		req = append(req, transformed)
	}
	return req, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecGrpcPortsContainerPort(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecSharedMemorySizeMb(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbe(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedExec, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeExec(original["exec"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedExec); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["exec"] = transformedExec
	}

	transformedHttpGet, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeHttpGet(original["http_get"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedHttpGet); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["httpGet"] = transformedHttpGet
	}

	transformedGrpc, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeGrpc(original["grpc"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedGrpc); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["grpc"] = transformedGrpc
	}

	transformedTcpSocket, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeTcpSocket(original["tcp_socket"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedTcpSocket); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["tcpSocket"] = transformedTcpSocket
	}

	transformedTimeoutSeconds, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeTimeoutSeconds(original["timeout_seconds"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedTimeoutSeconds); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["timeoutSeconds"] = transformedTimeoutSeconds
	}

	transformedSuccessThreshold, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeSuccessThreshold(original["success_threshold"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedSuccessThreshold); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["successThreshold"] = transformedSuccessThreshold
	}

	transformedInitialDelaySeconds, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeInitialDelaySeconds(original["initial_delay_seconds"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedInitialDelaySeconds); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["initialDelaySeconds"] = transformedInitialDelaySeconds
	}

	transformedPeriodSeconds, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbePeriodSeconds(original["period_seconds"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedPeriodSeconds); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["periodSeconds"] = transformedPeriodSeconds
	}

	transformedFailureThreshold, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeFailureThreshold(original["failure_threshold"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedFailureThreshold); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["failureThreshold"] = transformedFailureThreshold
	}

	return transformed, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeExec(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedCommand, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeExecCommand(original["command"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedCommand); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["command"] = transformedCommand
	}

	return transformed, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeExecCommand(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeHttpGet(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedPath, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeHttpGetPath(original["path"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedPath); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["path"] = transformedPath
	}

	transformedPort, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeHttpGetPort(original["port"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedPort); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["port"] = transformedPort
	}

	transformedHost, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeHttpGetHost(original["host"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedHost); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["host"] = transformedHost
	}

	transformedScheme, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeHttpGetScheme(original["scheme"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedScheme); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["scheme"] = transformedScheme
	}

	transformedHttpHeaders, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeHttpGetHttpHeaders(original["http_headers"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedHttpHeaders); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["httpHeaders"] = transformedHttpHeaders
	}

	return transformed, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeHttpGetPath(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeHttpGetPort(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeHttpGetHost(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeHttpGetScheme(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeHttpGetHttpHeaders(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	req := make([]interface{}, 0, len(l))
	for _, raw := range l {
		if raw == nil {
			continue
		}
		original := raw.(map[string]interface{})
		transformed := make(map[string]interface{})

		transformedName, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeHttpGetHttpHeadersName(original["name"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedName); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["name"] = transformedName
		}

		transformedValue, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeHttpGetHttpHeadersValue(original["value"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedValue); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["value"] = transformedValue
		}

		req = append(req, transformed)
	}
	return req, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeHttpGetHttpHeadersName(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeHttpGetHttpHeadersValue(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeGrpc(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedService, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeGrpcService(original["service"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedService); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["service"] = transformedService
	}

	transformedPort, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeGrpcPort(original["port"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedPort); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["port"] = transformedPort
	}

	return transformed, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeGrpcService(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeGrpcPort(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeTcpSocket(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedPort, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeTcpSocketPort(original["port"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedPort); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["port"] = transformedPort
	}

	transformedHost, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeTcpSocketHost(original["host"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedHost); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["host"] = transformedHost
	}

	return transformed, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeTcpSocketPort(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeTcpSocketHost(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeTimeoutSeconds(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeSuccessThreshold(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeInitialDelaySeconds(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbePeriodSeconds(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeFailureThreshold(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecEnv(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	req := make([]interface{}, 0, len(l))
	for _, raw := range l {
		if raw == nil {
			continue
		}
		original := raw.(map[string]interface{})
		transformed := make(map[string]interface{})

		transformedName, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecEnvName(original["name"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedName); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["name"] = transformedName
		}

		transformedValue, err := expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecEnvValue(original["value"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedValue); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["value"] = transformedValue
		}

		req = append(req, transformed)
	}
	return req, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecEnvName(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigContainerSpecEnvValue(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigAcceptEula(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentModelConfigHuggingFaceAccessToken(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentEndpointConfig(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedEndpointDisplayName, err := expandVertexAIEndpointWithModelGardenDeploymentEndpointConfigEndpointDisplayName(original["endpoint_display_name"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedEndpointDisplayName); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["endpointDisplayName"] = transformedEndpointDisplayName
	}

	transformedDedicatedEndpointEnabled, err := expandVertexAIEndpointWithModelGardenDeploymentEndpointConfigDedicatedEndpointEnabled(original["dedicated_endpoint_enabled"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedDedicatedEndpointEnabled); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["dedicatedEndpointEnabled"] = transformedDedicatedEndpointEnabled
	}

	return transformed, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentEndpointConfigEndpointDisplayName(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentEndpointConfigDedicatedEndpointEnabled(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentDeployConfig(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedSystemLabels, err := expandVertexAIEndpointWithModelGardenDeploymentDeployConfigSystemLabels(original["system_labels"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedSystemLabels); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["systemLabels"] = transformedSystemLabels
	}

	transformedDedicatedResources, err := expandVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResources(original["dedicated_resources"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedDedicatedResources); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["dedicatedResources"] = transformedDedicatedResources
	}

	transformedFastTryoutEnabled, err := expandVertexAIEndpointWithModelGardenDeploymentDeployConfigFastTryoutEnabled(original["fast_tryout_enabled"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedFastTryoutEnabled); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["fastTryoutEnabled"] = transformedFastTryoutEnabled
	}

	return transformed, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentDeployConfigSystemLabels(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (map[string]string, error) {
	if v == nil {
		return map[string]string{}, nil
	}
	m := make(map[string]string)
	for k, val := range v.(map[string]interface{}) {
		m[k] = val.(string)
	}
	return m, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResources(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedMachineSpec, err := expandVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpec(original["machine_spec"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedMachineSpec); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["machineSpec"] = transformedMachineSpec
	}

	transformedMinReplicaCount, err := expandVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMinReplicaCount(original["min_replica_count"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedMinReplicaCount); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["minReplicaCount"] = transformedMinReplicaCount
	}

	transformedMaxReplicaCount, err := expandVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMaxReplicaCount(original["max_replica_count"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedMaxReplicaCount); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["maxReplicaCount"] = transformedMaxReplicaCount
	}

	transformedRequiredReplicaCount, err := expandVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesRequiredReplicaCount(original["required_replica_count"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedRequiredReplicaCount); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["requiredReplicaCount"] = transformedRequiredReplicaCount
	}

	transformedAutoscalingMetricSpecs, err := expandVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesAutoscalingMetricSpecs(original["autoscaling_metric_specs"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedAutoscalingMetricSpecs); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["autoscalingMetricSpecs"] = transformedAutoscalingMetricSpecs
	}

	transformedSpot, err := expandVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesSpot(original["spot"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedSpot); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["spot"] = transformedSpot
	}

	return transformed, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpec(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedReservationAffinity, err := expandVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecReservationAffinity(original["reservation_affinity"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedReservationAffinity); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["reservationAffinity"] = transformedReservationAffinity
	}

	transformedMachineType, err := expandVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecMachineType(original["machine_type"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedMachineType); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["machineType"] = transformedMachineType
	}

	transformedAcceleratorType, err := expandVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecAcceleratorType(original["accelerator_type"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedAcceleratorType); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["acceleratorType"] = transformedAcceleratorType
	}

	transformedAcceleratorCount, err := expandVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecAcceleratorCount(original["accelerator_count"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedAcceleratorCount); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["acceleratorCount"] = transformedAcceleratorCount
	}

	transformedTpuTopology, err := expandVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecTpuTopology(original["tpu_topology"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedTpuTopology); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["tpuTopology"] = transformedTpuTopology
	}

	transformedMultihostGpuNodeCount, err := expandVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecMultihostGpuNodeCount(original["multihost_gpu_node_count"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedMultihostGpuNodeCount); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["multihostGpuNodeCount"] = transformedMultihostGpuNodeCount
	}

	return transformed, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecReservationAffinity(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedReservationAffinityType, err := expandVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecReservationAffinityReservationAffinityType(original["reservation_affinity_type"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedReservationAffinityType); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["reservationAffinityType"] = transformedReservationAffinityType
	}

	transformedKey, err := expandVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecReservationAffinityKey(original["key"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedKey); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["key"] = transformedKey
	}

	transformedValues, err := expandVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecReservationAffinityValues(original["values"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedValues); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["values"] = transformedValues
	}

	return transformed, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecReservationAffinityReservationAffinityType(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecReservationAffinityKey(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecReservationAffinityValues(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecMachineType(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecAcceleratorType(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecAcceleratorCount(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecTpuTopology(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecMultihostGpuNodeCount(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMinReplicaCount(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMaxReplicaCount(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesRequiredReplicaCount(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesAutoscalingMetricSpecs(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	req := make([]interface{}, 0, len(l))
	for _, raw := range l {
		if raw == nil {
			continue
		}
		original := raw.(map[string]interface{})
		transformed := make(map[string]interface{})

		transformedMetricName, err := expandVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesAutoscalingMetricSpecsMetricName(original["metric_name"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedMetricName); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["metricName"] = transformedMetricName
		}

		transformedTarget, err := expandVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesAutoscalingMetricSpecsTarget(original["target"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedTarget); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["target"] = transformedTarget
		}

		req = append(req, transformed)
	}
	return req, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesAutoscalingMetricSpecsMetricName(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesAutoscalingMetricSpecsTarget(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesSpot(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointWithModelGardenDeploymentDeployConfigFastTryoutEnabled(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}
