// Copyright (c) HashiCorp, Inc.
// SPDX-License-Identifier: MPL-2.0

// ----------------------------------------------------------------------------
//
//     ***     AUTO GENERATED CODE    ***    Type: MMv1     ***
//
// ----------------------------------------------------------------------------
//
//     This code is generated by Magic Modules using the following:
//
//     Configuration: https://github.com/GoogleCloudPlatform/magic-modules/tree/main/mmv1/products/managedkafka/Connector.yaml
//     Template:      https://github.com/GoogleCloudPlatform/magic-modules/tree/main/mmv1/templates/terraform/resource.go.tmpl
//
//     DO NOT EDIT this file directly. Any changes made to this file will be
//     overwritten during the next generation cycle.
//
// ----------------------------------------------------------------------------

package managedkafka

import (
	"bytes"
	"context"
	"encoding/base64"
	"encoding/json"
	"fmt"
	"log"
	"net/http"
	"reflect"
	"regexp"
	"slices"
	"sort"
	"strconv"
	"strings"
	"time"

	"github.com/hashicorp/errwrap"
	"github.com/hashicorp/go-cty/cty"
	"github.com/hashicorp/terraform-plugin-sdk/v2/diag"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/customdiff"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/id"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/logging"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/retry"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/schema"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/structure"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/validation"
	"github.com/hashicorp/terraform-plugin-sdk/v2/terraform"

	"github.com/hashicorp/terraform-provider-google/google/tpgresource"
	transport_tpg "github.com/hashicorp/terraform-provider-google/google/transport"
	"github.com/hashicorp/terraform-provider-google/google/verify"

	"google.golang.org/api/googleapi"
)

var (
	_ = bytes.Clone
	_ = context.WithCancel
	_ = base64.NewDecoder
	_ = json.Marshal
	_ = fmt.Sprintf
	_ = log.Print
	_ = http.Get
	_ = reflect.ValueOf
	_ = regexp.Match
	_ = slices.Min([]int{1})
	_ = sort.IntSlice{}
	_ = strconv.Atoi
	_ = strings.Trim
	_ = time.Now
	_ = errwrap.Wrap
	_ = cty.BoolVal
	_ = diag.Diagnostic{}
	_ = customdiff.All
	_ = id.UniqueId
	_ = logging.LogLevel
	_ = retry.Retry
	_ = schema.Noop
	_ = validation.All
	_ = structure.ExpandJsonFromString
	_ = terraform.State{}
	_ = tpgresource.SetLabels
	_ = transport_tpg.Config{}
	_ = verify.ValidateEnum
	_ = googleapi.Error{}
)

func ResourceManagedKafkaConnector() *schema.Resource {
	return &schema.Resource{
		Create: resourceManagedKafkaConnectorCreate,
		Read:   resourceManagedKafkaConnectorRead,
		Update: resourceManagedKafkaConnectorUpdate,
		Delete: resourceManagedKafkaConnectorDelete,

		Importer: &schema.ResourceImporter{
			State: resourceManagedKafkaConnectorImport,
		},

		Timeouts: &schema.ResourceTimeout{
			Create: schema.DefaultTimeout(60 * time.Minute),
			Update: schema.DefaultTimeout(30 * time.Minute),
			Delete: schema.DefaultTimeout(30 * time.Minute),
		},

		CustomizeDiff: customdiff.All(
			tpgresource.DefaultProviderProject,
		),

		Identity: &schema.ResourceIdentity{
			Version: 1,
			SchemaFunc: func() map[string]*schema.Schema {
				return map[string]*schema.Schema{
					"location": {
						Type:              schema.TypeString,
						RequiredForImport: true,
					},
					"connect_cluster": {
						Type:              schema.TypeString,
						RequiredForImport: true,
					},
					"connector_id": {
						Type:              schema.TypeString,
						RequiredForImport: true,
					},
					"project": {
						Type:              schema.TypeString,
						OptionalForImport: true,
					},
				}
			},
		},
		Schema: map[string]*schema.Schema{
			"connect_cluster": {
				Type:        schema.TypeString,
				Required:    true,
				ForceNew:    true,
				Description: `The connect cluster name.`,
			},
			"connector_id": {
				Type:        schema.TypeString,
				Required:    true,
				ForceNew:    true,
				Description: `The ID to use for the connector, which will become the final component of the connector's name. This value is structured like: 'my-connector-id'.`,
			},
			"location": {
				Type:        schema.TypeString,
				Required:    true,
				ForceNew:    true,
				Description: `ID of the location of the Kafka Connect resource. See https://cloud.google.com/managed-kafka/docs/locations for a list of supported locations.`,
			},
			"configs": {
				Type:        schema.TypeMap,
				Optional:    true,
				Description: `Connector config as keys/values. The keys of the map are connector property names, for example: 'connector.class', 'tasks.max', 'key.converter'.`,
				Elem:        &schema.Schema{Type: schema.TypeString},
			},
			"task_restart_policy": {
				Type:        schema.TypeList,
				Optional:    true,
				Description: `A policy that specifies how to restart the failed connectors/tasks in a Cluster resource. If not set, the failed connectors/tasks won't be restarted.`,
				MaxItems:    1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"maximum_backoff": {
							Type:     schema.TypeString,
							Optional: true,
							Description: `The maximum amount of time to wait before retrying a failed task. This sets an upper bound for the backoff delay.
A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s".`,
						},
						"minimum_backoff": {
							Type:     schema.TypeString,
							Optional: true,
							Description: `The minimum amount of time to wait before retrying a failed task. This sets a lower bound for the backoff delay.
A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s".`,
						},
					},
				},
			},
			"name": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `The name of the connector. The 'connector' segment is used when connecting directly to the connect cluster. Structured like: 'projects/PROJECT_ID/locations/LOCATION/connectClusters/CONNECT_CLUSTER/connectors/CONNECTOR_ID'.`,
			},
			"state": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `The current state of the connect. Possible values: 'STATE_UNSPECIFIED', 'UNASSIGNED', 'RUNNING', 'PAUSED', 'FAILED', 'RESTARTING', and 'STOPPED'.`,
			},
			"project": {
				Type:     schema.TypeString,
				Optional: true,
				Computed: true,
				ForceNew: true,
			},
		},
		UseJSONNumber: true,
	}
}

func resourceManagedKafkaConnectorCreate(d *schema.ResourceData, meta interface{}) error {
	config := meta.(*transport_tpg.Config)
	userAgent, err := tpgresource.GenerateUserAgentString(d, config.UserAgent)
	if err != nil {
		return err
	}

	obj := make(map[string]interface{})
	configsProp, err := expandManagedKafkaConnectorConfigs(d.Get("configs"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("configs"); !tpgresource.IsEmptyValue(reflect.ValueOf(configsProp)) && (ok || !reflect.DeepEqual(v, configsProp)) {
		obj["configs"] = configsProp
	}
	taskRestartPolicyProp, err := expandManagedKafkaConnectorTaskRestartPolicy(d.Get("task_restart_policy"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("task_restart_policy"); !tpgresource.IsEmptyValue(reflect.ValueOf(taskRestartPolicyProp)) && (ok || !reflect.DeepEqual(v, taskRestartPolicyProp)) {
		obj["taskRestartPolicy"] = taskRestartPolicyProp
	}

	url, err := tpgresource.ReplaceVars(d, config, "{{ManagedKafkaBasePath}}projects/{{project}}/locations/{{location}}/connectClusters/{{connect_cluster}}/connectors?connectorId={{connector_id}}")
	if err != nil {
		return err
	}

	log.Printf("[DEBUG] Creating new Connector: %#v", obj)
	billingProject := ""

	project, err := tpgresource.GetProject(d, config)
	if err != nil {
		return fmt.Errorf("Error fetching project for Connector: %s", err)
	}
	billingProject = project

	// err == nil indicates that the billing_project value was found
	if bp, err := tpgresource.GetBillingProject(d, config); err == nil {
		billingProject = bp
	}

	headers := make(http.Header)
	res, err := transport_tpg.SendRequest(transport_tpg.SendRequestOptions{
		Config:    config,
		Method:    "POST",
		Project:   billingProject,
		RawURL:    url,
		UserAgent: userAgent,
		Body:      obj,
		Timeout:   d.Timeout(schema.TimeoutCreate),
		Headers:   headers,
	})
	if err != nil {
		return fmt.Errorf("Error creating Connector: %s", err)
	}

	// Store the ID now
	id, err := tpgresource.ReplaceVars(d, config, "projects/{{project}}/locations/{{location}}/connectClusters/{{connect_cluster}}/connectors/{{connector_id}}")
	if err != nil {
		return fmt.Errorf("Error constructing id: %s", err)
	}
	d.SetId(id)

	log.Printf("[DEBUG] Finished creating Connector %q: %#v", d.Id(), res)

	return resourceManagedKafkaConnectorRead(d, meta)
}

func resourceManagedKafkaConnectorRead(d *schema.ResourceData, meta interface{}) error {
	config := meta.(*transport_tpg.Config)
	userAgent, err := tpgresource.GenerateUserAgentString(d, config.UserAgent)
	if err != nil {
		return err
	}

	url, err := tpgresource.ReplaceVars(d, config, "{{ManagedKafkaBasePath}}projects/{{project}}/locations/{{location}}/connectClusters/{{connect_cluster}}/connectors/{{connector_id}}")
	if err != nil {
		return err
	}

	billingProject := ""

	project, err := tpgresource.GetProject(d, config)
	if err != nil {
		return fmt.Errorf("Error fetching project for Connector: %s", err)
	}
	billingProject = project

	// err == nil indicates that the billing_project value was found
	if bp, err := tpgresource.GetBillingProject(d, config); err == nil {
		billingProject = bp
	}

	headers := make(http.Header)
	res, err := transport_tpg.SendRequest(transport_tpg.SendRequestOptions{
		Config:    config,
		Method:    "GET",
		Project:   billingProject,
		RawURL:    url,
		UserAgent: userAgent,
		Headers:   headers,
	})
	if err != nil {
		return transport_tpg.HandleNotFoundError(err, d, fmt.Sprintf("ManagedKafkaConnector %q", d.Id()))
	}

	if err := d.Set("project", project); err != nil {
		return fmt.Errorf("Error reading Connector: %s", err)
	}

	if err := d.Set("name", flattenManagedKafkaConnectorName(res["name"], d, config)); err != nil {
		return fmt.Errorf("Error reading Connector: %s", err)
	}
	if err := d.Set("configs", flattenManagedKafkaConnectorConfigs(res["configs"], d, config)); err != nil {
		return fmt.Errorf("Error reading Connector: %s", err)
	}
	if err := d.Set("state", flattenManagedKafkaConnectorState(res["state"], d, config)); err != nil {
		return fmt.Errorf("Error reading Connector: %s", err)
	}
	if err := d.Set("task_restart_policy", flattenManagedKafkaConnectorTaskRestartPolicy(res["taskRestartPolicy"], d, config)); err != nil {
		return fmt.Errorf("Error reading Connector: %s", err)
	}

	identity, err := d.Identity()
	if err != nil && identity != nil {
		if v, ok := identity.GetOk("location"); ok && v != "" {
			err = identity.Set("location", d.Get("location").(string))
			if err != nil {
				return fmt.Errorf("Error setting location: %s", err)
			}
		}
		if v, ok := identity.GetOk("connect_cluster"); ok && v != "" {
			err = identity.Set("connect_cluster", d.Get("connect_cluster").(string))
			if err != nil {
				return fmt.Errorf("Error setting connect_cluster: %s", err)
			}
		}
		if v, ok := identity.GetOk("connector_id"); ok && v != "" {
			err = identity.Set("connector_id", d.Get("connector_id").(string))
			if err != nil {
				return fmt.Errorf("Error setting connector_id: %s", err)
			}
		}
		if v, ok := identity.GetOk("project"); ok && v != "" {
			err = identity.Set("project", d.Get("project").(string))
			if err != nil {
				return fmt.Errorf("Error setting project: %s", err)
			}
		}
	} else {
		log.Printf("[DEBUG] identity not set: %s", err)
	}
	return nil
}

func resourceManagedKafkaConnectorUpdate(d *schema.ResourceData, meta interface{}) error {
	config := meta.(*transport_tpg.Config)
	userAgent, err := tpgresource.GenerateUserAgentString(d, config.UserAgent)
	if err != nil {
		return err
	}

	billingProject := ""

	project, err := tpgresource.GetProject(d, config)
	if err != nil {
		return fmt.Errorf("Error fetching project for Connector: %s", err)
	}
	billingProject = project

	obj := make(map[string]interface{})
	configsProp, err := expandManagedKafkaConnectorConfigs(d.Get("configs"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("configs"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, configsProp)) {
		obj["configs"] = configsProp
	}
	taskRestartPolicyProp, err := expandManagedKafkaConnectorTaskRestartPolicy(d.Get("task_restart_policy"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("task_restart_policy"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, taskRestartPolicyProp)) {
		obj["taskRestartPolicy"] = taskRestartPolicyProp
	}

	url, err := tpgresource.ReplaceVars(d, config, "{{ManagedKafkaBasePath}}projects/{{project}}/locations/{{location}}/connectClusters/{{connect_cluster}}/connectors/{{connector_id}}")
	if err != nil {
		return err
	}

	log.Printf("[DEBUG] Updating Connector %q: %#v", d.Id(), obj)
	headers := make(http.Header)
	updateMask := []string{}

	if d.HasChange("configs") {
		updateMask = append(updateMask, "configs")
	}

	if d.HasChange("task_restart_policy") {
		updateMask = append(updateMask, "taskRestartPolicy")
	}
	// updateMask is a URL parameter but not present in the schema, so ReplaceVars
	// won't set it
	url, err = transport_tpg.AddQueryParams(url, map[string]string{"updateMask": strings.Join(updateMask, ",")})
	if err != nil {
		return err
	}

	// err == nil indicates that the billing_project value was found
	if bp, err := tpgresource.GetBillingProject(d, config); err == nil {
		billingProject = bp
	}

	// if updateMask is empty we are not updating anything so skip the post
	if len(updateMask) > 0 {
		res, err := transport_tpg.SendRequest(transport_tpg.SendRequestOptions{
			Config:    config,
			Method:    "PATCH",
			Project:   billingProject,
			RawURL:    url,
			UserAgent: userAgent,
			Body:      obj,
			Timeout:   d.Timeout(schema.TimeoutUpdate),
			Headers:   headers,
		})

		if err != nil {
			return fmt.Errorf("Error updating Connector %q: %s", d.Id(), err)
		} else {
			log.Printf("[DEBUG] Finished updating Connector %q: %#v", d.Id(), res)
		}

	}

	return resourceManagedKafkaConnectorRead(d, meta)
}

func resourceManagedKafkaConnectorDelete(d *schema.ResourceData, meta interface{}) error {
	config := meta.(*transport_tpg.Config)
	userAgent, err := tpgresource.GenerateUserAgentString(d, config.UserAgent)
	if err != nil {
		return err
	}

	billingProject := ""

	project, err := tpgresource.GetProject(d, config)
	if err != nil {
		return fmt.Errorf("Error fetching project for Connector: %s", err)
	}
	billingProject = project

	url, err := tpgresource.ReplaceVars(d, config, "{{ManagedKafkaBasePath}}projects/{{project}}/locations/{{location}}/connectClusters/{{connect_cluster}}/connectors/{{connector_id}}")
	if err != nil {
		return err
	}

	var obj map[string]interface{}

	// err == nil indicates that the billing_project value was found
	if bp, err := tpgresource.GetBillingProject(d, config); err == nil {
		billingProject = bp
	}

	headers := make(http.Header)

	log.Printf("[DEBUG] Deleting Connector %q", d.Id())
	res, err := transport_tpg.SendRequest(transport_tpg.SendRequestOptions{
		Config:    config,
		Method:    "DELETE",
		Project:   billingProject,
		RawURL:    url,
		UserAgent: userAgent,
		Body:      obj,
		Timeout:   d.Timeout(schema.TimeoutDelete),
		Headers:   headers,
	})
	if err != nil {
		return transport_tpg.HandleNotFoundError(err, d, "Connector")
	}

	log.Printf("[DEBUG] Finished deleting Connector %q: %#v", d.Id(), res)
	return nil
}

func resourceManagedKafkaConnectorImport(d *schema.ResourceData, meta interface{}) ([]*schema.ResourceData, error) {
	config := meta.(*transport_tpg.Config)
	if err := tpgresource.ParseImportId([]string{
		"^projects/(?P<project>[^/]+)/locations/(?P<location>[^/]+)/connectClusters/(?P<connect_cluster>[^/]+)/connectors/(?P<connector_id>[^/]+)$",
		"^(?P<project>[^/]+)/(?P<location>[^/]+)/(?P<connect_cluster>[^/]+)/(?P<connector_id>[^/]+)$",
		"^(?P<location>[^/]+)/(?P<connect_cluster>[^/]+)/(?P<connector_id>[^/]+)$",
	}, d, config); err != nil {
		return nil, err
	}

	// Replace import id for the resource id
	id, err := tpgresource.ReplaceVars(d, config, "projects/{{project}}/locations/{{location}}/connectClusters/{{connect_cluster}}/connectors/{{connector_id}}")
	if err != nil {
		return nil, fmt.Errorf("Error constructing id: %s", err)
	}
	d.SetId(id)

	return []*schema.ResourceData{d}, nil
}

func flattenManagedKafkaConnectorName(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenManagedKafkaConnectorConfigs(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenManagedKafkaConnectorState(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenManagedKafkaConnectorTaskRestartPolicy(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["minimum_backoff"] =
		flattenManagedKafkaConnectorTaskRestartPolicyMinimumBackoff(original["minimumBackoff"], d, config)
	transformed["maximum_backoff"] =
		flattenManagedKafkaConnectorTaskRestartPolicyMaximumBackoff(original["maximumBackoff"], d, config)
	return []interface{}{transformed}
}
func flattenManagedKafkaConnectorTaskRestartPolicyMinimumBackoff(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenManagedKafkaConnectorTaskRestartPolicyMaximumBackoff(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func expandManagedKafkaConnectorConfigs(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (map[string]string, error) {
	if v == nil {
		return map[string]string{}, nil
	}
	m := make(map[string]string)
	for k, val := range v.(map[string]interface{}) {
		m[k] = val.(string)
	}
	return m, nil
}

func expandManagedKafkaConnectorTaskRestartPolicy(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	if v == nil {
		return nil, nil
	}
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedMinimumBackoff, err := expandManagedKafkaConnectorTaskRestartPolicyMinimumBackoff(original["minimum_backoff"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedMinimumBackoff); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["minimumBackoff"] = transformedMinimumBackoff
	}

	transformedMaximumBackoff, err := expandManagedKafkaConnectorTaskRestartPolicyMaximumBackoff(original["maximum_backoff"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedMaximumBackoff); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["maximumBackoff"] = transformedMaximumBackoff
	}

	return transformed, nil
}

func expandManagedKafkaConnectorTaskRestartPolicyMinimumBackoff(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandManagedKafkaConnectorTaskRestartPolicyMaximumBackoff(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}
