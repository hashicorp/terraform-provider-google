// Copyright (c) HashiCorp, Inc.
// SPDX-License-Identifier: MPL-2.0

// ----------------------------------------------------------------------------
//
//     ***     AUTO GENERATED CODE    ***    Type: MMv1     ***
//
// ----------------------------------------------------------------------------
//
//     This code is generated by Magic Modules using the following:
//
//     Configuration: https://github.com/GoogleCloudPlatform/magic-modules/tree/main/mmv1/products/storageinsights/DatasetConfig.yaml
//     Template:      https://github.com/GoogleCloudPlatform/magic-modules/tree/main/mmv1/templates/terraform/resource.go.tmpl
//
//     DO NOT EDIT this file directly. Any changes made to this file will be
//     overwritten during the next generation cycle.
//
// ----------------------------------------------------------------------------

package storageinsights

import (
	"bytes"
	"context"
	"encoding/base64"
	"encoding/json"
	"fmt"
	"log"
	"net/http"
	"reflect"
	"regexp"
	"slices"
	"sort"
	"strconv"
	"strings"
	"time"

	"github.com/hashicorp/errwrap"
	"github.com/hashicorp/go-cty/cty"
	"github.com/hashicorp/terraform-plugin-sdk/v2/diag"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/customdiff"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/id"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/logging"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/retry"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/schema"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/structure"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/validation"
	"github.com/hashicorp/terraform-plugin-sdk/v2/terraform"

	"github.com/hashicorp/terraform-provider-google/google/tpgresource"
	transport_tpg "github.com/hashicorp/terraform-provider-google/google/transport"
	"github.com/hashicorp/terraform-provider-google/google/verify"

	"google.golang.org/api/googleapi"
)

var (
	_ = bytes.Clone
	_ = context.WithCancel
	_ = base64.NewDecoder
	_ = json.Marshal
	_ = fmt.Sprintf
	_ = log.Print
	_ = http.Get
	_ = reflect.ValueOf
	_ = regexp.Match
	_ = slices.Min([]int{1})
	_ = sort.IntSlice{}
	_ = strconv.Atoi
	_ = strings.Trim
	_ = time.Now
	_ = errwrap.Wrap
	_ = cty.BoolVal
	_ = diag.Diagnostic{}
	_ = customdiff.All
	_ = id.UniqueId
	_ = logging.LogLevel
	_ = retry.Retry
	_ = schema.Noop
	_ = validation.All
	_ = structure.ExpandJsonFromString
	_ = terraform.State{}
	_ = tpgresource.SetLabels
	_ = transport_tpg.Config{}
	_ = verify.ValidateEnum
	_ = googleapi.Error{}
)

func ResourceStorageInsightsDatasetConfig() *schema.Resource {
	return &schema.Resource{
		Create: resourceStorageInsightsDatasetConfigCreate,
		Read:   resourceStorageInsightsDatasetConfigRead,
		Update: resourceStorageInsightsDatasetConfigUpdate,
		Delete: resourceStorageInsightsDatasetConfigDelete,

		Importer: &schema.ResourceImporter{
			State: resourceStorageInsightsDatasetConfigImport,
		},

		Timeouts: &schema.ResourceTimeout{
			Create: schema.DefaultTimeout(20 * time.Minute),
			Update: schema.DefaultTimeout(20 * time.Minute),
			Delete: schema.DefaultTimeout(20 * time.Minute),
		},

		CustomizeDiff: customdiff.All(
			tpgresource.DefaultProviderProject,
		),

		Identity: &schema.ResourceIdentity{
			Version: 1,
			SchemaFunc: func() map[string]*schema.Schema {
				return map[string]*schema.Schema{
					"location": {
						Type:              schema.TypeString,
						RequiredForImport: true,
					},
					"dataset_config_id": {
						Type:              schema.TypeString,
						RequiredForImport: true,
					},
					"project": {
						Type:              schema.TypeString,
						OptionalForImport: true,
					},
				}
			},
		},
		Schema: map[string]*schema.Schema{
			"dataset_config_id": {
				Type:        schema.TypeString,
				Required:    true,
				ForceNew:    true,
				Description: `The user-defined ID of the DatasetConfig`,
			},
			"identity": {
				Type:        schema.TypeList,
				Required:    true,
				ForceNew:    true,
				Description: `Identity used by DatasetConfig.`,
				MaxItems:    1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"type": {
							Type:         schema.TypeString,
							Required:     true,
							ValidateFunc: verify.ValidateEnum([]string{"IDENTITY_TYPE_PER_CONFIG", "IDENTITY_TYPE_PER_PROJECT"}),
							Description:  `Type of identity to use for the DatasetConfig. Possible values: ["IDENTITY_TYPE_PER_CONFIG", "IDENTITY_TYPE_PER_PROJECT"]`,
						},
						"name": {
							Type:        schema.TypeString,
							Computed:    true,
							Description: `Name of the identity.`,
						},
					},
				},
			},
			"location": {
				Type:        schema.TypeString,
				Required:    true,
				ForceNew:    true,
				Description: `The location of the DatasetConfig.`,
			},
			"retention_period_days": {
				Type:        schema.TypeInt,
				Required:    true,
				Description: `Number of days of history that must be retained.`,
			},
			"activity_data_retention_period_days": {
				Type:        schema.TypeInt,
				Computed:    true,
				Optional:    true,
				Description: `Number of days of activity data that must be retained. If not specified, retentionPeriodDays will be used. Set to 0 to turn off the activity data.`,
			},
			"description": {
				Type:        schema.TypeString,
				Optional:    true,
				Description: `An optional user-provided description for the dataset configuration with a maximum length of 256 characters.`,
			},
			"exclude_cloud_storage_buckets": {
				Type:        schema.TypeList,
				Optional:    true,
				Description: `Defined the options for excluding cloud storage buckets for the DatasetConfig.`,
				MaxItems:    1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"cloud_storage_buckets": {
							Type:        schema.TypeList,
							Required:    true,
							Description: `The list of cloud storage buckets/bucket prefix regexes to exclude in the DatasetConfig.`,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"bucket_name": {
										Type:     schema.TypeString,
										Optional: true,
										Description: `The list of cloud storage bucket names to exclude in the DatasetConfig.
Exactly one of the bucket_name and bucket_prefix_regex should be specified.`,
									},
									"bucket_prefix_regex": {
										Type:     schema.TypeString,
										Optional: true,
										Description: `The list of regex patterns for bucket names matching the regex.
Regex should follow the syntax specified in google/re2 on GitHub.
Exactly one of the bucket_name and bucket_prefix_regex should be specified.`,
									},
								},
							},
						},
					},
				},
				ConflictsWith: []string{"include_cloud_storage_buckets"},
			},
			"exclude_cloud_storage_locations": {
				Type:        schema.TypeList,
				Optional:    true,
				Description: `Defines the options for excluding cloud storage locations for the DatasetConfig.`,
				MaxItems:    1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"locations": {
							Type:        schema.TypeList,
							Required:    true,
							Description: `The list of cloud storage locations to exclude in the DatasetConfig.`,
							Elem: &schema.Schema{
								Type: schema.TypeString,
							},
						},
					},
				},
				ConflictsWith: []string{"include_cloud_storage_locations"},
			},
			"include_cloud_storage_buckets": {
				Type:        schema.TypeList,
				Optional:    true,
				Description: `Defines the options for including cloud storage buckets for the DatasetConfig.`,
				MaxItems:    1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"cloud_storage_buckets": {
							Type:        schema.TypeList,
							Required:    true,
							Description: `The list of cloud storage buckets/bucket prefix regexes to include in the DatasetConfig.`,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"bucket_name": {
										Type:     schema.TypeString,
										Optional: true,
										Description: `The list of cloud storage bucket names to include in the DatasetConfig.
Exactly one of the bucket_name and bucket_prefix_regex should be specified.`,
									},
									"bucket_prefix_regex": {
										Type:     schema.TypeString,
										Optional: true,
										Description: `The list of regex patterns for bucket names matching the regex.
Regex should follow the syntax specified in google/re2 on GitHub.
Exactly one of the bucket_name and bucket_prefix_regex should be specified.`,
									},
								},
							},
						},
					},
				},
				ConflictsWith: []string{"exclude_cloud_storage_buckets"},
			},
			"include_cloud_storage_locations": {
				Type:        schema.TypeList,
				Optional:    true,
				Description: `Defines the options for including cloud storage locations for the DatasetConfig.`,
				MaxItems:    1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"locations": {
							Type:        schema.TypeList,
							Required:    true,
							Description: `The list of cloud storage locations to include in the DatasetConfig.`,
							Elem: &schema.Schema{
								Type: schema.TypeString,
							},
						},
					},
				},
				ConflictsWith: []string{"exclude_cloud_storage_locations"},
			},
			"include_newly_created_buckets": {
				Type:        schema.TypeBool,
				Optional:    true,
				Description: `If set to true, the request includes all the newly created buckets in the dataset that meet the inclusion and exclusion rules.`,
			},
			"organization_number": {
				Type:     schema.TypeString,
				Computed: true,
				Optional: true,
				ForceNew: true,
				Description: `Organization resource ID that the source projects should belong to.
Projects that do not belong to the provided organization are not considered when creating the dataset.`,
			},
			"organization_scope": {
				Type:         schema.TypeBool,
				Optional:     true,
				Description:  `Defines the options for providing a source organization for the DatasetConfig.`,
				ExactlyOneOf: []string{"organization_scope", "source_folders", "source_projects"},
			},
			"source_folders": {
				Type:        schema.TypeList,
				Optional:    true,
				Description: `Defines the options for providing source folders for the DatasetConfig.`,
				MaxItems:    1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"folder_numbers": {
							Type:        schema.TypeList,
							Optional:    true,
							Description: `The list of folder numbers to include in the DatasetConfig.`,
							Elem: &schema.Schema{
								Type: schema.TypeString,
							},
						},
					},
				},
				ExactlyOneOf: []string{"organization_scope", "source_folders", "source_projects"},
			},
			"source_projects": {
				Type:        schema.TypeList,
				Optional:    true,
				Description: `Defines the options for providing source projects for the DatasetConfig.`,
				MaxItems:    1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"project_numbers": {
							Type:        schema.TypeList,
							Optional:    true,
							Description: `The list of project numbers to include in the DatasetConfig.`,
							Elem: &schema.Schema{
								Type: schema.TypeString,
							},
						},
					},
				},
				ExactlyOneOf: []string{"organization_scope", "source_folders", "source_projects"},
			},
			"create_time": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `The UTC time at which the DatasetConfig was created. This is auto-populated.`,
			},
			"dataset_config_state": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `State of the DatasetConfig.`,
			},
			"link": {
				Type:        schema.TypeList,
				Computed:    true,
				Description: `Details of the linked DatasetConfig.`,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"dataset": {
							Type:        schema.TypeString,
							Computed:    true,
							Description: `Dataset name for the linked DatasetConfig.`,
						},
						"linked": {
							Type:        schema.TypeBool,
							Computed:    true,
							Description: `State of the linked DatasetConfig.`,
						},
					},
				},
			},
			"name": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `The full canonical resource name of the DatasetConfig (e.g., projects/P/locations/L/datasetConfigs/ID).`,
			},
			"uid": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `System generated unique identifier for the resource.`,
			},
			"update_time": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `The UTC time at which the DatasetConfig was updated. This is auto-populated.`,
			},
			"link_dataset": {
				Type:     schema.TypeBool,
				Optional: true,
				Description: `A boolean terraform only flag to link/unlink dataset.

Setting this field to true while creation will automatically link the created dataset as an additional functionality.
-> **Note** A dataset config resource can only be destroyed once it is unlinked,
so users must set this field to false to unlink the dataset and destroy the dataset config resource.`,
				Default: false,
			},
			"project": {
				Type:     schema.TypeString,
				Optional: true,
				Computed: true,
				ForceNew: true,
			},
		},
		UseJSONNumber: true,
	}
}

func resourceStorageInsightsDatasetConfigCreate(d *schema.ResourceData, meta interface{}) error {
	config := meta.(*transport_tpg.Config)
	userAgent, err := tpgresource.GenerateUserAgentString(d, config.UserAgent)
	if err != nil {
		return err
	}

	obj := make(map[string]interface{})
	organizationNumberProp, err := expandStorageInsightsDatasetConfigOrganizationNumber(d.Get("organization_number"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("organization_number"); !tpgresource.IsEmptyValue(reflect.ValueOf(organizationNumberProp)) && (ok || !reflect.DeepEqual(v, organizationNumberProp)) {
		obj["organizationNumber"] = organizationNumberProp
	}
	includeNewlyCreatedBucketsProp, err := expandStorageInsightsDatasetConfigIncludeNewlyCreatedBuckets(d.Get("include_newly_created_buckets"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("include_newly_created_buckets"); !tpgresource.IsEmptyValue(reflect.ValueOf(includeNewlyCreatedBucketsProp)) && (ok || !reflect.DeepEqual(v, includeNewlyCreatedBucketsProp)) {
		obj["includeNewlyCreatedBuckets"] = includeNewlyCreatedBucketsProp
	}
	retentionPeriodDaysProp, err := expandStorageInsightsDatasetConfigRetentionPeriodDays(d.Get("retention_period_days"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("retention_period_days"); !tpgresource.IsEmptyValue(reflect.ValueOf(retentionPeriodDaysProp)) && (ok || !reflect.DeepEqual(v, retentionPeriodDaysProp)) {
		obj["retentionPeriodDays"] = retentionPeriodDaysProp
	}
	activityDataRetentionPeriodDaysProp, err := expandStorageInsightsDatasetConfigActivityDataRetentionPeriodDays(d.Get("activity_data_retention_period_days"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("activity_data_retention_period_days"); !tpgresource.IsEmptyValue(reflect.ValueOf(activityDataRetentionPeriodDaysProp)) && (ok || !reflect.DeepEqual(v, activityDataRetentionPeriodDaysProp)) {
		obj["activityDataRetentionPeriodDays"] = activityDataRetentionPeriodDaysProp
	}
	identityProp, err := expandStorageInsightsDatasetConfigIdentity(d.Get("identity"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("identity"); !tpgresource.IsEmptyValue(reflect.ValueOf(identityProp)) && (ok || !reflect.DeepEqual(v, identityProp)) {
		obj["identity"] = identityProp
	}
	descriptionProp, err := expandStorageInsightsDatasetConfigDescription(d.Get("description"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("description"); !tpgresource.IsEmptyValue(reflect.ValueOf(descriptionProp)) && (ok || !reflect.DeepEqual(v, descriptionProp)) {
		obj["description"] = descriptionProp
	}
	sourceProjectsProp, err := expandStorageInsightsDatasetConfigSourceProjects(d.Get("source_projects"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("source_projects"); !tpgresource.IsEmptyValue(reflect.ValueOf(sourceProjectsProp)) && (ok || !reflect.DeepEqual(v, sourceProjectsProp)) {
		obj["sourceProjects"] = sourceProjectsProp
	}
	sourceFoldersProp, err := expandStorageInsightsDatasetConfigSourceFolders(d.Get("source_folders"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("source_folders"); !tpgresource.IsEmptyValue(reflect.ValueOf(sourceFoldersProp)) && (ok || !reflect.DeepEqual(v, sourceFoldersProp)) {
		obj["sourceFolders"] = sourceFoldersProp
	}
	organizationScopeProp, err := expandStorageInsightsDatasetConfigOrganizationScope(d.Get("organization_scope"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("organization_scope"); !tpgresource.IsEmptyValue(reflect.ValueOf(organizationScopeProp)) && (ok || !reflect.DeepEqual(v, organizationScopeProp)) {
		obj["organizationScope"] = organizationScopeProp
	}
	includeCloudStorageLocationsProp, err := expandStorageInsightsDatasetConfigIncludeCloudStorageLocations(d.Get("include_cloud_storage_locations"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("include_cloud_storage_locations"); !tpgresource.IsEmptyValue(reflect.ValueOf(includeCloudStorageLocationsProp)) && (ok || !reflect.DeepEqual(v, includeCloudStorageLocationsProp)) {
		obj["includeCloudStorageLocations"] = includeCloudStorageLocationsProp
	}
	excludeCloudStorageLocationsProp, err := expandStorageInsightsDatasetConfigExcludeCloudStorageLocations(d.Get("exclude_cloud_storage_locations"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("exclude_cloud_storage_locations"); !tpgresource.IsEmptyValue(reflect.ValueOf(excludeCloudStorageLocationsProp)) && (ok || !reflect.DeepEqual(v, excludeCloudStorageLocationsProp)) {
		obj["excludeCloudStorageLocations"] = excludeCloudStorageLocationsProp
	}
	includeCloudStorageBucketsProp, err := expandStorageInsightsDatasetConfigIncludeCloudStorageBuckets(d.Get("include_cloud_storage_buckets"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("include_cloud_storage_buckets"); !tpgresource.IsEmptyValue(reflect.ValueOf(includeCloudStorageBucketsProp)) && (ok || !reflect.DeepEqual(v, includeCloudStorageBucketsProp)) {
		obj["includeCloudStorageBuckets"] = includeCloudStorageBucketsProp
	}
	excludeCloudStorageBucketsProp, err := expandStorageInsightsDatasetConfigExcludeCloudStorageBuckets(d.Get("exclude_cloud_storage_buckets"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("exclude_cloud_storage_buckets"); !tpgresource.IsEmptyValue(reflect.ValueOf(excludeCloudStorageBucketsProp)) && (ok || !reflect.DeepEqual(v, excludeCloudStorageBucketsProp)) {
		obj["excludeCloudStorageBuckets"] = excludeCloudStorageBucketsProp
	}

	url, err := tpgresource.ReplaceVars(d, config, "{{StorageInsightsBasePath}}projects/{{project}}/locations/{{location}}/datasetConfigs?datasetConfigId={{dataset_config_id}}")
	if err != nil {
		return err
	}

	log.Printf("[DEBUG] Creating new DatasetConfig: %#v", obj)
	billingProject := ""

	project, err := tpgresource.GetProject(d, config)
	if err != nil {
		return fmt.Errorf("Error fetching project for DatasetConfig: %s", err)
	}
	billingProject = project

	// err == nil indicates that the billing_project value was found
	if bp, err := tpgresource.GetBillingProject(d, config); err == nil {
		billingProject = bp
	}

	headers := make(http.Header)
	res, err := transport_tpg.SendRequest(transport_tpg.SendRequestOptions{
		Config:    config,
		Method:    "POST",
		Project:   billingProject,
		RawURL:    url,
		UserAgent: userAgent,
		Body:      obj,
		Timeout:   d.Timeout(schema.TimeoutCreate),
		Headers:   headers,
	})
	if err != nil {
		return fmt.Errorf("Error creating DatasetConfig: %s", err)
	}

	// Store the ID now
	id, err := tpgresource.ReplaceVars(d, config, "projects/{{project}}/locations/{{location}}/datasetConfigs/{{dataset_config_id}}")
	if err != nil {
		return fmt.Errorf("Error constructing id: %s", err)
	}
	d.SetId(id)

	identity, err := d.Identity()
	if err == nil && identity != nil {
		if locationValue, ok := d.GetOk("location"); ok && locationValue.(string) != "" {
			if err = identity.Set("location", locationValue.(string)); err != nil {
				return fmt.Errorf("Error setting location: %s", err)
			}
		}
		if datasetConfigIdValue, ok := d.GetOk("dataset_config_id"); ok && datasetConfigIdValue.(string) != "" {
			if err = identity.Set("dataset_config_id", datasetConfigIdValue.(string)); err != nil {
				return fmt.Errorf("Error setting dataset_config_id: %s", err)
			}
		}
		if projectValue, ok := d.GetOk("project"); ok && projectValue.(string) != "" {
			if err = identity.Set("project", projectValue.(string)); err != nil {
				return fmt.Errorf("Error setting project: %s", err)
			}
		}
	} else {
		log.Printf("[DEBUG] (Create) identity not set: %s", err)
	}

	err = StorageInsightsOperationWaitTime(
		config, res, project, "Creating DatasetConfig", userAgent,
		d.Timeout(schema.TimeoutCreate))

	if err != nil {
		// The resource didn't actually create
		d.SetId("")
		return fmt.Errorf("Error waiting to create DatasetConfig: %s", err)
	}

	if d.Get("link_dataset") == true {

		linkUrl := strings.Replace(url, "?datasetConfigId=", "/", 1) + ":linkDataset"

		// err == nil indicates that the billing_project value was found
		if bp, err := tpgresource.GetBillingProject(d, config); err == nil {
			billingProject = bp
		}

		res, err := transport_tpg.SendRequest(transport_tpg.SendRequestOptions{
			Config:    config,
			Method:    "POST",
			Project:   billingProject,
			RawURL:    linkUrl,
			UserAgent: userAgent,
			Timeout:   d.Timeout(schema.TimeoutUpdate),
			Headers:   headers,
		})

		if err != nil {
			return fmt.Errorf("Error Linking DatasetConfig %q: %s", d.Id(), err)
		} else {
			log.Printf("[DEBUG] Finished Linking DatasetConfig %q: %#v", d.Id(), res)
		}

		err = StorageInsightsOperationWaitTime(
			config, res, project, "Linking DatasetConfig", userAgent,
			d.Timeout(schema.TimeoutUpdate))

		if err != nil {
			return err
		}
	}

	log.Printf("[DEBUG] Finished creating DatasetConfig %q: %#v", d.Id(), res)

	return resourceStorageInsightsDatasetConfigRead(d, meta)
}

func resourceStorageInsightsDatasetConfigRead(d *schema.ResourceData, meta interface{}) error {
	config := meta.(*transport_tpg.Config)
	userAgent, err := tpgresource.GenerateUserAgentString(d, config.UserAgent)
	if err != nil {
		return err
	}

	url, err := tpgresource.ReplaceVars(d, config, "{{StorageInsightsBasePath}}projects/{{project}}/locations/{{location}}/datasetConfigs/{{dataset_config_id}}")
	if err != nil {
		return err
	}

	billingProject := ""

	project, err := tpgresource.GetProject(d, config)
	if err != nil {
		return fmt.Errorf("Error fetching project for DatasetConfig: %s", err)
	}
	billingProject = project

	// err == nil indicates that the billing_project value was found
	if bp, err := tpgresource.GetBillingProject(d, config); err == nil {
		billingProject = bp
	}

	headers := make(http.Header)
	res, err := transport_tpg.SendRequest(transport_tpg.SendRequestOptions{
		Config:    config,
		Method:    "GET",
		Project:   billingProject,
		RawURL:    url,
		UserAgent: userAgent,
		Headers:   headers,
	})
	if err != nil {
		return transport_tpg.HandleNotFoundError(err, d, fmt.Sprintf("StorageInsightsDatasetConfig %q", d.Id()))
	}

	// Explicitly set virtual fields to default values if unset
	if _, ok := d.GetOkExists("link_dataset"); !ok {
		if err := d.Set("link_dataset", false); err != nil {
			return fmt.Errorf("Error setting link_dataset: %s", err)
		}
	}
	if err := d.Set("project", project); err != nil {
		return fmt.Errorf("Error reading DatasetConfig: %s", err)
	}

	if err := d.Set("name", flattenStorageInsightsDatasetConfigName(res["name"], d, config)); err != nil {
		return fmt.Errorf("Error reading DatasetConfig: %s", err)
	}
	if err := d.Set("create_time", flattenStorageInsightsDatasetConfigCreateTime(res["createTime"], d, config)); err != nil {
		return fmt.Errorf("Error reading DatasetConfig: %s", err)
	}
	if err := d.Set("update_time", flattenStorageInsightsDatasetConfigUpdateTime(res["updateTime"], d, config)); err != nil {
		return fmt.Errorf("Error reading DatasetConfig: %s", err)
	}
	if err := d.Set("uid", flattenStorageInsightsDatasetConfigUid(res["uid"], d, config)); err != nil {
		return fmt.Errorf("Error reading DatasetConfig: %s", err)
	}
	if err := d.Set("organization_number", flattenStorageInsightsDatasetConfigOrganizationNumber(res["organizationNumber"], d, config)); err != nil {
		return fmt.Errorf("Error reading DatasetConfig: %s", err)
	}
	if err := d.Set("include_newly_created_buckets", flattenStorageInsightsDatasetConfigIncludeNewlyCreatedBuckets(res["includeNewlyCreatedBuckets"], d, config)); err != nil {
		return fmt.Errorf("Error reading DatasetConfig: %s", err)
	}
	if err := d.Set("retention_period_days", flattenStorageInsightsDatasetConfigRetentionPeriodDays(res["retentionPeriodDays"], d, config)); err != nil {
		return fmt.Errorf("Error reading DatasetConfig: %s", err)
	}
	if err := d.Set("activity_data_retention_period_days", flattenStorageInsightsDatasetConfigActivityDataRetentionPeriodDays(res["activityDataRetentionPeriodDays"], d, config)); err != nil {
		return fmt.Errorf("Error reading DatasetConfig: %s", err)
	}
	if err := d.Set("link", flattenStorageInsightsDatasetConfigLink(res["link"], d, config)); err != nil {
		return fmt.Errorf("Error reading DatasetConfig: %s", err)
	}
	if err := d.Set("identity", flattenStorageInsightsDatasetConfigIdentity(res["identity"], d, config)); err != nil {
		return fmt.Errorf("Error reading DatasetConfig: %s", err)
	}
	if err := d.Set("dataset_config_state", flattenStorageInsightsDatasetConfigDatasetConfigState(res["datasetConfigState"], d, config)); err != nil {
		return fmt.Errorf("Error reading DatasetConfig: %s", err)
	}
	if err := d.Set("description", flattenStorageInsightsDatasetConfigDescription(res["description"], d, config)); err != nil {
		return fmt.Errorf("Error reading DatasetConfig: %s", err)
	}
	if err := d.Set("source_projects", flattenStorageInsightsDatasetConfigSourceProjects(res["sourceProjects"], d, config)); err != nil {
		return fmt.Errorf("Error reading DatasetConfig: %s", err)
	}
	if err := d.Set("source_folders", flattenStorageInsightsDatasetConfigSourceFolders(res["sourceFolders"], d, config)); err != nil {
		return fmt.Errorf("Error reading DatasetConfig: %s", err)
	}
	if err := d.Set("organization_scope", flattenStorageInsightsDatasetConfigOrganizationScope(res["organizationScope"], d, config)); err != nil {
		return fmt.Errorf("Error reading DatasetConfig: %s", err)
	}
	if err := d.Set("include_cloud_storage_locations", flattenStorageInsightsDatasetConfigIncludeCloudStorageLocations(res["includeCloudStorageLocations"], d, config)); err != nil {
		return fmt.Errorf("Error reading DatasetConfig: %s", err)
	}
	if err := d.Set("exclude_cloud_storage_locations", flattenStorageInsightsDatasetConfigExcludeCloudStorageLocations(res["excludeCloudStorageLocations"], d, config)); err != nil {
		return fmt.Errorf("Error reading DatasetConfig: %s", err)
	}
	if err := d.Set("include_cloud_storage_buckets", flattenStorageInsightsDatasetConfigIncludeCloudStorageBuckets(res["includeCloudStorageBuckets"], d, config)); err != nil {
		return fmt.Errorf("Error reading DatasetConfig: %s", err)
	}
	if err := d.Set("exclude_cloud_storage_buckets", flattenStorageInsightsDatasetConfigExcludeCloudStorageBuckets(res["excludeCloudStorageBuckets"], d, config)); err != nil {
		return fmt.Errorf("Error reading DatasetConfig: %s", err)
	}

	identity, err := d.Identity()
	if err == nil && identity != nil {
		if v, ok := identity.GetOk("location"); !ok && v == "" {
			err = identity.Set("location", d.Get("location").(string))
			if err != nil {
				return fmt.Errorf("Error setting location: %s", err)
			}
		}
		if v, ok := identity.GetOk("dataset_config_id"); !ok && v == "" {
			err = identity.Set("dataset_config_id", d.Get("dataset_config_id").(string))
			if err != nil {
				return fmt.Errorf("Error setting dataset_config_id: %s", err)
			}
		}
		if v, ok := identity.GetOk("project"); !ok && v == "" {
			err = identity.Set("project", d.Get("project").(string))
			if err != nil {
				return fmt.Errorf("Error setting project: %s", err)
			}
		}
	} else {
		log.Printf("[DEBUG] (Read) identity not set: %s", err)
	}

	return nil
}

func resourceStorageInsightsDatasetConfigUpdate(d *schema.ResourceData, meta interface{}) error {
	config := meta.(*transport_tpg.Config)
	userAgent, err := tpgresource.GenerateUserAgentString(d, config.UserAgent)
	if err != nil {
		return err
	}

	identity, err := d.Identity()
	if err == nil && identity != nil {
		if locationValue, ok := d.GetOk("location"); ok && locationValue.(string) != "" {
			if err = identity.Set("location", locationValue.(string)); err != nil {
				return fmt.Errorf("Error setting location: %s", err)
			}
		}
		if datasetConfigIdValue, ok := d.GetOk("dataset_config_id"); ok && datasetConfigIdValue.(string) != "" {
			if err = identity.Set("dataset_config_id", datasetConfigIdValue.(string)); err != nil {
				return fmt.Errorf("Error setting dataset_config_id: %s", err)
			}
		}
		if projectValue, ok := d.GetOk("project"); ok && projectValue.(string) != "" {
			if err = identity.Set("project", projectValue.(string)); err != nil {
				return fmt.Errorf("Error setting project: %s", err)
			}
		}
	} else {
		log.Printf("[DEBUG] (Update) identity not set: %s", err)
	}

	billingProject := ""

	project, err := tpgresource.GetProject(d, config)
	if err != nil {
		return fmt.Errorf("Error fetching project for DatasetConfig: %s", err)
	}
	billingProject = project

	obj := make(map[string]interface{})
	includeNewlyCreatedBucketsProp, err := expandStorageInsightsDatasetConfigIncludeNewlyCreatedBuckets(d.Get("include_newly_created_buckets"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("include_newly_created_buckets"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, includeNewlyCreatedBucketsProp)) {
		obj["includeNewlyCreatedBuckets"] = includeNewlyCreatedBucketsProp
	}
	retentionPeriodDaysProp, err := expandStorageInsightsDatasetConfigRetentionPeriodDays(d.Get("retention_period_days"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("retention_period_days"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, retentionPeriodDaysProp)) {
		obj["retentionPeriodDays"] = retentionPeriodDaysProp
	}
	activityDataRetentionPeriodDaysProp, err := expandStorageInsightsDatasetConfigActivityDataRetentionPeriodDays(d.Get("activity_data_retention_period_days"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("activity_data_retention_period_days"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, activityDataRetentionPeriodDaysProp)) {
		obj["activityDataRetentionPeriodDays"] = activityDataRetentionPeriodDaysProp
	}
	descriptionProp, err := expandStorageInsightsDatasetConfigDescription(d.Get("description"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("description"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, descriptionProp)) {
		obj["description"] = descriptionProp
	}
	sourceProjectsProp, err := expandStorageInsightsDatasetConfigSourceProjects(d.Get("source_projects"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("source_projects"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, sourceProjectsProp)) {
		obj["sourceProjects"] = sourceProjectsProp
	}
	sourceFoldersProp, err := expandStorageInsightsDatasetConfigSourceFolders(d.Get("source_folders"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("source_folders"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, sourceFoldersProp)) {
		obj["sourceFolders"] = sourceFoldersProp
	}
	organizationScopeProp, err := expandStorageInsightsDatasetConfigOrganizationScope(d.Get("organization_scope"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("organization_scope"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, organizationScopeProp)) {
		obj["organizationScope"] = organizationScopeProp
	}
	includeCloudStorageLocationsProp, err := expandStorageInsightsDatasetConfigIncludeCloudStorageLocations(d.Get("include_cloud_storage_locations"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("include_cloud_storage_locations"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, includeCloudStorageLocationsProp)) {
		obj["includeCloudStorageLocations"] = includeCloudStorageLocationsProp
	}
	excludeCloudStorageLocationsProp, err := expandStorageInsightsDatasetConfigExcludeCloudStorageLocations(d.Get("exclude_cloud_storage_locations"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("exclude_cloud_storage_locations"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, excludeCloudStorageLocationsProp)) {
		obj["excludeCloudStorageLocations"] = excludeCloudStorageLocationsProp
	}
	includeCloudStorageBucketsProp, err := expandStorageInsightsDatasetConfigIncludeCloudStorageBuckets(d.Get("include_cloud_storage_buckets"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("include_cloud_storage_buckets"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, includeCloudStorageBucketsProp)) {
		obj["includeCloudStorageBuckets"] = includeCloudStorageBucketsProp
	}
	excludeCloudStorageBucketsProp, err := expandStorageInsightsDatasetConfigExcludeCloudStorageBuckets(d.Get("exclude_cloud_storage_buckets"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("exclude_cloud_storage_buckets"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, excludeCloudStorageBucketsProp)) {
		obj["excludeCloudStorageBuckets"] = excludeCloudStorageBucketsProp
	}

	url, err := tpgresource.ReplaceVars(d, config, "{{StorageInsightsBasePath}}projects/{{project}}/locations/{{location}}/datasetConfigs/{{dataset_config_id}}")
	if err != nil {
		return err
	}

	log.Printf("[DEBUG] Updating DatasetConfig %q: %#v", d.Id(), obj)
	headers := make(http.Header)
	updateMask := []string{}

	if d.HasChange("include_newly_created_buckets") {
		updateMask = append(updateMask, "includeNewlyCreatedBuckets")
	}

	if d.HasChange("retention_period_days") {
		updateMask = append(updateMask, "retentionPeriodDays")
	}

	if d.HasChange("activity_data_retention_period_days") {
		updateMask = append(updateMask, "activityDataRetentionPeriodDays")
	}

	if d.HasChange("description") {
		updateMask = append(updateMask, "description")
	}

	if d.HasChange("include_cloud_storage_locations") {
		_, new_storage_locations := d.GetChange("include_cloud_storage_locations")
		if new_locations, ok := new_storage_locations.([]interface{}); ok && len(new_locations) > 0 {
			updateMask = append(updateMask, "includeCloudStorageLocations")
		}
	}

	if d.HasChange("exclude_cloud_storage_locations") {
		_, new_storage_locations := d.GetChange("exclude_cloud_storage_locations")
		if new_locations, ok := new_storage_locations.([]interface{}); ok && len(new_locations) > 0 {
			updateMask = append(updateMask, "excludeCloudStorageLocations")
		}
	}

	if d.HasChange("include_cloud_storage_buckets") {
		_, new_storage_buckets := d.GetChange("include_cloud_storage_buckets")
		if new_buckets, ok := new_storage_buckets.([]interface{}); ok && len(new_buckets) > 0 {
			updateMask = append(updateMask, "includeCloudStorageBuckets")
		}
	}

	if d.HasChange("exclude_cloud_storage_buckets") {
		_, new_storage_buckets := d.GetChange("exclude_cloud_storage_buckets")
		if new_buckets, ok := new_storage_buckets.([]interface{}); ok && len(new_buckets) > 0 {
			updateMask = append(updateMask, "excludeCloudStorageBuckets")
		}
	}

	if d.HasChange("source_projects") {
		_, new_source_projects := d.GetChange("source_projects")
		if new_project_numbers, ok := new_source_projects.([]interface{}); ok && len(new_project_numbers) > 0 {
			updateMask = append(updateMask, "sourceProjects")
		}
	}

	if d.HasChange("source_folders") {
		_, new_source_folders := d.GetChange("source_folders")
		if new_folder_numbers, ok := new_source_folders.([]interface{}); ok && len(new_folder_numbers) > 0 {
			updateMask = append(updateMask, "sourceFolders")
		}
	}

	if d.HasChange("organization_scope") {
		_, new_organization_scope := d.GetChange("organization_scope")
		if new_organization_scope == true {
			updateMask = append(updateMask, "organizationScope")
		}
	}

	// Link or Unlink a dataset if required
	if d.HasChange("link_dataset") {
		_, new_link_dataset := d.GetChange("link_dataset")
		linkAPIEndPoint := "linkDataset"
		if new_link_dataset == false {
			linkAPIEndPoint = "unlinkDataset"
		}

		linkUrl := fmt.Sprintf("%s:%s", url, linkAPIEndPoint)

		// err == nil indicates that the billing_project value was found
		if bp, err := tpgresource.GetBillingProject(d, config); err == nil {
			billingProject = bp
		}

		res, err := transport_tpg.SendRequest(transport_tpg.SendRequestOptions{
			Config:    config,
			Method:    "POST",
			Project:   billingProject,
			RawURL:    linkUrl,
			UserAgent: userAgent,
			Timeout:   d.Timeout(schema.TimeoutUpdate),
			Headers:   headers,
		})

		if err != nil {
			return fmt.Errorf("Error %v DatasetConfig %q: %s", linkAPIEndPoint, d.Id(), err)
		} else {
			log.Printf("[DEBUG] Finished %s DatasetConfig %q: %#v", linkAPIEndPoint, d.Id(), res)
		}

		err = StorageInsightsOperationWaitTime(
			config, res, project, "Linking/Unlinking DatasetConfig", userAgent,
			d.Timeout(schema.TimeoutUpdate))

		if err != nil {
			return err
		}
	}

	// if updateMask is empty we are not updating anything so skip the post
	if len(updateMask) == 0 {
		return resourceStorageInsightsDatasetConfigRead(d, meta)
	}

	// updateMask is a URL parameter but not present in the schema, so ReplaceVars
	// won't set it
	url, err = transport_tpg.AddQueryParams(url, map[string]string{"updateMask": strings.Join(updateMask, ",")})
	if err != nil {
		return err
	}

	// err == nil indicates that the billing_project value was found
	if bp, err := tpgresource.GetBillingProject(d, config); err == nil {
		billingProject = bp
	}

	res, err := transport_tpg.SendRequest(transport_tpg.SendRequestOptions{
		Config:    config,
		Method:    "PATCH",
		Project:   billingProject,
		RawURL:    url,
		UserAgent: userAgent,
		Body:      obj,
		Timeout:   d.Timeout(schema.TimeoutUpdate),
		Headers:   headers,
	})

	if err != nil {
		return fmt.Errorf("Error updating DatasetConfig %q: %s", d.Id(), err)
	} else {
		log.Printf("[DEBUG] Finished updating DatasetConfig %q: %#v", d.Id(), res)
	}

	err = StorageInsightsOperationWaitTime(
		config, res, project, "Updating DatasetConfig", userAgent,
		d.Timeout(schema.TimeoutUpdate))

	if err != nil {
		return err
	}

	return resourceStorageInsightsDatasetConfigRead(d, meta)
}

func resourceStorageInsightsDatasetConfigDelete(d *schema.ResourceData, meta interface{}) error {
	config := meta.(*transport_tpg.Config)
	userAgent, err := tpgresource.GenerateUserAgentString(d, config.UserAgent)
	if err != nil {
		return err
	}

	billingProject := ""

	project, err := tpgresource.GetProject(d, config)
	if err != nil {
		return fmt.Errorf("Error fetching project for DatasetConfig: %s", err)
	}
	billingProject = project

	url, err := tpgresource.ReplaceVars(d, config, "{{StorageInsightsBasePath}}projects/{{project}}/locations/{{location}}/datasetConfigs/{{dataset_config_id}}")
	if err != nil {
		return err
	}

	var obj map[string]interface{}

	// err == nil indicates that the billing_project value was found
	if bp, err := tpgresource.GetBillingProject(d, config); err == nil {
		billingProject = bp
	}

	headers := make(http.Header)

	log.Printf("[DEBUG] Deleting DatasetConfig %q", d.Id())
	res, err := transport_tpg.SendRequest(transport_tpg.SendRequestOptions{
		Config:    config,
		Method:    "DELETE",
		Project:   billingProject,
		RawURL:    url,
		UserAgent: userAgent,
		Body:      obj,
		Timeout:   d.Timeout(schema.TimeoutDelete),
		Headers:   headers,
	})
	if err != nil {
		return transport_tpg.HandleNotFoundError(err, d, "DatasetConfig")
	}

	err = StorageInsightsOperationWaitTime(
		config, res, project, "Deleting DatasetConfig", userAgent,
		d.Timeout(schema.TimeoutDelete))

	if err != nil {
		return err
	}

	log.Printf("[DEBUG] Finished deleting DatasetConfig %q: %#v", d.Id(), res)
	return nil
}

func resourceStorageInsightsDatasetConfigImport(d *schema.ResourceData, meta interface{}) ([]*schema.ResourceData, error) {
	config := meta.(*transport_tpg.Config)
	if err := tpgresource.ParseImportId([]string{
		"^projects/(?P<project>[^/]+)/locations/(?P<location>[^/]+)/datasetConfigs/(?P<dataset_config_id>[^/]+)$",
		"^(?P<project>[^/]+)/(?P<location>[^/]+)/(?P<dataset_config_id>[^/]+)$",
		"^(?P<location>[^/]+)/(?P<dataset_config_id>[^/]+)$",
	}, d, config); err != nil {
		return nil, err
	}

	// Replace import id for the resource id
	id, err := tpgresource.ReplaceVars(d, config, "projects/{{project}}/locations/{{location}}/datasetConfigs/{{dataset_config_id}}")
	if err != nil {
		return nil, fmt.Errorf("Error constructing id: %s", err)
	}
	d.SetId(id)

	// Explicitly set virtual fields to default values on import
	if err := d.Set("link_dataset", false); err != nil {
		return nil, fmt.Errorf("Error setting link_dataset: %s", err)
	}

	return []*schema.ResourceData{d}, nil
}

func flattenStorageInsightsDatasetConfigName(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenStorageInsightsDatasetConfigCreateTime(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenStorageInsightsDatasetConfigUpdateTime(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenStorageInsightsDatasetConfigUid(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenStorageInsightsDatasetConfigOrganizationNumber(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenStorageInsightsDatasetConfigIncludeNewlyCreatedBuckets(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenStorageInsightsDatasetConfigRetentionPeriodDays(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenStorageInsightsDatasetConfigActivityDataRetentionPeriodDays(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenStorageInsightsDatasetConfigLink(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["dataset"] =
		flattenStorageInsightsDatasetConfigLinkDataset(original["dataset"], d, config)
	transformed["linked"] =
		flattenStorageInsightsDatasetConfigLinkLinked(original["linked"], d, config)
	return []interface{}{transformed}
}
func flattenStorageInsightsDatasetConfigLinkDataset(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenStorageInsightsDatasetConfigLinkLinked(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenStorageInsightsDatasetConfigIdentity(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["name"] =
		flattenStorageInsightsDatasetConfigIdentityName(original["name"], d, config)
	transformed["type"] =
		flattenStorageInsightsDatasetConfigIdentityType(original["type"], d, config)
	return []interface{}{transformed}
}
func flattenStorageInsightsDatasetConfigIdentityName(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenStorageInsightsDatasetConfigIdentityType(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenStorageInsightsDatasetConfigDatasetConfigState(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenStorageInsightsDatasetConfigDescription(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenStorageInsightsDatasetConfigSourceProjects(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["project_numbers"] =
		flattenStorageInsightsDatasetConfigSourceProjectsProjectNumbers(original["projectNumbers"], d, config)
	return []interface{}{transformed}
}
func flattenStorageInsightsDatasetConfigSourceProjectsProjectNumbers(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenStorageInsightsDatasetConfigSourceFolders(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["folder_numbers"] =
		flattenStorageInsightsDatasetConfigSourceFoldersFolderNumbers(original["folderNumbers"], d, config)
	return []interface{}{transformed}
}
func flattenStorageInsightsDatasetConfigSourceFoldersFolderNumbers(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenStorageInsightsDatasetConfigOrganizationScope(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenStorageInsightsDatasetConfigIncludeCloudStorageLocations(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["locations"] =
		flattenStorageInsightsDatasetConfigIncludeCloudStorageLocationsLocations(original["locations"], d, config)
	return []interface{}{transformed}
}
func flattenStorageInsightsDatasetConfigIncludeCloudStorageLocationsLocations(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenStorageInsightsDatasetConfigExcludeCloudStorageLocations(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["locations"] =
		flattenStorageInsightsDatasetConfigExcludeCloudStorageLocationsLocations(original["locations"], d, config)
	return []interface{}{transformed}
}
func flattenStorageInsightsDatasetConfigExcludeCloudStorageLocationsLocations(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenStorageInsightsDatasetConfigIncludeCloudStorageBuckets(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["cloud_storage_buckets"] =
		flattenStorageInsightsDatasetConfigIncludeCloudStorageBucketsCloudStorageBuckets(original["cloudStorageBuckets"], d, config)
	return []interface{}{transformed}
}
func flattenStorageInsightsDatasetConfigIncludeCloudStorageBucketsCloudStorageBuckets(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}
	l := v.([]interface{})
	transformed := make([]interface{}, 0, len(l))
	for _, raw := range l {
		original := raw.(map[string]interface{})
		if len(original) < 1 {
			// Do not include empty json objects coming back from the api
			continue
		}
		transformed = append(transformed, map[string]interface{}{
			"bucket_name":         flattenStorageInsightsDatasetConfigIncludeCloudStorageBucketsCloudStorageBucketsBucketName(original["bucketName"], d, config),
			"bucket_prefix_regex": flattenStorageInsightsDatasetConfigIncludeCloudStorageBucketsCloudStorageBucketsBucketPrefixRegex(original["bucketPrefixRegex"], d, config),
		})
	}
	return transformed
}
func flattenStorageInsightsDatasetConfigIncludeCloudStorageBucketsCloudStorageBucketsBucketName(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenStorageInsightsDatasetConfigIncludeCloudStorageBucketsCloudStorageBucketsBucketPrefixRegex(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenStorageInsightsDatasetConfigExcludeCloudStorageBuckets(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["cloud_storage_buckets"] =
		flattenStorageInsightsDatasetConfigExcludeCloudStorageBucketsCloudStorageBuckets(original["cloudStorageBuckets"], d, config)
	return []interface{}{transformed}
}
func flattenStorageInsightsDatasetConfigExcludeCloudStorageBucketsCloudStorageBuckets(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}
	l := v.([]interface{})
	transformed := make([]interface{}, 0, len(l))
	for _, raw := range l {
		original := raw.(map[string]interface{})
		if len(original) < 1 {
			// Do not include empty json objects coming back from the api
			continue
		}
		transformed = append(transformed, map[string]interface{}{
			"bucket_name":         flattenStorageInsightsDatasetConfigExcludeCloudStorageBucketsCloudStorageBucketsBucketName(original["bucketName"], d, config),
			"bucket_prefix_regex": flattenStorageInsightsDatasetConfigExcludeCloudStorageBucketsCloudStorageBucketsBucketPrefixRegex(original["bucketPrefixRegex"], d, config),
		})
	}
	return transformed
}
func flattenStorageInsightsDatasetConfigExcludeCloudStorageBucketsCloudStorageBucketsBucketName(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenStorageInsightsDatasetConfigExcludeCloudStorageBucketsCloudStorageBucketsBucketPrefixRegex(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func expandStorageInsightsDatasetConfigOrganizationNumber(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandStorageInsightsDatasetConfigIncludeNewlyCreatedBuckets(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandStorageInsightsDatasetConfigRetentionPeriodDays(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandStorageInsightsDatasetConfigActivityDataRetentionPeriodDays(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandStorageInsightsDatasetConfigIdentity(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	if v == nil {
		return nil, nil
	}
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedName, err := expandStorageInsightsDatasetConfigIdentityName(original["name"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedName); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["name"] = transformedName
	}

	transformedType, err := expandStorageInsightsDatasetConfigIdentityType(original["type"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedType); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["type"] = transformedType
	}

	return transformed, nil
}

func expandStorageInsightsDatasetConfigIdentityName(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandStorageInsightsDatasetConfigIdentityType(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandStorageInsightsDatasetConfigDescription(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandStorageInsightsDatasetConfigSourceProjects(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	if v == nil {
		return nil, nil
	}
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedProjectNumbers, err := expandStorageInsightsDatasetConfigSourceProjectsProjectNumbers(original["project_numbers"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedProjectNumbers); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["projectNumbers"] = transformedProjectNumbers
	}

	return transformed, nil
}

func expandStorageInsightsDatasetConfigSourceProjectsProjectNumbers(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandStorageInsightsDatasetConfigSourceFolders(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	if v == nil {
		return nil, nil
	}
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedFolderNumbers, err := expandStorageInsightsDatasetConfigSourceFoldersFolderNumbers(original["folder_numbers"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedFolderNumbers); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["folderNumbers"] = transformedFolderNumbers
	}

	return transformed, nil
}

func expandStorageInsightsDatasetConfigSourceFoldersFolderNumbers(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandStorageInsightsDatasetConfigOrganizationScope(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandStorageInsightsDatasetConfigIncludeCloudStorageLocations(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	if v == nil {
		return nil, nil
	}
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedLocations, err := expandStorageInsightsDatasetConfigIncludeCloudStorageLocationsLocations(original["locations"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedLocations); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["locations"] = transformedLocations
	}

	return transformed, nil
}

func expandStorageInsightsDatasetConfigIncludeCloudStorageLocationsLocations(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandStorageInsightsDatasetConfigExcludeCloudStorageLocations(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	if v == nil {
		return nil, nil
	}
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedLocations, err := expandStorageInsightsDatasetConfigExcludeCloudStorageLocationsLocations(original["locations"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedLocations); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["locations"] = transformedLocations
	}

	return transformed, nil
}

func expandStorageInsightsDatasetConfigExcludeCloudStorageLocationsLocations(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandStorageInsightsDatasetConfigIncludeCloudStorageBuckets(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	if v == nil {
		return nil, nil
	}
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedCloudStorageBuckets, err := expandStorageInsightsDatasetConfigIncludeCloudStorageBucketsCloudStorageBuckets(original["cloud_storage_buckets"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedCloudStorageBuckets); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["cloudStorageBuckets"] = transformedCloudStorageBuckets
	}

	return transformed, nil
}

func expandStorageInsightsDatasetConfigIncludeCloudStorageBucketsCloudStorageBuckets(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	if v == nil {
		return nil, nil
	}
	l := v.([]interface{})
	req := make([]interface{}, 0, len(l))
	for _, raw := range l {
		if raw == nil {
			continue
		}
		original := raw.(map[string]interface{})
		transformed := make(map[string]interface{})

		transformedBucketName, err := expandStorageInsightsDatasetConfigIncludeCloudStorageBucketsCloudStorageBucketsBucketName(original["bucket_name"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedBucketName); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["bucketName"] = transformedBucketName
		}

		transformedBucketPrefixRegex, err := expandStorageInsightsDatasetConfigIncludeCloudStorageBucketsCloudStorageBucketsBucketPrefixRegex(original["bucket_prefix_regex"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedBucketPrefixRegex); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["bucketPrefixRegex"] = transformedBucketPrefixRegex
		}

		req = append(req, transformed)
	}
	return req, nil
}

func expandStorageInsightsDatasetConfigIncludeCloudStorageBucketsCloudStorageBucketsBucketName(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandStorageInsightsDatasetConfigIncludeCloudStorageBucketsCloudStorageBucketsBucketPrefixRegex(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandStorageInsightsDatasetConfigExcludeCloudStorageBuckets(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	if v == nil {
		return nil, nil
	}
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedCloudStorageBuckets, err := expandStorageInsightsDatasetConfigExcludeCloudStorageBucketsCloudStorageBuckets(original["cloud_storage_buckets"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedCloudStorageBuckets); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["cloudStorageBuckets"] = transformedCloudStorageBuckets
	}

	return transformed, nil
}

func expandStorageInsightsDatasetConfigExcludeCloudStorageBucketsCloudStorageBuckets(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	if v == nil {
		return nil, nil
	}
	l := v.([]interface{})
	req := make([]interface{}, 0, len(l))
	for _, raw := range l {
		if raw == nil {
			continue
		}
		original := raw.(map[string]interface{})
		transformed := make(map[string]interface{})

		transformedBucketName, err := expandStorageInsightsDatasetConfigExcludeCloudStorageBucketsCloudStorageBucketsBucketName(original["bucket_name"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedBucketName); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["bucketName"] = transformedBucketName
		}

		transformedBucketPrefixRegex, err := expandStorageInsightsDatasetConfigExcludeCloudStorageBucketsCloudStorageBucketsBucketPrefixRegex(original["bucket_prefix_regex"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedBucketPrefixRegex); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["bucketPrefixRegex"] = transformedBucketPrefixRegex
		}

		req = append(req, transformed)
	}
	return req, nil
}

func expandStorageInsightsDatasetConfigExcludeCloudStorageBucketsCloudStorageBucketsBucketName(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandStorageInsightsDatasetConfigExcludeCloudStorageBucketsCloudStorageBucketsBucketPrefixRegex(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}
