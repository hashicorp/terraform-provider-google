// Copyright (c) HashiCorp, Inc.
// SPDX-License-Identifier: MPL-2.0

// ----------------------------------------------------------------------------
//
//     ***     AUTO GENERATED CODE    ***    Type: MMv1     ***
//
// ----------------------------------------------------------------------------
//
//     This file is automatically generated by Magic Modules and manual
//     changes will be clobbered when the file is regenerated.
//
//     Please read more about how to change this file in
//     .github/CONTRIBUTING.md.
//
// ----------------------------------------------------------------------------

package dataprocgdc_test

import (
	"fmt"
	"strings"
	"testing"

	"github.com/hashicorp/terraform-plugin-testing/helper/resource"
	"github.com/hashicorp/terraform-plugin-testing/terraform"

	"github.com/hashicorp/terraform-provider-google/google/acctest"
	"github.com/hashicorp/terraform-provider-google/google/tpgresource"
	transport_tpg "github.com/hashicorp/terraform-provider-google/google/transport"
)

func TestAccDataprocGdcSparkApplication_dataprocgdcSparkapplicationBasicExample(t *testing.T) {
	t.Parallel()

	context := map[string]interface{}{
		"project":       "gdce-cluster-monitoring",
		"random_suffix": acctest.RandString(t, 10),
	}

	acctest.VcrTest(t, resource.TestCase{
		PreCheck:                 func() { acctest.AccTestPreCheck(t) },
		ProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories(t),
		CheckDestroy:             testAccCheckDataprocGdcSparkApplicationDestroyProducer(t),
		Steps: []resource.TestStep{
			{
				Config: testAccDataprocGdcSparkApplication_dataprocgdcSparkapplicationBasicExample(context),
			},
			{
				ResourceName:            "google_dataproc_gdc_spark_application.spark-application",
				ImportState:             true,
				ImportStateVerify:       true,
				ImportStateVerifyIgnore: []string{"annotations", "labels", "location", "serviceinstance", "spark_application_id", "terraform_labels"},
			},
		},
	})
}

func testAccDataprocGdcSparkApplication_dataprocgdcSparkapplicationBasicExample(context map[string]interface{}) string {
	return acctest.Nprintf(`
resource "google_dataproc_gdc_spark_application" "spark-application" {
  spark_application_id = "tf-test-tf-e2e-spark-app-basic%{random_suffix}"
  serviceinstance = "do-not-delete-dataproc-gdc-instance"
  project         = "%{project}"
  location        = "us-west2"
  namespace = "default"
  spark_application_config {
    main_class = "org.apache.spark.examples.SparkPi"
    jar_file_uris = ["file:///usr/lib/spark/examples/jars/spark-examples.jar"]
    args = ["10000"]
  }
}
`, context)
}

func TestAccDataprocGdcSparkApplication_dataprocgdcSparkapplicationExample(t *testing.T) {
	t.Parallel()

	context := map[string]interface{}{
		"project":       "gdce-cluster-monitoring",
		"random_suffix": acctest.RandString(t, 10),
	}

	acctest.VcrTest(t, resource.TestCase{
		PreCheck:                 func() { acctest.AccTestPreCheck(t) },
		ProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories(t),
		CheckDestroy:             testAccCheckDataprocGdcSparkApplicationDestroyProducer(t),
		Steps: []resource.TestStep{
			{
				Config: testAccDataprocGdcSparkApplication_dataprocgdcSparkapplicationExample(context),
			},
			{
				ResourceName:            "google_dataproc_gdc_spark_application.spark-application",
				ImportState:             true,
				ImportStateVerify:       true,
				ImportStateVerifyIgnore: []string{"annotations", "labels", "location", "serviceinstance", "spark_application_id", "terraform_labels"},
			},
		},
	})
}

func testAccDataprocGdcSparkApplication_dataprocgdcSparkapplicationExample(context map[string]interface{}) string {
	return acctest.Nprintf(`
resource "google_dataproc_gdc_application_environment" "app_env" {
  application_environment_id = "tf-test-tf-e2e-spark-app-env%{random_suffix}"
  serviceinstance = "do-not-delete-dataproc-gdc-instance"
  project         = "%{project}"
  location        = "us-west2"
  namespace = "default"
}

resource "google_dataproc_gdc_spark_application" "spark-application" {
  spark_application_id = "tf-test-tf-e2e-spark-app%{random_suffix}"
  serviceinstance = "do-not-delete-dataproc-gdc-instance"
  project         = "%{project}"
  location        = "us-west2"
  namespace = "default"
  labels = {
    "test-label": "label-value"
  }
  annotations = {
    "an_annotation": "annotation_value"
  }
  properties = {
    "spark.executor.instances": "2"
  }
  application_environment = google_dataproc_gdc_application_environment.app_env.name
  version = "1.2"
  spark_application_config {
    main_jar_file_uri = "file:///usr/lib/spark/examples/jars/spark-examples.jar"
    jar_file_uris = ["file:///usr/lib/spark/examples/jars/spark-examples.jar"]
    archive_uris = ["file://usr/lib/spark/examples/spark-examples.jar"]
    file_uris = ["file:///usr/lib/spark/examples/jars/spark-examples.jar"]
  }
}
`, context)
}

func TestAccDataprocGdcSparkApplication_dataprocgdcSparkapplicationPysparkExample(t *testing.T) {
	t.Parallel()

	context := map[string]interface{}{
		"project":       "gdce-cluster-monitoring",
		"random_suffix": acctest.RandString(t, 10),
	}

	acctest.VcrTest(t, resource.TestCase{
		PreCheck:                 func() { acctest.AccTestPreCheck(t) },
		ProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories(t),
		CheckDestroy:             testAccCheckDataprocGdcSparkApplicationDestroyProducer(t),
		Steps: []resource.TestStep{
			{
				Config: testAccDataprocGdcSparkApplication_dataprocgdcSparkapplicationPysparkExample(context),
			},
			{
				ResourceName:            "google_dataproc_gdc_spark_application.spark-application",
				ImportState:             true,
				ImportStateVerify:       true,
				ImportStateVerifyIgnore: []string{"annotations", "labels", "location", "serviceinstance", "spark_application_id", "terraform_labels"},
			},
		},
	})
}

func testAccDataprocGdcSparkApplication_dataprocgdcSparkapplicationPysparkExample(context map[string]interface{}) string {
	return acctest.Nprintf(`
resource "google_dataproc_gdc_spark_application" "spark-application" {
  spark_application_id = "tf-test-tf-e2e-pyspark-app%{random_suffix}"
  serviceinstance = "do-not-delete-dataproc-gdc-instance"
  project         = "%{project}"
  location        = "us-west2"
  namespace = "default"
  display_name = "A Pyspark application for a Terraform create test"
  dependency_images = ["gcr.io/some/image"]
  pyspark_application_config {
    main_python_file_uri = "gs://goog-dataproc-initialization-actions-us-west2/conda/test_conda.py"
    jar_file_uris = ["file:///usr/lib/spark/examples/jars/spark-examples.jar"]
    python_file_uris = ["gs://goog-dataproc-initialization-actions-us-west2/conda/get-sys-exec.py"]
    file_uris = ["file://usr/lib/spark/examples/spark-examples.jar"]
    archive_uris = ["file://usr/lib/spark/examples/spark-examples.jar"]
    args = ["10"]
  }
}
`, context)
}

func TestAccDataprocGdcSparkApplication_dataprocgdcSparkapplicationSparkrExample(t *testing.T) {
	t.Parallel()

	context := map[string]interface{}{
		"project":       "gdce-cluster-monitoring",
		"random_suffix": acctest.RandString(t, 10),
	}

	acctest.VcrTest(t, resource.TestCase{
		PreCheck:                 func() { acctest.AccTestPreCheck(t) },
		ProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories(t),
		CheckDestroy:             testAccCheckDataprocGdcSparkApplicationDestroyProducer(t),
		Steps: []resource.TestStep{
			{
				Config: testAccDataprocGdcSparkApplication_dataprocgdcSparkapplicationSparkrExample(context),
			},
			{
				ResourceName:            "google_dataproc_gdc_spark_application.spark-application",
				ImportState:             true,
				ImportStateVerify:       true,
				ImportStateVerifyIgnore: []string{"annotations", "labels", "location", "serviceinstance", "spark_application_id", "terraform_labels"},
			},
		},
	})
}

func testAccDataprocGdcSparkApplication_dataprocgdcSparkapplicationSparkrExample(context map[string]interface{}) string {
	return acctest.Nprintf(`
resource "google_dataproc_gdc_spark_application" "spark-application" {
  spark_application_id = "tf-test-tf-e2e-sparkr-app%{random_suffix}"
  serviceinstance = "do-not-delete-dataproc-gdc-instance"
  project         = "%{project}"
  location        = "us-west2"
  namespace = "default"
  display_name = "A SparkR application for a Terraform create test"
  spark_r_application_config {
    main_r_file_uri = "gs://some-bucket/something.R"
    file_uris = ["file://usr/lib/spark/examples/spark-examples.jar"]
    archive_uris = ["file://usr/lib/spark/examples/spark-examples.jar"]
    args = ["10"]
  }
}
`, context)
}

func TestAccDataprocGdcSparkApplication_dataprocgdcSparkapplicationSparksqlExample(t *testing.T) {
	t.Parallel()

	context := map[string]interface{}{
		"project":       "gdce-cluster-monitoring",
		"random_suffix": acctest.RandString(t, 10),
	}

	acctest.VcrTest(t, resource.TestCase{
		PreCheck:                 func() { acctest.AccTestPreCheck(t) },
		ProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories(t),
		CheckDestroy:             testAccCheckDataprocGdcSparkApplicationDestroyProducer(t),
		Steps: []resource.TestStep{
			{
				Config: testAccDataprocGdcSparkApplication_dataprocgdcSparkapplicationSparksqlExample(context),
			},
			{
				ResourceName:            "google_dataproc_gdc_spark_application.spark-application",
				ImportState:             true,
				ImportStateVerify:       true,
				ImportStateVerifyIgnore: []string{"annotations", "labels", "location", "serviceinstance", "spark_application_id", "terraform_labels"},
			},
		},
	})
}

func testAccDataprocGdcSparkApplication_dataprocgdcSparkapplicationSparksqlExample(context map[string]interface{}) string {
	return acctest.Nprintf(`
resource "google_dataproc_gdc_spark_application" "spark-application" {
  spark_application_id = "tf-test-tf-e2e-sparksql-app%{random_suffix}"
  serviceinstance = "do-not-delete-dataproc-gdc-instance"
  project         = "%{project}"
  location        = "us-west2"
  namespace = "default"
  display_name = "A SparkSql application for a Terraform create test"
  spark_sql_application_config {
    jar_file_uris = ["file:///usr/lib/spark/examples/jars/spark-examples.jar"]
    query_list {
      queries = ["show tables;"]
    }
    script_variables = {
      "MY_VAR": "1"
    }
  }
}
`, context)
}

func TestAccDataprocGdcSparkApplication_dataprocgdcSparkapplicationSparksqlQueryFileExample(t *testing.T) {
	t.Parallel()

	context := map[string]interface{}{
		"project":       "gdce-cluster-monitoring",
		"random_suffix": acctest.RandString(t, 10),
	}

	acctest.VcrTest(t, resource.TestCase{
		PreCheck:                 func() { acctest.AccTestPreCheck(t) },
		ProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories(t),
		CheckDestroy:             testAccCheckDataprocGdcSparkApplicationDestroyProducer(t),
		Steps: []resource.TestStep{
			{
				Config: testAccDataprocGdcSparkApplication_dataprocgdcSparkapplicationSparksqlQueryFileExample(context),
			},
			{
				ResourceName:            "google_dataproc_gdc_spark_application.spark-application",
				ImportState:             true,
				ImportStateVerify:       true,
				ImportStateVerifyIgnore: []string{"annotations", "labels", "location", "serviceinstance", "spark_application_id", "terraform_labels"},
			},
		},
	})
}

func testAccDataprocGdcSparkApplication_dataprocgdcSparkapplicationSparksqlQueryFileExample(context map[string]interface{}) string {
	return acctest.Nprintf(`
resource "google_dataproc_gdc_spark_application" "spark-application" {
  spark_application_id = "tf-test-tf-e2e-sparksql-app%{random_suffix}"
  serviceinstance = "do-not-delete-dataproc-gdc-instance"
  project         = "%{project}"
  location        = "us-west2"
  namespace = "default"
  display_name = "A SparkSql application for a Terraform create test"
  spark_sql_application_config {
    jar_file_uris = ["file:///usr/lib/spark/examples/jars/spark-examples.jar"]
    query_file_uri = "gs://some-bucket/something.sql"
    script_variables = {
      "MY_VAR": "1"
    }
  }
}
`, context)
}

func testAccCheckDataprocGdcSparkApplicationDestroyProducer(t *testing.T) func(s *terraform.State) error {
	return func(s *terraform.State) error {
		for name, rs := range s.RootModule().Resources {
			if rs.Type != "google_dataproc_gdc_spark_application" {
				continue
			}
			if strings.HasPrefix(name, "data.") {
				continue
			}

			config := acctest.GoogleProviderConfig(t)

			url, err := tpgresource.ReplaceVarsForTest(config, rs, "{{DataprocGdcBasePath}}projects/{{project}}/locations/{{location}}/serviceInstances/{{serviceinstance}}/sparkApplications/{{spark_application_id}}")
			if err != nil {
				return err
			}

			billingProject := ""

			if config.BillingProject != "" {
				billingProject = config.BillingProject
			}

			_, err = transport_tpg.SendRequest(transport_tpg.SendRequestOptions{
				Config:    config,
				Method:    "GET",
				Project:   billingProject,
				RawURL:    url,
				UserAgent: config.UserAgent,
			})
			if err == nil {
				return fmt.Errorf("DataprocGdcSparkApplication still exists at %s", url)
			}
		}

		return nil
	}
}
