// Copyright (c) HashiCorp, Inc.
// SPDX-License-Identifier: MPL-2.0

// ----------------------------------------------------------------------------
//
//     ***     AUTO GENERATED CODE    ***    Type: MMv1     ***
//
// ----------------------------------------------------------------------------
//
//     This file is automatically generated by Magic Modules and manual
//     changes will be clobbered when the file is regenerated.
//
//     Please read more about how to change this file in
//     .github/CONTRIBUTING.md.
//
// ----------------------------------------------------------------------------

package dataproc_test

import (
	"fmt"
	"strings"
	"testing"

	"github.com/hashicorp/terraform-plugin-testing/helper/resource"
	"github.com/hashicorp/terraform-plugin-testing/terraform"

	"github.com/hashicorp/terraform-provider-google/google/acctest"
	"github.com/hashicorp/terraform-provider-google/google/envvar"
	"github.com/hashicorp/terraform-provider-google/google/tpgresource"
	transport_tpg "github.com/hashicorp/terraform-provider-google/google/transport"
)

func TestAccDataprocBatch_dataprocBatchSparkExample(t *testing.T) {
	t.Parallel()

	context := map[string]interface{}{
		"project_name":    envvar.GetTestProjectFromEnv(),
		"prevent_destroy": false,
		"subnetwork_name": acctest.BootstrapSubnetWithFirewallForDataprocBatches(t, "dataproc-spark-test-network", "dataproc-spark-test-subnetwork"),
		"random_suffix":   acctest.RandString(t, 10),
	}

	acctest.VcrTest(t, resource.TestCase{
		PreCheck:                 func() { acctest.AccTestPreCheck(t) },
		ProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories(t),
		CheckDestroy:             testAccCheckDataprocBatchDestroyProducer(t),
		Steps: []resource.TestStep{
			{
				Config: testAccDataprocBatch_dataprocBatchSparkExample(context),
			},
			{
				ResourceName:            "google_dataproc_batch.example_batch_spark",
				ImportState:             true,
				ImportStateVerify:       true,
				ImportStateVerifyIgnore: []string{"batch_id", "labels", "location", "runtime_config.0.properties", "terraform_labels"},
			},
		},
	})
}

func testAccDataprocBatch_dataprocBatchSparkExample(context map[string]interface{}) string {
	return acctest.Nprintf(`
resource "google_dataproc_batch" "example_batch_spark" {

    batch_id      = "tf-test-batch%{random_suffix}"
    location      = "us-central1"
    labels        = {"batch_test": "terraform"}

    runtime_config {
      properties    = { "spark.dynamicAllocation.enabled": "false", "spark.executor.instances": "2" }
    }

    environment_config {
      execution_config {
        subnetwork_uri = "%{subnetwork_name}"
        ttl            = "3600s"
        network_tags   = ["tag1"]
      }
    }

    spark_batch {
      main_class    = "org.apache.spark.examples.SparkPi"
      args          = ["10"]
      jar_file_uris = ["file:///usr/lib/spark/examples/jars/spark-examples.jar"]
    }
}
`, context)
}

func TestAccDataprocBatch_dataprocBatchSparkFullExample(t *testing.T) {
	t.Parallel()

	context := map[string]interface{}{
		"project_name":    envvar.GetTestProjectFromEnv(),
		"kms_key_name":    acctest.BootstrapKMSKeyWithPurposeInLocationAndName(t, "ENCRYPT_DECRYPT", "us-central1", "tf-bootstrap-dataproc-batch-key1").CryptoKey.Name,
		"prevent_destroy": false,
		"random_suffix":   acctest.RandString(t, 10),
	}

	acctest.VcrTest(t, resource.TestCase{
		PreCheck:                 func() { acctest.AccTestPreCheck(t) },
		ProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories(t),
		CheckDestroy:             testAccCheckDataprocBatchDestroyProducer(t),
		Steps: []resource.TestStep{
			{
				Config: testAccDataprocBatch_dataprocBatchSparkFullExample(context),
			},
			{
				ResourceName:            "google_dataproc_batch.example_batch_spark",
				ImportState:             true,
				ImportStateVerify:       true,
				ImportStateVerifyIgnore: []string{"batch_id", "labels", "location", "runtime_config.0.properties", "terraform_labels"},
			},
		},
	})
}

func testAccDataprocBatch_dataprocBatchSparkFullExample(context map[string]interface{}) string {
	return acctest.Nprintf(`
data "google_project" "project" {
}

data "google_storage_project_service_account" "gcs_account" {
}

resource "google_dataproc_batch" "example_batch_spark" {
    batch_id      = "tf-test-dataproc-batch%{random_suffix}"
    location      = "us-central1"
    labels        = {"batch_test": "terraform"}

    runtime_config {
      properties    = { "spark.dynamicAllocation.enabled": "false", "spark.executor.instances": "2" }
      version = "2.2"
    }

    environment_config {
      execution_config {
        ttl = "3600s"
        network_tags = ["tag1"]
        kms_key = "%{kms_key_name}"
        network_uri = "default"
        service_account = "${data.google_project.project.number}-compute@developer.gserviceaccount.com"
        staging_bucket = google_storage_bucket.bucket.name
      }
      peripherals_config {
        metastore_service = google_dataproc_metastore_service.ms.name
        spark_history_server_config {
          dataproc_cluster = google_dataproc_cluster.basic.id
        }
      }
    }

    spark_batch {
      main_class    = "org.apache.spark.examples.SparkPi"
      args          = ["10"]
      jar_file_uris = ["file:///usr/lib/spark/examples/jars/spark-examples.jar"]
    }

    depends_on = [
      google_kms_crypto_key_iam_member.crypto_key_member_1,
    ]
}

resource "google_storage_bucket" "bucket" {
  uniform_bucket_level_access = true
  name                        = "tf-test-dataproc-bucket%{random_suffix}"
  location                    = "US"
  force_destroy               = true
}

resource "google_kms_crypto_key_iam_member" "crypto_key_member_1" {
  crypto_key_id = "%{kms_key_name}"
  role          = "roles/cloudkms.cryptoKeyEncrypterDecrypter"
  member        = "serviceAccount:service-${data.google_project.project.number}@dataproc-accounts.iam.gserviceaccount.com"
}

resource "google_dataproc_cluster" "basic" {
  name   = "tf-test-dataproc-batch%{random_suffix}"
  region = "us-central1"

  cluster_config {
    # Keep the costs down with smallest config we can get away with
    software_config {
      override_properties = {
        "dataproc:dataproc.allow.zero.workers" = "true"
        "spark:spark.history.fs.logDirectory"  = "gs://${google_storage_bucket.bucket.name}/*/spark-job-history"
      }
    }
 
    endpoint_config {
      enable_http_port_access = true
    }

    master_config {
      num_instances = 1
      machine_type  = "e2-standard-2"
      disk_config {
        boot_disk_size_gb = 35
      }
    }

    metastore_config {
      dataproc_metastore_service = google_dataproc_metastore_service.ms.name
    }
  }   
}

 resource "google_dataproc_metastore_service" "ms" {
  service_id = "tf-test-dataproc-batch%{random_suffix}"
  location   = "us-central1"
  port       = 9080
  tier       = "DEVELOPER"

  maintenance_window {
    hour_of_day = 2
    day_of_week = "SUNDAY"
  }

  hive_metastore_config {
    version = "3.1.2"
  }
}
`, context)
}

func TestAccDataprocBatch_dataprocBatchSparksqlExample(t *testing.T) {
	t.Parallel()

	context := map[string]interface{}{
		"project_name":    envvar.GetTestProjectFromEnv(),
		"prevent_destroy": false,
		"subnetwork_name": acctest.BootstrapSubnetWithFirewallForDataprocBatches(t, "dataproc-sparksql-test-network", "dataproc-sparksql-test-subnetwork"),
		"random_suffix":   acctest.RandString(t, 10),
	}

	acctest.VcrTest(t, resource.TestCase{
		PreCheck:                 func() { acctest.AccTestPreCheck(t) },
		ProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories(t),
		CheckDestroy:             testAccCheckDataprocBatchDestroyProducer(t),
		Steps: []resource.TestStep{
			{
				Config: testAccDataprocBatch_dataprocBatchSparksqlExample(context),
			},
			{
				ResourceName:            "google_dataproc_batch.example_batch_sparsql",
				ImportState:             true,
				ImportStateVerify:       true,
				ImportStateVerifyIgnore: []string{"batch_id", "labels", "location", "runtime_config.0.properties", "terraform_labels"},
			},
		},
	})
}

func testAccDataprocBatch_dataprocBatchSparksqlExample(context map[string]interface{}) string {
	return acctest.Nprintf(`
resource "google_dataproc_batch" "example_batch_sparsql" {

    batch_id      = "tf-test-batch%{random_suffix}"
    location      = "us-central1"

    runtime_config {
      properties    = { "spark.dynamicAllocation.enabled": "false", "spark.executor.instances": "2" }
    }

    environment_config {
      execution_config {
        subnetwork_uri = "%{subnetwork_name}"
      }
    }

    spark_sql_batch {
      query_file_uri   = "gs://dataproc-examples/spark-sql/natality/cigarette_correlations.sql"
      jar_file_uris    = ["file:///usr/lib/spark/examples/jars/spark-examples.jar"]
      query_variables  = {
        name = "value"
      }
    }
}
`, context)
}

func TestAccDataprocBatch_dataprocBatchPysparkExample(t *testing.T) {
	t.Parallel()

	context := map[string]interface{}{
		"project_name":    envvar.GetTestProjectFromEnv(),
		"prevent_destroy": false,
		"subnetwork_name": acctest.BootstrapSubnetWithFirewallForDataprocBatches(t, "dataproc-pyspark-test-network", "dataproc-pyspark-test-subnetwork"),
		"random_suffix":   acctest.RandString(t, 10),
	}

	acctest.VcrTest(t, resource.TestCase{
		PreCheck:                 func() { acctest.AccTestPreCheck(t) },
		ProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories(t),
		CheckDestroy:             testAccCheckDataprocBatchDestroyProducer(t),
		Steps: []resource.TestStep{
			{
				Config: testAccDataprocBatch_dataprocBatchPysparkExample(context),
			},
			{
				ResourceName:            "google_dataproc_batch.example_batch_pyspark",
				ImportState:             true,
				ImportStateVerify:       true,
				ImportStateVerifyIgnore: []string{"batch_id", "labels", "location", "runtime_config.0.properties", "terraform_labels"},
			},
		},
	})
}

func testAccDataprocBatch_dataprocBatchPysparkExample(context map[string]interface{}) string {
	return acctest.Nprintf(`
resource "google_dataproc_batch" "example_batch_pyspark" {
    batch_id      = "tf-test-batch%{random_suffix}"
    location      = "us-central1"

    runtime_config {
      properties    = { "spark.dynamicAllocation.enabled": "false", "spark.executor.instances": "2" }
    }

    environment_config {
      execution_config {
        subnetwork_uri = "%{subnetwork_name}"
      }
    }

    pyspark_batch {
      main_python_file_uri = "https://storage.googleapis.com/terraform-batches/test_util.py"
      args                 = ["10"]
      jar_file_uris        = ["file:///usr/lib/spark/examples/jars/spark-examples.jar"]
      python_file_uris     = ["gs://dataproc-examples/pyspark/hello-world/hello-world.py"]
      archive_uris         = [
        "https://storage.googleapis.com/terraform-batches/animals.txt.tar.gz#unpacked",
        "https://storage.googleapis.com/terraform-batches/animals.txt.jar",
        "https://storage.googleapis.com/terraform-batches/animals.txt"
      ]
      file_uris            = ["https://storage.googleapis.com/terraform-batches/people.txt"]
    }
}
`, context)
}

func TestAccDataprocBatch_dataprocBatchSparkrExample(t *testing.T) {
	t.Parallel()

	context := map[string]interface{}{
		"project_name":    envvar.GetTestProjectFromEnv(),
		"prevent_destroy": false,
		"subnetwork_name": acctest.BootstrapSubnetWithFirewallForDataprocBatches(t, "dataproc-pyspark-test-network", "dataproc-pyspark-test-subnetwork"),
		"random_suffix":   acctest.RandString(t, 10),
	}

	acctest.VcrTest(t, resource.TestCase{
		PreCheck:                 func() { acctest.AccTestPreCheck(t) },
		ProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories(t),
		CheckDestroy:             testAccCheckDataprocBatchDestroyProducer(t),
		Steps: []resource.TestStep{
			{
				Config: testAccDataprocBatch_dataprocBatchSparkrExample(context),
			},
			{
				ResourceName:            "google_dataproc_batch.example_batch_sparkr",
				ImportState:             true,
				ImportStateVerify:       true,
				ImportStateVerifyIgnore: []string{"batch_id", "labels", "location", "runtime_config.0.properties", "terraform_labels"},
			},
		},
	})
}

func testAccDataprocBatch_dataprocBatchSparkrExample(context map[string]interface{}) string {
	return acctest.Nprintf(`
resource "google_dataproc_batch" "example_batch_sparkr" {

    batch_id      = "tf-test-batch%{random_suffix}"
    location      = "us-central1"
    labels        = {"batch_test": "terraform"}

    runtime_config {
      properties    = { "spark.dynamicAllocation.enabled": "false", "spark.executor.instances": "2" }
    }

    environment_config {
      execution_config {
        subnetwork_uri = "%{subnetwork_name}"
        ttl            = "3600s"
        network_tags   = ["tag1"]
      }
    }

    spark_r_batch {
      main_r_file_uri  = "https://storage.googleapis.com/terraform-batches/spark-r-flights.r"
      args             = ["https://storage.googleapis.com/terraform-batches/flights.csv"]
    }
}
`, context)
}

func TestAccDataprocBatch_dataprocBatchAutotuningExample(t *testing.T) {
	t.Parallel()

	context := map[string]interface{}{
		"project_name":    envvar.GetTestProjectFromEnv(),
		"prevent_destroy": false,
		"subnetwork_name": acctest.BootstrapSubnetWithFirewallForDataprocBatches(t, "dataproc-autotuning-test-network", "dataproc-autotuning-test-subnetwork"),
		"random_suffix":   acctest.RandString(t, 10),
	}

	acctest.VcrTest(t, resource.TestCase{
		PreCheck:                 func() { acctest.AccTestPreCheck(t) },
		ProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories(t),
		CheckDestroy:             testAccCheckDataprocBatchDestroyProducer(t),
		Steps: []resource.TestStep{
			{
				Config: testAccDataprocBatch_dataprocBatchAutotuningExample(context),
			},
			{
				ResourceName:            "google_dataproc_batch.example_batch_autotuning",
				ImportState:             true,
				ImportStateVerify:       true,
				ImportStateVerifyIgnore: []string{"batch_id", "labels", "location", "runtime_config.0.properties", "terraform_labels"},
			},
		},
	})
}

func testAccDataprocBatch_dataprocBatchAutotuningExample(context map[string]interface{}) string {
	return acctest.Nprintf(`
resource "google_dataproc_batch" "example_batch_autotuning" {

    batch_id      = "tf-test-batch%{random_suffix}"
    location      = "us-central1"
    labels        = {"batch_test": "terraform"}

    runtime_config {
      version       = "2.2"
      properties    = { "spark.dynamicAllocation.enabled": "false", "spark.executor.instances": "2" }
      cohort        = "tf-dataproc-batch-example"
      autotuning_config {
        scenarios = ["SCALING", "MEMORY"]
      }
    }

    environment_config {
      execution_config {
        subnetwork_uri = "%{subnetwork_name}"
        ttl            = "3600s"
      }
    }

    spark_batch {
      main_class    = "org.apache.spark.examples.SparkPi"
      args          = ["10"]
      jar_file_uris = ["file:///usr/lib/spark/examples/jars/spark-examples.jar"]
    }
}
`, context)
}

func testAccCheckDataprocBatchDestroyProducer(t *testing.T) func(s *terraform.State) error {
	return func(s *terraform.State) error {
		for name, rs := range s.RootModule().Resources {
			if rs.Type != "google_dataproc_batch" {
				continue
			}
			if strings.HasPrefix(name, "data.") {
				continue
			}

			config := acctest.GoogleProviderConfig(t)

			url, err := tpgresource.ReplaceVarsForTest(config, rs, "{{DataprocBasePath}}projects/{{project}}/locations/{{location}}/batches/{{batch_id}}")
			if err != nil {
				return err
			}

			billingProject := ""

			if config.BillingProject != "" {
				billingProject = config.BillingProject
			}

			_, err = transport_tpg.SendRequest(transport_tpg.SendRequestOptions{
				Config:    config,
				Method:    "GET",
				Project:   billingProject,
				RawURL:    url,
				UserAgent: config.UserAgent,
			})
			if err == nil {
				return fmt.Errorf("DataprocBatch still exists at %s", url)
			}
		}

		return nil
	}
}
