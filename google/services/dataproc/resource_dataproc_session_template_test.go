// Copyright IBM Corp. 2014, 2025
// SPDX-License-Identifier: MPL-2.0
// ----------------------------------------------------------------------------
//
//	***     AUTO GENERATED CODE    ***    Type: Handwritten     ***
//
// ----------------------------------------------------------------------------
//
//	This code is generated by Magic Modules using the following:
//
//	Source file: https://github.com/GoogleCloudPlatform/magic-modules/tree/main/mmv1/third_party/terraform/services/dataproc/resource_dataproc_session_template_test.go
//
//	DO NOT EDIT this file directly. Any changes made to this file will be
//	overwritten during the next generation cycle.
//
// ----------------------------------------------------------------------------
package dataproc_test

import (
	"testing"

	"github.com/hashicorp/terraform-plugin-testing/helper/resource"
	"github.com/hashicorp/terraform-plugin-testing/plancheck"

	"github.com/hashicorp/terraform-provider-google/google/acctest"
	"github.com/hashicorp/terraform-provider-google/google/envvar"
)

func TestAccDataprocSessionTemplate_update(t *testing.T) {
	t.Parallel()

	acctest.BootstrapIamMembers(t, []acctest.IamMember{
		{
			Member: "serviceAccount:service-{project_number}@dataproc-accounts.iam.gserviceaccount.com",
			Role:   "roles/cloudkms.cryptoKeyEncrypterDecrypter",
		},
	})

	context := map[string]interface{}{
		"project_name":    envvar.GetTestProjectFromEnv(),
		"kms_key_name":    acctest.BootstrapKMSKeyWithPurposeInLocationAndName(t, "ENCRYPT_DECRYPT", "us-central1", "tf-bootstrap-dataproc-session-template-key1").CryptoKey.Name,
		"prevent_destroy": false,
		"subnetwork_name": acctest.BootstrapSubnetWithFirewallForDataprocBatches(t, "jupyer-session-test-network", "jupyter-session-test-subnetwork"),
		"random_suffix":   acctest.RandString(t, 10),
	}

	acctest.VcrTest(t, resource.TestCase{
		PreCheck:                 func() { acctest.AccTestPreCheck(t) },
		ProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories(t),
		CheckDestroy:             testAccCheckDataprocSessionTemplateDestroyProducer(t),
		Steps: []resource.TestStep{
			{
				Config: testAccDataprocSessionTemplate_preupdate(context),
			},
			{
				ResourceName:            "google_dataproc_session_template.dataproc_session_templates_jupyter_update",
				ImportState:             true,
				ImportStateVerify:       true,
				ImportStateVerifyIgnore: []string{"labels", "location", "runtime_config.0.properties", "terraform_labels"},
			},
			{
				Config: testAccDataprocSessionTemplate_updated(context),
				ConfigPlanChecks: resource.ConfigPlanChecks{
					PreApply: []plancheck.PlanCheck{
						plancheck.ExpectResourceAction("google_dataproc_session_template.dataproc_session_templates_jupyter_update", plancheck.ResourceActionUpdate),
					},
				},
			},
			{
				ResourceName:            "google_dataproc_session_template.dataproc_session_templates_jupyter_update",
				ImportState:             true,
				ImportStateVerify:       true,
				ImportStateVerifyIgnore: []string{"labels", "location", "runtime_config.0.properties", "terraform_labels"},
			},
		},
	})
}

func testAccDataprocSessionTemplate_preupdate(context map[string]interface{}) string {
	return acctest.Nprintf(`
resource "google_dataproc_session_template" "dataproc_session_templates_jupyter_update" {
    name     = "projects/%{project_name}/locations/us-central1/sessionTemplates/tf-test-jupyter-session-template%{random_suffix}"
    location = "us-central1"
    labels   = {"session_template_test": "terraform"}

    runtime_config {
      properties    = { "spark.dynamicAllocation.enabled": "false", "spark.executor.instances": "2" }
    }

    environment_config {
      execution_config {
        subnetwork_uri = "%{subnetwork_name}"
        ttl            = "3600s"
        network_tags   = ["tag1"]
      }
    }

    jupyter_session {
      kernel       = "PYTHON"
      display_name = "tf python kernel"
    }
}
`, context)
}

func testAccDataprocSessionTemplate_updated(context map[string]interface{}) string {
	return acctest.Nprintf(`
data "google_project" "project" {
}

data "google_storage_project_service_account" "gcs_account" {
}

resource "google_dataproc_session_template" "dataproc_session_templates_jupyter_update" {
    name     = "projects/%{project_name}/locations/us-central1/sessionTemplates/tf-test-jupyter-session-template%{random_suffix}"
    location      = "us-central1"
    labels        = {"session_template_test": "terraform"}

    runtime_config {
      properties    = { "spark.dynamicAllocation.enabled": "false", "spark.executor.instances": "2" }
      version = "2.2"
    }

    environment_config {
      execution_config {
        ttl = "4800s"
        network_tags = ["tag2"]
        kms_key = "%{kms_key_name}"
	subnetwork_uri = "%{subnetwork_name}"
        service_account = "${data.google_project.project.number}-compute@developer.gserviceaccount.com"
        staging_bucket = google_storage_bucket.bucket.name
      }
      peripherals_config {
        metastore_service = google_dataproc_metastore_service.ms.name
        spark_history_server_config {
          dataproc_cluster = google_dataproc_cluster.basic.id
        }
      }
    }

    jupyter_session {
      kernel       = "SCALA"
      display_name = "tf scala kernel"
    }
}

resource "google_storage_bucket" "bucket" {
  uniform_bucket_level_access = true
  name                        = "tf-test-dataproc-bucket%{random_suffix}"
  location                    = "US"
  force_destroy               = true
}

resource "google_dataproc_cluster" "basic" {
  name   = "tf-test-jupyter-session-template%{random_suffix}"
  region = "us-central1"

  cluster_config {
    # Keep the costs down with smallest config we can get away with
    software_config {
      override_properties = {
        "dataproc:dataproc.allow.zero.workers" = "true"
        "spark:spark.history.fs.logDirectory"  = "gs://${google_storage_bucket.bucket.name}/*/spark-job-history"
      }
    }

    gce_cluster_config {
      subnetwork = "%{subnetwork_name}"
    }

    endpoint_config {
      enable_http_port_access = true
    }

    master_config {
      num_instances = 1
      machine_type  = "e2-standard-2"
      disk_config {
        boot_disk_size_gb = 35
      }
    }

    metastore_config {
      dataproc_metastore_service = google_dataproc_metastore_service.ms.name
    }
  }
}

resource "google_dataproc_metastore_service" "ms" {
  service_id = "tf-test-jupyter-session-template%{random_suffix}"
  location   = "us-central1"
  port       = 9080
  tier       = "DEVELOPER"

  maintenance_window {
    hour_of_day = 2
    day_of_week = "SUNDAY"
  }

  hive_metastore_config {
    version = "3.1.2"
  }

  network_config {
    consumers {
      subnetwork = "projects/%{project_name}/regions/us-central1/subnetworks/%{subnetwork_name}"
    }
  }
}
`, context)
}
