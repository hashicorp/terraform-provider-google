// Copyright (c) HashiCorp, Inc.
// SPDX-License-Identifier: MPL-2.0

// ----------------------------------------------------------------------------
//
//     ***     AUTO GENERATED CODE    ***    Type: MMv1     ***
//
// ----------------------------------------------------------------------------
//
//     This file is automatically generated by Magic Modules and manual
//     changes will be clobbered when the file is regenerated.
//
//     Please read more about how to change this file in
//     .github/CONTRIBUTING.md.
//
// ----------------------------------------------------------------------------

package dataproc_test

import (
	"fmt"
	"log"
	"strconv"
	"strings"
	"testing"
	"time"

	"github.com/hashicorp/terraform-plugin-testing/helper/resource"
	"github.com/hashicorp/terraform-plugin-testing/terraform"

	"github.com/hashicorp/terraform-provider-google/google/acctest"
	"github.com/hashicorp/terraform-provider-google/google/envvar"
	"github.com/hashicorp/terraform-provider-google/google/tpgresource"
	transport_tpg "github.com/hashicorp/terraform-provider-google/google/transport"

	"google.golang.org/api/googleapi"
)

var (
	_ = fmt.Sprintf
	_ = log.Print
	_ = strconv.Atoi
	_ = strings.Trim
	_ = time.Now
	_ = resource.TestMain
	_ = terraform.NewState
	_ = envvar.TestEnvVar
	_ = tpgresource.SetLabels
	_ = transport_tpg.Config{}
	_ = googleapi.Error{}
)

func TestAccDataprocSessionTemplate_dataprocSessionTemplatesJupyterExample(t *testing.T) {
	t.Parallel()

	context := map[string]interface{}{
		"project_name":    envvar.GetTestProjectFromEnv(),
		"prevent_destroy": false,
		"subnetwork_name": acctest.BootstrapSubnetWithFirewallForDataprocBatches(t, "jupyer-session-test-network", "jupyter-session-test-subnetwork"),
		"random_suffix":   acctest.RandString(t, 10),
	}

	acctest.VcrTest(t, resource.TestCase{
		PreCheck:                 func() { acctest.AccTestPreCheck(t) },
		ProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories(t),
		CheckDestroy:             testAccCheckDataprocSessionTemplateDestroyProducer(t),
		Steps: []resource.TestStep{
			{
				Config: testAccDataprocSessionTemplate_dataprocSessionTemplatesJupyterExample(context),
			},
			{
				ResourceName:            "google_dataproc_session_template.example_session_templates_jupyter",
				ImportState:             true,
				ImportStateVerify:       true,
				ImportStateVerifyIgnore: []string{"labels", "location", "runtime_config.0.properties", "terraform_labels"},
			},
			{
				ResourceName:       "google_dataproc_session_template.example_session_templates_jupyter",
				RefreshState:       true,
				ExpectNonEmptyPlan: true,
				ImportStateKind:    resource.ImportBlockWithResourceIdentity,
			},
		},
	})
}

func testAccDataprocSessionTemplate_dataprocSessionTemplatesJupyterExample(context map[string]interface{}) string {
	return acctest.Nprintf(`
resource "google_dataproc_session_template" "example_session_templates_jupyter" {
    name     = "projects/%{project_name}/locations/us-central1/sessionTemplates/tf-test-jupyter-session-template%{random_suffix}"
    location = "us-central1"
    labels   = {"session_template_test": "terraform"}

    runtime_config {
      properties    = { "spark.dynamicAllocation.enabled": "false", "spark.executor.instances": "2" }
    }

    environment_config {
      execution_config {
        subnetwork_uri = "%{subnetwork_name}"
        idle_ttl       = "3600s"
        network_tags   = ["tag1"]
        authentication_config {
          user_workload_authentication_type = "END_USER_CREDENTIALS"
        }
      }
    }

    jupyter_session {
      kernel       = "PYTHON"
      display_name = "tf python kernel"
    }
}
`, context)
}

func TestAccDataprocSessionTemplate_dataprocSessionTemplatesJupyterFullExample(t *testing.T) {
	t.Parallel()

	context := map[string]interface{}{
		"project_name":    envvar.GetTestProjectFromEnv(),
		"kms_key_name":    acctest.BootstrapKMSKeyWithPurposeInLocationAndName(t, "ENCRYPT_DECRYPT", "us-central1", "tf-bootstrap-dataproc-session-template-key1").CryptoKey.Name,
		"prevent_destroy": false,
		"subnetwork_name": acctest.BootstrapSubnetWithFirewallForDataprocBatches(t, "jupyer-session-test-network", "jupyter-session-test-subnetwork"),
		"random_suffix":   acctest.RandString(t, 10),
	}

	acctest.VcrTest(t, resource.TestCase{
		PreCheck:                 func() { acctest.AccTestPreCheck(t) },
		ProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories(t),
		CheckDestroy:             testAccCheckDataprocSessionTemplateDestroyProducer(t),
		Steps: []resource.TestStep{
			{
				Config: testAccDataprocSessionTemplate_dataprocSessionTemplatesJupyterFullExample(context),
			},
			{
				ResourceName:            "google_dataproc_session_template.dataproc_session_templates_jupyter_full",
				ImportState:             true,
				ImportStateVerify:       true,
				ImportStateVerifyIgnore: []string{"labels", "location", "runtime_config.0.properties", "terraform_labels"},
			},
			{
				ResourceName:       "google_dataproc_session_template.dataproc_session_templates_jupyter_full",
				RefreshState:       true,
				ExpectNonEmptyPlan: true,
				ImportStateKind:    resource.ImportBlockWithResourceIdentity,
			},
		},
	})
}

func testAccDataprocSessionTemplate_dataprocSessionTemplatesJupyterFullExample(context map[string]interface{}) string {
	return acctest.Nprintf(`
data "google_project" "project" {
}

data "google_storage_project_service_account" "gcs_account" {
}

resource "google_dataproc_session_template" "dataproc_session_templates_jupyter_full" {
    name     = "projects/%{project_name}/locations/us-central1/sessionTemplates/tf-test-jupyter-session-template%{random_suffix}"
    location      = "us-central1"
    labels        = {"session_template_test": "terraform"}

    runtime_config {
      properties    = { "spark.dynamicAllocation.enabled": "false", "spark.executor.instances": "2" }
      version = "2.2"
      container_image = "us-docker.pkg.dev/%{project_name}/s8s-spark-test-images/s8s-spark:latest"
    }

    environment_config {
      execution_config {
        ttl = "3600s"
        network_tags = ["tag1"]
        kms_key = "%{kms_key_name}"
	subnetwork_uri = "%{subnetwork_name}"
        service_account = "${data.google_project.project.number}-compute@developer.gserviceaccount.com"
        staging_bucket = google_storage_bucket.bucket.name
        authentication_config {
          user_workload_authentication_type = "SERVICE_ACCOUNT"
        }
      }
      peripherals_config {
        metastore_service = google_dataproc_metastore_service.ms.name
        spark_history_server_config {
          dataproc_cluster = google_dataproc_cluster.basic.id
        }
      }
    }

    jupyter_session {
      kernel       = "PYTHON"
      display_name = "tf python kernel"
    }

    depends_on = [
      google_kms_crypto_key_iam_member.crypto_key_member_1,
    ]
}

resource "google_storage_bucket" "bucket" {
  uniform_bucket_level_access = true
  name                        = "tf-test-dataproc-bucket%{random_suffix}"
  location                    = "US"
  force_destroy               = true
}

resource "google_kms_crypto_key_iam_member" "crypto_key_member_1" {
  crypto_key_id = "%{kms_key_name}"
  role          = "roles/cloudkms.cryptoKeyEncrypterDecrypter"
  member        = "serviceAccount:service-${data.google_project.project.number}@dataproc-accounts.iam.gserviceaccount.com"
}

resource "google_dataproc_cluster" "basic" {
  name   = "tf-test-jupyter-session-template%{random_suffix}"
  region = "us-central1"

  cluster_config {
    # Keep the costs down with smallest config we can get away with
    software_config {
      override_properties = {
        "dataproc:dataproc.allow.zero.workers" = "true"
        "spark:spark.history.fs.logDirectory"  = "gs://${google_storage_bucket.bucket.name}/*/spark-job-history"
      }
    }

    gce_cluster_config {
      subnetwork = "%{subnetwork_name}"
    }

    endpoint_config {
      enable_http_port_access = true
    }

    master_config {
      num_instances = 1
      machine_type  = "e2-standard-2"
      disk_config {
        boot_disk_size_gb = 35
      }
    }

    metastore_config {
      dataproc_metastore_service = google_dataproc_metastore_service.ms.name
    }
  }
}

resource "google_dataproc_metastore_service" "ms" {
  service_id = "tf-test-jupyter-session-template%{random_suffix}"
  location   = "us-central1"
  port       = 9080
  tier       = "DEVELOPER"

  maintenance_window {
    hour_of_day = 2
    day_of_week = "SUNDAY"
  }

  hive_metastore_config {
    version = "3.1.2"
  }

  network_config {
    consumers {
      subnetwork = "projects/%{project_name}/regions/us-central1/subnetworks/%{subnetwork_name}"
    }
  }
}
`, context)
}

func TestAccDataprocSessionTemplate_dataprocSessionTemplatesSparkConnectExample(t *testing.T) {
	t.Parallel()

	context := map[string]interface{}{
		"project_name":    envvar.GetTestProjectFromEnv(),
		"prevent_destroy": false,
		"subnetwork_name": acctest.BootstrapSubnetWithFirewallForDataprocBatches(t, "spark-connect-session-test-network", "spark-connect-session-test-subnetwork"),
		"random_suffix":   acctest.RandString(t, 10),
	}

	acctest.VcrTest(t, resource.TestCase{
		PreCheck:                 func() { acctest.AccTestPreCheck(t) },
		ProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories(t),
		CheckDestroy:             testAccCheckDataprocSessionTemplateDestroyProducer(t),
		Steps: []resource.TestStep{
			{
				Config: testAccDataprocSessionTemplate_dataprocSessionTemplatesSparkConnectExample(context),
			},
			{
				ResourceName:            "google_dataproc_session_template.example_session_templates_spark_connect",
				ImportState:             true,
				ImportStateVerify:       true,
				ImportStateVerifyIgnore: []string{"labels", "location", "runtime_config.0.properties", "terraform_labels"},
			},
			{
				ResourceName:       "google_dataproc_session_template.example_session_templates_spark_connect",
				RefreshState:       true,
				ExpectNonEmptyPlan: true,
				ImportStateKind:    resource.ImportBlockWithResourceIdentity,
			},
		},
	})
}

func testAccDataprocSessionTemplate_dataprocSessionTemplatesSparkConnectExample(context map[string]interface{}) string {
	return acctest.Nprintf(`
resource "google_dataproc_session_template" "example_session_templates_spark_connect" {
    name     = "projects/%{project_name}/locations/us-central1/sessionTemplates/tf-test-sc-session-template%{random_suffix}"
    location      = "us-central1"
    labels        = {"session_template_test": "terraform"}

    runtime_config {
      properties    = { "spark.dynamicAllocation.enabled": "false", "spark.executor.instances": "2" }
    }

    environment_config {
      execution_config {
        subnetwork_uri = "%{subnetwork_name}"
        ttl            = "3600s"
        network_tags   = ["tag1"]
      }
    }
}
`, context)
}

func testAccCheckDataprocSessionTemplateDestroyProducer(t *testing.T) func(s *terraform.State) error {
	return func(s *terraform.State) error {
		for name, rs := range s.RootModule().Resources {
			if rs.Type != "google_dataproc_session_template" {
				continue
			}
			if strings.HasPrefix(name, "data.") {
				continue
			}

			config := acctest.GoogleProviderConfig(t)

			url, err := tpgresource.ReplaceVarsForTest(config, rs, "{{DataprocBasePath}}{{name}}")
			if err != nil {
				return err
			}

			billingProject := ""

			if config.BillingProject != "" {
				billingProject = config.BillingProject
			}

			_, err = transport_tpg.SendRequest(transport_tpg.SendRequestOptions{
				Config:    config,
				Method:    "GET",
				Project:   billingProject,
				RawURL:    url,
				UserAgent: config.UserAgent,
			})
			if err == nil {
				return fmt.Errorf("DataprocSessionTemplate still exists at %s", url)
			}
		}

		return nil
	}
}
