// ----------------------------------------------------------------------------
//
//     ***     AUTO GENERATED CODE    ***    AUTO GENERATED CODE     ***
//
// ----------------------------------------------------------------------------
//
//     This file is automatically generated by Magic Modules and manual
//     changes will be clobbered when the file is regenerated.
//
//     Please read more about how to change this file in
//     .github/CONTRIBUTING.md.
//
// ----------------------------------------------------------------------------

package google

import (
	"fmt"
	"log"
	"reflect"
	"strconv"
	"time"

	"github.com/hashicorp/terraform-plugin-sdk/helper/schema"
	"github.com/hashicorp/terraform-plugin-sdk/helper/validation"
)

func resourceMLEngineJob() *schema.Resource {
	return &schema.Resource{
		Create: resourceMLEngineJobCreate,
		Read:   resourceMLEngineJobRead,
		Delete: resourceMLEngineJobDelete,

		Importer: &schema.ResourceImporter{
			State: resourceMLEngineJobImport,
		},

		Timeouts: &schema.ResourceTimeout{
			Create: schema.DefaultTimeout(4 * time.Minute),
			Delete: schema.DefaultTimeout(4 * time.Minute),
		},

		Schema: map[string]*schema.Schema{
			"job_id": {
				Type:        schema.TypeString,
				Required:    true,
				ForceNew:    true,
				Description: `The user-specified id of the job.`,
			},
			"labels": {
				Type:     schema.TypeMap,
				Optional: true,
				ForceNew: true,
				Description: `One or more labels that you can add, to organize your jobs. Each label is a key-value pair, 
where both the key and the value are arbitrary Strings that you supply.`,
				Elem: &schema.Schema{Type: schema.TypeString},
			},
			"training_input": {
				Type:        schema.TypeList,
				Optional:    true,
				ForceNew:    true,
				Description: `Input parameters to create a training job.`,
				MaxItems:    1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"region": {
							Type:        schema.TypeString,
							Required:    true,
							ForceNew:    true,
							Description: `The Google Compute Engine region to run the training job in. See the available regions for AI Platform services.`,
						},
						"args": {
							Type:        schema.TypeList,
							Optional:    true,
							ForceNew:    true,
							Description: `Command line arguments to pass to the program.`,
							Elem: &schema.Schema{
								Type: schema.TypeString,
							},
						},
						"job_dir": {
							Type:     schema.TypeString,
							Optional: true,
							ForceNew: true,
							Description: `A Google Cloud Storage path in which to store training outputs and other data needed for training. This path is passed 
to your TensorFlow program as the '--job-dir' command-line argument. The benefit of specifying this field is that Cloud 
ML validates the path for use in training.`,
						},
						"master_config": {
							Type:        schema.TypeList,
							Optional:    true,
							ForceNew:    true,
							Description: `The configuration for your master worker.`,
							MaxItems:    1,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"accelerator_config": {
										Type:        schema.TypeList,
										Optional:    true,
										ForceNew:    true,
										Description: `Represents the type and number of accelerators used by the replica`,
										MaxItems:    1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"count": {
													Type:        schema.TypeInt,
													Optional:    true,
													ForceNew:    true,
													Description: `The number of accelerators to attach to each machine running the job.`,
												},
												"type": {
													Type:         schema.TypeString,
													Optional:     true,
													ForceNew:     true,
													ValidateFunc: validation.StringInSlice([]string{"ACCELERATOR_TYPE_UNSPECIFIED", "NVIDIA_TESLA_K80", "NVIDIA_TESLA_P100", "NVIDIA_TESLA_V100", "NVIDIA_TESLA_P4", "NVIDIA_TESLA_T4", "TPU_V2", "TPU_V3", ""}, false),
													Description:  `The type of accelerator to use.`,
												},
											},
										},
									},
									"image_uri": {
										Type:        schema.TypeString,
										Optional:    true,
										ForceNew:    true,
										Description: `The Docker image to run on the replica. This image must be in Container Registry.`,
									},
									"tpu_tf_version": {
										Type:     schema.TypeString,
										Optional: true,
										ForceNew: true,
										Description: `The AI Platform runtime version that includes a TensorFlow version matching the one used in the 
custom container. This field is required if the replica is a TPU worker that uses a custom container. 
Otherwise, do not specify this field.`,
									},
								},
							},
						},
						"master_type": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: `Specifies the type of virtual machine to use for your training job's master worker`,
						},
						"package_uris": {
							Type:     schema.TypeList,
							Optional: true,
							ForceNew: true,
							Description: `The Google Cloud Storage location of the packages with the training program and any additional dependencies.
 The maximum number of package URIs is 100.`,
							Elem: &schema.Schema{
								Type: schema.TypeString,
							},
						},
						"parameter_server_config": {
							Type:        schema.TypeList,
							Optional:    true,
							ForceNew:    true,
							Description: `The configuration for parameter servers.`,
							MaxItems:    1,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"accelerator_config": {
										Type:        schema.TypeList,
										Optional:    true,
										ForceNew:    true,
										Description: `Represents the type and number of accelerators used by the replica`,
										MaxItems:    1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"count": {
													Type:        schema.TypeInt,
													Optional:    true,
													ForceNew:    true,
													Description: `The number of accelerators to attach to each machine running the job.`,
												},
												"type": {
													Type:         schema.TypeString,
													Optional:     true,
													ForceNew:     true,
													ValidateFunc: validation.StringInSlice([]string{"ACCELERATOR_TYPE_UNSPECIFIED", "NVIDIA_TESLA_K80", "NVIDIA_TESLA_P100", "NVIDIA_TESLA_V100", "NVIDIA_TESLA_P4", "NVIDIA_TESLA_T4", "TPU_V2", "TPU_V3", ""}, false),
													Description:  `The type of accelerator to use.`,
												},
											},
										},
									},
									"image_uri": {
										Type:        schema.TypeString,
										Optional:    true,
										ForceNew:    true,
										Description: `The Docker image to run on the replica. This image must be in Container Registry.`,
									},
									"tpu_tf_version": {
										Type:     schema.TypeString,
										Optional: true,
										ForceNew: true,
										Description: `The AI Platform runtime version that includes a TensorFlow version matching the one used in the 
custom container. This field is required if the replica is a TPU worker that uses a custom container. 
Otherwise, do not specify this field.`,
									},
								},
							},
						},
						"parameter_server_count": {
							Type:     schema.TypeInt,
							Optional: true,
							ForceNew: true,
							Description: `The number of parameter server replicas to use for the training job. Each replica in the cluster will be 
of the type specified in parameterServerType.`,
						},
						"parameter_server_type": {
							Type:     schema.TypeString,
							Optional: true,
							ForceNew: true,
							Description: `Specifies the type of virtual machine to use for your training job's parameter server.
The supported values are the same as those described in the entry for masterType. 
This value must be consistent with the category of machine type that masterType uses. 
In other words, both must be Compute Engine machine types or both must be legacy machine types. This
value must be present when scaleTier is set to CUSTOM and parameterServerCount is greater than zero.`,
						},
						"python_module": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: `The Python module name to run after installing the packages.`,
						},
						"python_version": {
							Type:     schema.TypeString,
							Optional: true,
							ForceNew: true,
							Description: `The version of Python used in training. If not set, the default version is '2.7'. Python '3.5' is available when 
runtimeVersion is set to '1.4' and above. Python '2.7' works with all supported runtime versions.`,
						},
						"runtime_version": {
							Type:     schema.TypeString,
							Optional: true,
							ForceNew: true,
							Description: `The AI Platform runtime version to use for training. If not set, AI Platform uses the default stable version, 1.0. For more 
information, see the runtime version list and how to manage runtime versions.`,
						},
						"scale_tier": {
							Type:         schema.TypeString,
							Optional:     true,
							ForceNew:     true,
							ValidateFunc: validation.StringInSlice([]string{"BASIC", "STANDARD_1", "PREMIUM_1", "BASIC_GPU", "BASIC_TPU", "CUSTOM", ""}, false),
							Description: `Specifies the machine types, the number of replicas for workers and parameter servers. 
https://cloud.google.com/ml-engine/reference/rest/v1/projects.jobs#ScaleTier`,
						},
						"worker_config": {
							Type:        schema.TypeList,
							Optional:    true,
							ForceNew:    true,
							Description: `The configuration for workers.`,
							MaxItems:    1,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"accelerator_config": {
										Type:        schema.TypeList,
										Optional:    true,
										ForceNew:    true,
										Description: `Represents the type and number of accelerators used by the replica`,
										MaxItems:    1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"count": {
													Type:        schema.TypeInt,
													Optional:    true,
													ForceNew:    true,
													Description: `The number of accelerators to attach to each machine running the job.`,
												},
												"type": {
													Type:         schema.TypeString,
													Optional:     true,
													ForceNew:     true,
													ValidateFunc: validation.StringInSlice([]string{"ACCELERATOR_TYPE_UNSPECIFIED", "NVIDIA_TESLA_K80", "NVIDIA_TESLA_P100", "NVIDIA_TESLA_V100", "NVIDIA_TESLA_P4", "NVIDIA_TESLA_T4", "TPU_V2", "TPU_V3", ""}, false),
													Description:  `The type of accelerator to use.`,
												},
											},
										},
									},
									"image_uri": {
										Type:        schema.TypeString,
										Optional:    true,
										ForceNew:    true,
										Description: `The Docker image to run on the replica. This image must be in Container Registry.`,
									},
									"tpu_tf_version": {
										Type:     schema.TypeString,
										Optional: true,
										ForceNew: true,
										Description: `The AI Platform runtime version that includes a TensorFlow version matching the one used in the 
custom container. This field is required if the replica is a TPU worker that uses a custom container. 
Otherwise, do not specify this field.`,
									},
								},
							},
						},
						"worker_count": {
							Type:     schema.TypeInt,
							Optional: true,
							ForceNew: true,
							Description: `The number of worker replicas to use for the training job. Each replica in the cluster will be of the type 
specified in workerType`,
						},
						"worker_type": {
							Type:     schema.TypeString,
							Optional: true,
							ForceNew: true,
							Description: `Specifies the type of virtual machine to use for your training job's worker nodes.
The supported values are the same as those described in the entry for masterType.
This value must be consistent with the category of machine type that masterType uses. In other words, 
both must be Compute Engine machine types or both must be legacy machine types.
If you use cloud_tpu for this value, see special instructions for configuring a custom TPU machine.
This value must be present when scaleTier is set to CUSTOM and workerCount is greater than zero.`,
						},
					},
				},
			},
			"training_output": {
				Type:        schema.TypeList,
				Optional:    true,
				ForceNew:    true,
				Description: `The current training job result.`,
				MaxItems:    1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"built_in_algorithm_output": {
							Type:        schema.TypeList,
							Optional:    true,
							ForceNew:    true,
							Description: `Details related to built-in algorithms jobs. Only set for built-in algorithms jobs.`,
							MaxItems:    1,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"framework": {
										Type:        schema.TypeString,
										Optional:    true,
										ForceNew:    true,
										Description: `Framework on which the built-in algorithm was trained.`,
									},
									"model_path": {
										Type:     schema.TypeInt,
										Optional: true,
										ForceNew: true,
										Description: `The Cloud Storage path to the model/ directory where the training job saves the trained model. 
Only set for successful jobs that don't use hyperparameter tuning`,
									},
									"python_version": {
										Type:        schema.TypeString,
										Optional:    true,
										ForceNew:    true,
										Description: `Python version on which the built-in algorithm was trained.`,
									},
									"runtime_version": {
										Type:        schema.TypeString,
										Optional:    true,
										ForceNew:    true,
										Description: `AI Platform runtime version on which the built-in algorithm was trained`,
									},
								},
							},
						},
						"consumed_ml_units": {
							Type:        schema.TypeFloat,
							Optional:    true,
							ForceNew:    true,
							Description: `The amount of ML units consumed by the job.`,
						},
						"hyperparameter_metric_tag": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: `The TensorFlow summary tag name used for optimizing hyperparameter tuning trials.`,
						},
						"is_built_in_algorithm_job": {
							Type:        schema.TypeBool,
							Optional:    true,
							ForceNew:    true,
							Description: `Whether this job is a built-in Algorithm job.`,
						},
						"is_hyperparameter_tuning_job": {
							Type:        schema.TypeBool,
							Optional:    true,
							ForceNew:    true,
							Description: `Whether this job is a hyperparameter tuning job.`,
						},
						"trials": {
							Type:        schema.TypeList,
							Optional:    true,
							ForceNew:    true,
							Description: `Results for individual Hyperparameter trials. Only set for hyperparameter tuning jobs.`,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"built_in_algorithm_output": {
										Type:        schema.TypeList,
										Optional:    true,
										ForceNew:    true,
										Description: `Details related to built-in algorithms jobs. Only set for trials of built-in algorithms jobs that have succeeded.`,
										MaxItems:    1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"framework": {
													Type:        schema.TypeString,
													Computed:    true,
													Description: `Framework on which the built-in algorithm was trained.`,
												},
												"model_path": {
													Type:     schema.TypeString,
													Computed: true,
													Description: `The Cloud Storage path to the model/ directory where the training job saves the trained model. 
Only set for successful jobs that don't use hyperparameter tuning`,
												},
												"python_version": {
													Type:        schema.TypeString,
													Computed:    true,
													Description: `Python version on which the built-in algorithm was trained.`,
												},
												"runtime_vresion": {
													Type:        schema.TypeString,
													Computed:    true,
													Description: `AI Platform runtime version on which the built-in algorithm was trained`,
												},
											},
										},
									},
									"final_metric": {
										Type:        schema.TypeList,
										Optional:    true,
										ForceNew:    true,
										Description: `The final objective metric seen for this trial.`,
										MaxItems:    1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"objective_value": {
													Type:        schema.TypeInt,
													Computed:    true,
													Description: `The objective value at this training step.`,
												},
												"training_step": {
													Type:        schema.TypeString,
													Computed:    true,
													Description: `The global training step for this metric`,
												},
											},
										},
									},
									"end_time": {
										Type:        schema.TypeString,
										Computed:    true,
										Description: `End time for the trial.`,
									},
									"hyperparameters": {
										Type:     schema.TypeMap,
										Computed: true,
										Description: `The hyperparameters given to this trial. An object containing a list of "key": value pairs. 
Example: { "name": "wrench", "mass": "1.3kg", "count": "3" }`,
										Elem: &schema.Schema{Type: schema.TypeString},
									},
									"is_trial_stopped_early": {
										Type:        schema.TypeBool,
										Computed:    true,
										Description: `True if the trial is stopped early.`,
									},
									"start_time": {
										Type:        schema.TypeString,
										Computed:    true,
										Description: `Start time for the trial.`,
									},
									"state": {
										Type:        schema.TypeString,
										Computed:    true,
										Description: `The detailed state of a job.`,
									},
									"trial_id": {
										Type:        schema.TypeString,
										Computed:    true,
										Description: `The trial id for these results.`,
									},
								},
							},
						},
						"completed_trial_count": {
							Type:        schema.TypeString,
							Computed:    true,
							Description: `The number of hyperparameter tuning trials that completed successfully. Only set for hyperparameter tuning jobs.`,
						},
					},
				},
			},
			"create_time": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `When the job was created.`,
			},
			"end_time": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `When the job processing was completed.`,
			},
			"error_message": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `The details of a failure or a cancellation.`,
			},
			"state": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `The detailed state of a job.`,
			},
			"project": {
				Type:     schema.TypeString,
				Optional: true,
				Computed: true,
				ForceNew: true,
			},
		},
	}
}

func resourceMLEngineJobCreate(d *schema.ResourceData, meta interface{}) error {
	config := meta.(*Config)

	obj := make(map[string]interface{})
	jobIdProp, err := expandMLEngineJobJobId(d.Get("job_id"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("job_id"); !isEmptyValue(reflect.ValueOf(jobIdProp)) && (ok || !reflect.DeepEqual(v, jobIdProp)) {
		obj["jobId"] = jobIdProp
	}
	labelsProp, err := expandMLEngineJobLabels(d.Get("labels"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("labels"); !isEmptyValue(reflect.ValueOf(labelsProp)) && (ok || !reflect.DeepEqual(v, labelsProp)) {
		obj["labels"] = labelsProp
	}
	trainingInputProp, err := expandMLEngineJobTrainingInput(d.Get("training_input"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("training_input"); !isEmptyValue(reflect.ValueOf(trainingInputProp)) && (ok || !reflect.DeepEqual(v, trainingInputProp)) {
		obj["trainingInput"] = trainingInputProp
	}
	trainingOutputProp, err := expandMLEngineJobTrainingOutput(d.Get("training_output"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("training_output"); !isEmptyValue(reflect.ValueOf(trainingOutputProp)) && (ok || !reflect.DeepEqual(v, trainingOutputProp)) {
		obj["trainingOutput"] = trainingOutputProp
	}

	url, err := replaceVars(d, config, "{{MLEngineBasePath}}projects/{{project}}/jobs")
	if err != nil {
		return err
	}

	log.Printf("[DEBUG] Creating new Job: %#v", obj)
	project, err := getProject(d, config)
	if err != nil {
		return err
	}
	res, err := sendRequestWithTimeout(config, "POST", project, url, obj, d.Timeout(schema.TimeoutCreate))
	if err != nil {
		return fmt.Errorf("Error creating Job: %s", err)
	}

	// Store the ID now
	id, err := replaceVars(d, config, "projects/{{project}}/jobs/{{name}}")
	if err != nil {
		return fmt.Errorf("Error constructing id: %s", err)
	}
	d.SetId(id)

	log.Printf("[DEBUG] Finished creating Job %q: %#v", d.Id(), res)

	return resourceMLEngineJobRead(d, meta)
}

func resourceMLEngineJobRead(d *schema.ResourceData, meta interface{}) error {
	config := meta.(*Config)

	url, err := replaceVars(d, config, "{{MLEngineBasePath}}projects/{{project}}/jobs/{{name}}")
	if err != nil {
		return err
	}

	project, err := getProject(d, config)
	if err != nil {
		return err
	}
	res, err := sendRequest(config, "GET", project, url, nil)
	if err != nil {
		return handleNotFoundError(err, d, fmt.Sprintf("MLEngineJob %q", d.Id()))
	}

	if err := d.Set("project", project); err != nil {
		return fmt.Errorf("Error reading Job: %s", err)
	}

	if err := d.Set("job_id", flattenMLEngineJobJobId(res["jobId"], d)); err != nil {
		return fmt.Errorf("Error reading Job: %s", err)
	}
	if err := d.Set("create_time", flattenMLEngineJobCreateTime(res["createTime"], d)); err != nil {
		return fmt.Errorf("Error reading Job: %s", err)
	}
	if err := d.Set("end_time", flattenMLEngineJobEndTime(res["endTime"], d)); err != nil {
		return fmt.Errorf("Error reading Job: %s", err)
	}
	if err := d.Set("state", flattenMLEngineJobState(res["state"], d)); err != nil {
		return fmt.Errorf("Error reading Job: %s", err)
	}
	if err := d.Set("error_message", flattenMLEngineJobErrorMessage(res["errorMessage"], d)); err != nil {
		return fmt.Errorf("Error reading Job: %s", err)
	}
	if err := d.Set("labels", flattenMLEngineJobLabels(res["labels"], d)); err != nil {
		return fmt.Errorf("Error reading Job: %s", err)
	}
	if err := d.Set("training_input", flattenMLEngineJobTrainingInput(res["trainingInput"], d)); err != nil {
		return fmt.Errorf("Error reading Job: %s", err)
	}
	if err := d.Set("training_output", flattenMLEngineJobTrainingOutput(res["trainingOutput"], d)); err != nil {
		return fmt.Errorf("Error reading Job: %s", err)
	}

	return nil
}

func resourceMLEngineJobDelete(d *schema.ResourceData, meta interface{}) error {
	log.Printf("[WARNING] MLEngine Job resources"+
		" cannot be deleted from GCP. The resource %s will be removed from Terraform"+
		" state, but will still be present on the server.", d.Id())
	d.SetId("")

	return nil
}

func resourceMLEngineJobImport(d *schema.ResourceData, meta interface{}) ([]*schema.ResourceData, error) {
	config := meta.(*Config)
	if err := parseImportId([]string{
		"projects/(?P<project>[^/]+)/jobs/(?P<name>[^/]+)",
		"(?P<project>[^/]+)/(?P<name>[^/]+)",
		"(?P<name>[^/]+)",
	}, d, config); err != nil {
		return nil, err
	}

	// Replace import id for the resource id
	id, err := replaceVars(d, config, "projects/{{project}}/jobs/{{name}}")
	if err != nil {
		return nil, fmt.Errorf("Error constructing id: %s", err)
	}
	d.SetId(id)

	return []*schema.ResourceData{d}, nil
}

func flattenMLEngineJobJobId(v interface{}, d *schema.ResourceData) interface{} {
	return v
}

func flattenMLEngineJobCreateTime(v interface{}, d *schema.ResourceData) interface{} {
	return v
}

func flattenMLEngineJobEndTime(v interface{}, d *schema.ResourceData) interface{} {
	return v
}

func flattenMLEngineJobState(v interface{}, d *schema.ResourceData) interface{} {
	return v
}

func flattenMLEngineJobErrorMessage(v interface{}, d *schema.ResourceData) interface{} {
	return v
}

func flattenMLEngineJobLabels(v interface{}, d *schema.ResourceData) interface{} {
	return v
}

func flattenMLEngineJobTrainingInput(v interface{}, d *schema.ResourceData) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["scale_tier"] =
		flattenMLEngineJobTrainingInputScaleTier(original["scaleTier"], d)
	transformed["master_type"] =
		flattenMLEngineJobTrainingInputMasterType(original["masterType"], d)
	transformed["master_config"] =
		flattenMLEngineJobTrainingInputMasterConfig(original["masterConfig"], d)
	transformed["worker_type"] =
		flattenMLEngineJobTrainingInputWorkerType(original["workerType"], d)
	transformed["worker_config"] =
		flattenMLEngineJobTrainingInputWorkerConfig(original["workerConfig"], d)
	transformed["parameter_server_type"] =
		flattenMLEngineJobTrainingInputParameterServerType(original["parameterServerType"], d)
	transformed["parameter_server_config"] =
		flattenMLEngineJobTrainingInputParameterServerConfig(original["parameterServerConfig"], d)
	transformed["worker_count"] =
		flattenMLEngineJobTrainingInputWorkerCount(original["workerCount"], d)
	transformed["parameter_server_count"] =
		flattenMLEngineJobTrainingInputParameterServerCount(original["parameterServerCount"], d)
	transformed["package_uris"] =
		flattenMLEngineJobTrainingInputPackageUris(original["packageUris"], d)
	transformed["python_module"] =
		flattenMLEngineJobTrainingInputPythonModule(original["pythonModule"], d)
	transformed["args"] =
		flattenMLEngineJobTrainingInputArgs(original["args"], d)
	transformed["region"] =
		flattenMLEngineJobTrainingInputRegion(original["region"], d)
	transformed["job_dir"] =
		flattenMLEngineJobTrainingInputJobDir(original["jobDir"], d)
	transformed["runtime_version"] =
		flattenMLEngineJobTrainingInputRuntimeVersion(original["runtimeVersion"], d)
	transformed["python_version"] =
		flattenMLEngineJobTrainingInputPythonVersion(original["pythonVersion"], d)
	return []interface{}{transformed}
}
func flattenMLEngineJobTrainingInputScaleTier(v interface{}, d *schema.ResourceData) interface{} {
	return v
}

func flattenMLEngineJobTrainingInputMasterType(v interface{}, d *schema.ResourceData) interface{} {
	return v
}

func flattenMLEngineJobTrainingInputMasterConfig(v interface{}, d *schema.ResourceData) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["accelerator_config"] =
		flattenMLEngineJobTrainingInputMasterConfigAcceleratorConfig(original["acceleratorConfig"], d)
	transformed["image_uri"] =
		flattenMLEngineJobTrainingInputMasterConfigImageUri(original["imageUri"], d)
	transformed["tpu_tf_version"] =
		flattenMLEngineJobTrainingInputMasterConfigTPUTfVersion(original["tpuTfVersion"], d)
	return []interface{}{transformed}
}
func flattenMLEngineJobTrainingInputMasterConfigAcceleratorConfig(v interface{}, d *schema.ResourceData) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["count"] =
		flattenMLEngineJobTrainingInputMasterConfigAcceleratorConfigCount(original["count"], d)
	transformed["type"] =
		flattenMLEngineJobTrainingInputMasterConfigAcceleratorConfigType(original["type"], d)
	return []interface{}{transformed}
}
func flattenMLEngineJobTrainingInputMasterConfigAcceleratorConfigCount(v interface{}, d *schema.ResourceData) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := strconv.ParseInt(strVal, 10, 64); err == nil {
			return intVal
		} // let terraform core handle it if we can't convert the string to an int.
	}
	return v
}

func flattenMLEngineJobTrainingInputMasterConfigAcceleratorConfigType(v interface{}, d *schema.ResourceData) interface{} {
	return v
}

func flattenMLEngineJobTrainingInputMasterConfigImageUri(v interface{}, d *schema.ResourceData) interface{} {
	return v
}

func flattenMLEngineJobTrainingInputMasterConfigTPUTfVersion(v interface{}, d *schema.ResourceData) interface{} {
	return v
}

func flattenMLEngineJobTrainingInputWorkerType(v interface{}, d *schema.ResourceData) interface{} {
	return v
}

func flattenMLEngineJobTrainingInputWorkerConfig(v interface{}, d *schema.ResourceData) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["accelerator_config"] =
		flattenMLEngineJobTrainingInputWorkerConfigAcceleratorConfig(original["acceleratorConfig"], d)
	transformed["image_uri"] =
		flattenMLEngineJobTrainingInputWorkerConfigImageUri(original["imageUri"], d)
	transformed["tpu_tf_version"] =
		flattenMLEngineJobTrainingInputWorkerConfigTPUTfVersion(original["tpuTfVersion"], d)
	return []interface{}{transformed}
}
func flattenMLEngineJobTrainingInputWorkerConfigAcceleratorConfig(v interface{}, d *schema.ResourceData) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["count"] =
		flattenMLEngineJobTrainingInputWorkerConfigAcceleratorConfigCount(original["count"], d)
	transformed["type"] =
		flattenMLEngineJobTrainingInputWorkerConfigAcceleratorConfigType(original["type"], d)
	return []interface{}{transformed}
}
func flattenMLEngineJobTrainingInputWorkerConfigAcceleratorConfigCount(v interface{}, d *schema.ResourceData) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := strconv.ParseInt(strVal, 10, 64); err == nil {
			return intVal
		} // let terraform core handle it if we can't convert the string to an int.
	}
	return v
}

func flattenMLEngineJobTrainingInputWorkerConfigAcceleratorConfigType(v interface{}, d *schema.ResourceData) interface{} {
	return v
}

func flattenMLEngineJobTrainingInputWorkerConfigImageUri(v interface{}, d *schema.ResourceData) interface{} {
	return v
}

func flattenMLEngineJobTrainingInputWorkerConfigTPUTfVersion(v interface{}, d *schema.ResourceData) interface{} {
	return v
}

func flattenMLEngineJobTrainingInputParameterServerType(v interface{}, d *schema.ResourceData) interface{} {
	return v
}

func flattenMLEngineJobTrainingInputParameterServerConfig(v interface{}, d *schema.ResourceData) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["accelerator_config"] =
		flattenMLEngineJobTrainingInputParameterServerConfigAcceleratorConfig(original["acceleratorConfig"], d)
	transformed["image_uri"] =
		flattenMLEngineJobTrainingInputParameterServerConfigImageUri(original["imageUri"], d)
	transformed["tpu_tf_version"] =
		flattenMLEngineJobTrainingInputParameterServerConfigTPUTfVersion(original["tpuTfVersion"], d)
	return []interface{}{transformed}
}
func flattenMLEngineJobTrainingInputParameterServerConfigAcceleratorConfig(v interface{}, d *schema.ResourceData) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["count"] =
		flattenMLEngineJobTrainingInputParameterServerConfigAcceleratorConfigCount(original["count"], d)
	transformed["type"] =
		flattenMLEngineJobTrainingInputParameterServerConfigAcceleratorConfigType(original["type"], d)
	return []interface{}{transformed}
}
func flattenMLEngineJobTrainingInputParameterServerConfigAcceleratorConfigCount(v interface{}, d *schema.ResourceData) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := strconv.ParseInt(strVal, 10, 64); err == nil {
			return intVal
		} // let terraform core handle it if we can't convert the string to an int.
	}
	return v
}

func flattenMLEngineJobTrainingInputParameterServerConfigAcceleratorConfigType(v interface{}, d *schema.ResourceData) interface{} {
	return v
}

func flattenMLEngineJobTrainingInputParameterServerConfigImageUri(v interface{}, d *schema.ResourceData) interface{} {
	return v
}

func flattenMLEngineJobTrainingInputParameterServerConfigTPUTfVersion(v interface{}, d *schema.ResourceData) interface{} {
	return v
}

func flattenMLEngineJobTrainingInputWorkerCount(v interface{}, d *schema.ResourceData) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := strconv.ParseInt(strVal, 10, 64); err == nil {
			return intVal
		} // let terraform core handle it if we can't convert the string to an int.
	}
	return v
}

func flattenMLEngineJobTrainingInputParameterServerCount(v interface{}, d *schema.ResourceData) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := strconv.ParseInt(strVal, 10, 64); err == nil {
			return intVal
		} // let terraform core handle it if we can't convert the string to an int.
	}
	return v
}

func flattenMLEngineJobTrainingInputPackageUris(v interface{}, d *schema.ResourceData) interface{} {
	return v
}

func flattenMLEngineJobTrainingInputPythonModule(v interface{}, d *schema.ResourceData) interface{} {
	return v
}

func flattenMLEngineJobTrainingInputArgs(v interface{}, d *schema.ResourceData) interface{} {
	return v
}

func flattenMLEngineJobTrainingInputRegion(v interface{}, d *schema.ResourceData) interface{} {
	return v
}

func flattenMLEngineJobTrainingInputJobDir(v interface{}, d *schema.ResourceData) interface{} {
	return v
}

func flattenMLEngineJobTrainingInputRuntimeVersion(v interface{}, d *schema.ResourceData) interface{} {
	return v
}

func flattenMLEngineJobTrainingInputPythonVersion(v interface{}, d *schema.ResourceData) interface{} {
	return v
}

func flattenMLEngineJobTrainingOutput(v interface{}, d *schema.ResourceData) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["completed_trial_count"] =
		flattenMLEngineJobTrainingOutputCompletedTrialCount(original["completedTrialCount"], d)
	transformed["trials"] =
		flattenMLEngineJobTrainingOutputTrials(original["trials"], d)
	transformed["consumed_ml_units"] =
		flattenMLEngineJobTrainingOutputConsumedMLUnits(original["consumedMLUnits"], d)
	transformed["is_hyperparameter_tuning_job"] =
		flattenMLEngineJobTrainingOutputIsHyperparameterTuningJob(original["isHyperparameterTuningJob"], d)
	transformed["is_built_in_algorithm_job"] =
		flattenMLEngineJobTrainingOutputIsBuiltInAlgorithmJob(original["isBuiltInAlgorithmJob"], d)
	transformed["built_in_algorithm_output"] =
		flattenMLEngineJobTrainingOutputBuiltInAlgorithmOutput(original["builtInAlgorithmOutput"], d)
	transformed["hyperparameter_metric_tag"] =
		flattenMLEngineJobTrainingOutputHyperparameterMetricTag(original["hyperparameterMetricTag"], d)
	return []interface{}{transformed}
}
func flattenMLEngineJobTrainingOutputCompletedTrialCount(v interface{}, d *schema.ResourceData) interface{} {
	return v
}

func flattenMLEngineJobTrainingOutputTrials(v interface{}, d *schema.ResourceData) interface{} {
	if v == nil {
		return v
	}
	l := v.([]interface{})
	transformed := make([]interface{}, 0, len(l))
	for _, raw := range l {
		original := raw.(map[string]interface{})
		if len(original) < 1 {
			// Do not include empty json objects coming back from the api
			continue
		}
		transformed = append(transformed, map[string]interface{}{
			"trial_id":                  flattenMLEngineJobTrainingOutputTrialsTrialId(original["trialId"], d),
			"hyperparameters":           flattenMLEngineJobTrainingOutputTrialsHyperparameters(original["hyperparameters"], d),
			"start_time":                flattenMLEngineJobTrainingOutputTrialsStartTime(original["startTime"], d),
			"end_time":                  flattenMLEngineJobTrainingOutputTrialsEndTime(original["endTime"], d),
			"state":                     flattenMLEngineJobTrainingOutputTrialsState(original["state"], d),
			"final_metric":              flattenMLEngineJobTrainingOutputTrialsFinalMetric(original["finalMetric"], d),
			"is_trial_stopped_early":    flattenMLEngineJobTrainingOutputTrialsIsTrialStoppedEarly(original["isTrialStoppedEarly"], d),
			"built_in_algorithm_output": flattenMLEngineJobTrainingOutputTrialsBuiltInAlgorithmOutput(original["builtInAlgorithmOutput"], d),
		})
	}
	return transformed
}
func flattenMLEngineJobTrainingOutputTrialsTrialId(v interface{}, d *schema.ResourceData) interface{} {
	return v
}

func flattenMLEngineJobTrainingOutputTrialsHyperparameters(v interface{}, d *schema.ResourceData) interface{} {
	return v
}

func flattenMLEngineJobTrainingOutputTrialsStartTime(v interface{}, d *schema.ResourceData) interface{} {
	return v
}

func flattenMLEngineJobTrainingOutputTrialsEndTime(v interface{}, d *schema.ResourceData) interface{} {
	return v
}

func flattenMLEngineJobTrainingOutputTrialsState(v interface{}, d *schema.ResourceData) interface{} {
	return v
}

func flattenMLEngineJobTrainingOutputTrialsFinalMetric(v interface{}, d *schema.ResourceData) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["training_step"] =
		flattenMLEngineJobTrainingOutputTrialsFinalMetricTrainingStep(original["trainingStep"], d)
	transformed["objective_value"] =
		flattenMLEngineJobTrainingOutputTrialsFinalMetricObjectiveValue(original["objectiveValue"], d)
	return []interface{}{transformed}
}
func flattenMLEngineJobTrainingOutputTrialsFinalMetricTrainingStep(v interface{}, d *schema.ResourceData) interface{} {
	return v
}

func flattenMLEngineJobTrainingOutputTrialsFinalMetricObjectiveValue(v interface{}, d *schema.ResourceData) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := strconv.ParseInt(strVal, 10, 64); err == nil {
			return intVal
		} // let terraform core handle it if we can't convert the string to an int.
	}
	return v
}

func flattenMLEngineJobTrainingOutputTrialsIsTrialStoppedEarly(v interface{}, d *schema.ResourceData) interface{} {
	return v
}

func flattenMLEngineJobTrainingOutputTrialsBuiltInAlgorithmOutput(v interface{}, d *schema.ResourceData) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["framework"] =
		flattenMLEngineJobTrainingOutputTrialsBuiltInAlgorithmOutputFramework(original["framework"], d)
	transformed["runtime_vresion"] =
		flattenMLEngineJobTrainingOutputTrialsBuiltInAlgorithmOutputRuntimeVresion(original["runtimeVresion"], d)
	transformed["python_version"] =
		flattenMLEngineJobTrainingOutputTrialsBuiltInAlgorithmOutputPythonVersion(original["pythonVersion"], d)
	transformed["model_path"] =
		flattenMLEngineJobTrainingOutputTrialsBuiltInAlgorithmOutputModelPath(original["modelPath"], d)
	return []interface{}{transformed}
}
func flattenMLEngineJobTrainingOutputTrialsBuiltInAlgorithmOutputFramework(v interface{}, d *schema.ResourceData) interface{} {
	return v
}

func flattenMLEngineJobTrainingOutputTrialsBuiltInAlgorithmOutputRuntimeVresion(v interface{}, d *schema.ResourceData) interface{} {
	return v
}

func flattenMLEngineJobTrainingOutputTrialsBuiltInAlgorithmOutputPythonVersion(v interface{}, d *schema.ResourceData) interface{} {
	return v
}

func flattenMLEngineJobTrainingOutputTrialsBuiltInAlgorithmOutputModelPath(v interface{}, d *schema.ResourceData) interface{} {
	return v
}

func flattenMLEngineJobTrainingOutputConsumedMLUnits(v interface{}, d *schema.ResourceData) interface{} {
	return v
}

func flattenMLEngineJobTrainingOutputIsHyperparameterTuningJob(v interface{}, d *schema.ResourceData) interface{} {
	return v
}

func flattenMLEngineJobTrainingOutputIsBuiltInAlgorithmJob(v interface{}, d *schema.ResourceData) interface{} {
	return v
}

func flattenMLEngineJobTrainingOutputBuiltInAlgorithmOutput(v interface{}, d *schema.ResourceData) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["framework"] =
		flattenMLEngineJobTrainingOutputBuiltInAlgorithmOutputFramework(original["framework"], d)
	transformed["runtime_version"] =
		flattenMLEngineJobTrainingOutputBuiltInAlgorithmOutputRuntimeVersion(original["runtimeVersion"], d)
	transformed["python_version"] =
		flattenMLEngineJobTrainingOutputBuiltInAlgorithmOutputPythonVersion(original["pythonVersion"], d)
	transformed["model_path"] =
		flattenMLEngineJobTrainingOutputBuiltInAlgorithmOutputModelPath(original["modelPath"], d)
	return []interface{}{transformed}
}
func flattenMLEngineJobTrainingOutputBuiltInAlgorithmOutputFramework(v interface{}, d *schema.ResourceData) interface{} {
	return v
}

func flattenMLEngineJobTrainingOutputBuiltInAlgorithmOutputRuntimeVersion(v interface{}, d *schema.ResourceData) interface{} {
	return v
}

func flattenMLEngineJobTrainingOutputBuiltInAlgorithmOutputPythonVersion(v interface{}, d *schema.ResourceData) interface{} {
	return v
}

func flattenMLEngineJobTrainingOutputBuiltInAlgorithmOutputModelPath(v interface{}, d *schema.ResourceData) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := strconv.ParseInt(strVal, 10, 64); err == nil {
			return intVal
		} // let terraform core handle it if we can't convert the string to an int.
	}
	return v
}

func flattenMLEngineJobTrainingOutputHyperparameterMetricTag(v interface{}, d *schema.ResourceData) interface{} {
	return v
}

func expandMLEngineJobJobId(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandMLEngineJobLabels(v interface{}, d TerraformResourceData, config *Config) (map[string]string, error) {
	if v == nil {
		return map[string]string{}, nil
	}
	m := make(map[string]string)
	for k, val := range v.(map[string]interface{}) {
		m[k] = val.(string)
	}
	return m, nil
}

func expandMLEngineJobTrainingInput(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedScaleTier, err := expandMLEngineJobTrainingInputScaleTier(original["scale_tier"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedScaleTier); val.IsValid() && !isEmptyValue(val) {
		transformed["scaleTier"] = transformedScaleTier
	}

	transformedMasterType, err := expandMLEngineJobTrainingInputMasterType(original["master_type"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedMasterType); val.IsValid() && !isEmptyValue(val) {
		transformed["masterType"] = transformedMasterType
	}

	transformedMasterConfig, err := expandMLEngineJobTrainingInputMasterConfig(original["master_config"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedMasterConfig); val.IsValid() && !isEmptyValue(val) {
		transformed["masterConfig"] = transformedMasterConfig
	}

	transformedWorkerType, err := expandMLEngineJobTrainingInputWorkerType(original["worker_type"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedWorkerType); val.IsValid() && !isEmptyValue(val) {
		transformed["workerType"] = transformedWorkerType
	}

	transformedWorkerConfig, err := expandMLEngineJobTrainingInputWorkerConfig(original["worker_config"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedWorkerConfig); val.IsValid() && !isEmptyValue(val) {
		transformed["workerConfig"] = transformedWorkerConfig
	}

	transformedParameterServerType, err := expandMLEngineJobTrainingInputParameterServerType(original["parameter_server_type"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedParameterServerType); val.IsValid() && !isEmptyValue(val) {
		transformed["parameterServerType"] = transformedParameterServerType
	}

	transformedParameterServerConfig, err := expandMLEngineJobTrainingInputParameterServerConfig(original["parameter_server_config"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedParameterServerConfig); val.IsValid() && !isEmptyValue(val) {
		transformed["parameterServerConfig"] = transformedParameterServerConfig
	}

	transformedWorkerCount, err := expandMLEngineJobTrainingInputWorkerCount(original["worker_count"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedWorkerCount); val.IsValid() && !isEmptyValue(val) {
		transformed["workerCount"] = transformedWorkerCount
	}

	transformedParameterServerCount, err := expandMLEngineJobTrainingInputParameterServerCount(original["parameter_server_count"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedParameterServerCount); val.IsValid() && !isEmptyValue(val) {
		transformed["parameterServerCount"] = transformedParameterServerCount
	}

	transformedPackageUris, err := expandMLEngineJobTrainingInputPackageUris(original["package_uris"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedPackageUris); val.IsValid() && !isEmptyValue(val) {
		transformed["packageUris"] = transformedPackageUris
	}

	transformedPythonModule, err := expandMLEngineJobTrainingInputPythonModule(original["python_module"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedPythonModule); val.IsValid() && !isEmptyValue(val) {
		transformed["pythonModule"] = transformedPythonModule
	}

	transformedArgs, err := expandMLEngineJobTrainingInputArgs(original["args"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedArgs); val.IsValid() && !isEmptyValue(val) {
		transformed["args"] = transformedArgs
	}

	transformedRegion, err := expandMLEngineJobTrainingInputRegion(original["region"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedRegion); val.IsValid() && !isEmptyValue(val) {
		transformed["region"] = transformedRegion
	}

	transformedJobDir, err := expandMLEngineJobTrainingInputJobDir(original["job_dir"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedJobDir); val.IsValid() && !isEmptyValue(val) {
		transformed["jobDir"] = transformedJobDir
	}

	transformedRuntimeVersion, err := expandMLEngineJobTrainingInputRuntimeVersion(original["runtime_version"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedRuntimeVersion); val.IsValid() && !isEmptyValue(val) {
		transformed["runtimeVersion"] = transformedRuntimeVersion
	}

	transformedPythonVersion, err := expandMLEngineJobTrainingInputPythonVersion(original["python_version"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedPythonVersion); val.IsValid() && !isEmptyValue(val) {
		transformed["pythonVersion"] = transformedPythonVersion
	}

	return transformed, nil
}

func expandMLEngineJobTrainingInputScaleTier(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandMLEngineJobTrainingInputMasterType(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandMLEngineJobTrainingInputMasterConfig(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedAcceleratorConfig, err := expandMLEngineJobTrainingInputMasterConfigAcceleratorConfig(original["accelerator_config"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedAcceleratorConfig); val.IsValid() && !isEmptyValue(val) {
		transformed["acceleratorConfig"] = transformedAcceleratorConfig
	}

	transformedImageUri, err := expandMLEngineJobTrainingInputMasterConfigImageUri(original["image_uri"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedImageUri); val.IsValid() && !isEmptyValue(val) {
		transformed["imageUri"] = transformedImageUri
	}

	transformedTPUTfVersion, err := expandMLEngineJobTrainingInputMasterConfigTPUTfVersion(original["tpu_tf_version"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedTPUTfVersion); val.IsValid() && !isEmptyValue(val) {
		transformed["tpuTfVersion"] = transformedTPUTfVersion
	}

	return transformed, nil
}

func expandMLEngineJobTrainingInputMasterConfigAcceleratorConfig(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedCount, err := expandMLEngineJobTrainingInputMasterConfigAcceleratorConfigCount(original["count"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedCount); val.IsValid() && !isEmptyValue(val) {
		transformed["count"] = transformedCount
	}

	transformedType, err := expandMLEngineJobTrainingInputMasterConfigAcceleratorConfigType(original["type"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedType); val.IsValid() && !isEmptyValue(val) {
		transformed["type"] = transformedType
	}

	return transformed, nil
}

func expandMLEngineJobTrainingInputMasterConfigAcceleratorConfigCount(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandMLEngineJobTrainingInputMasterConfigAcceleratorConfigType(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandMLEngineJobTrainingInputMasterConfigImageUri(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandMLEngineJobTrainingInputMasterConfigTPUTfVersion(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandMLEngineJobTrainingInputWorkerType(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandMLEngineJobTrainingInputWorkerConfig(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedAcceleratorConfig, err := expandMLEngineJobTrainingInputWorkerConfigAcceleratorConfig(original["accelerator_config"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedAcceleratorConfig); val.IsValid() && !isEmptyValue(val) {
		transformed["acceleratorConfig"] = transformedAcceleratorConfig
	}

	transformedImageUri, err := expandMLEngineJobTrainingInputWorkerConfigImageUri(original["image_uri"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedImageUri); val.IsValid() && !isEmptyValue(val) {
		transformed["imageUri"] = transformedImageUri
	}

	transformedTPUTfVersion, err := expandMLEngineJobTrainingInputWorkerConfigTPUTfVersion(original["tpu_tf_version"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedTPUTfVersion); val.IsValid() && !isEmptyValue(val) {
		transformed["tpuTfVersion"] = transformedTPUTfVersion
	}

	return transformed, nil
}

func expandMLEngineJobTrainingInputWorkerConfigAcceleratorConfig(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedCount, err := expandMLEngineJobTrainingInputWorkerConfigAcceleratorConfigCount(original["count"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedCount); val.IsValid() && !isEmptyValue(val) {
		transformed["count"] = transformedCount
	}

	transformedType, err := expandMLEngineJobTrainingInputWorkerConfigAcceleratorConfigType(original["type"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedType); val.IsValid() && !isEmptyValue(val) {
		transformed["type"] = transformedType
	}

	return transformed, nil
}

func expandMLEngineJobTrainingInputWorkerConfigAcceleratorConfigCount(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandMLEngineJobTrainingInputWorkerConfigAcceleratorConfigType(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandMLEngineJobTrainingInputWorkerConfigImageUri(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandMLEngineJobTrainingInputWorkerConfigTPUTfVersion(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandMLEngineJobTrainingInputParameterServerType(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandMLEngineJobTrainingInputParameterServerConfig(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedAcceleratorConfig, err := expandMLEngineJobTrainingInputParameterServerConfigAcceleratorConfig(original["accelerator_config"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedAcceleratorConfig); val.IsValid() && !isEmptyValue(val) {
		transformed["acceleratorConfig"] = transformedAcceleratorConfig
	}

	transformedImageUri, err := expandMLEngineJobTrainingInputParameterServerConfigImageUri(original["image_uri"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedImageUri); val.IsValid() && !isEmptyValue(val) {
		transformed["imageUri"] = transformedImageUri
	}

	transformedTPUTfVersion, err := expandMLEngineJobTrainingInputParameterServerConfigTPUTfVersion(original["tpu_tf_version"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedTPUTfVersion); val.IsValid() && !isEmptyValue(val) {
		transformed["tpuTfVersion"] = transformedTPUTfVersion
	}

	return transformed, nil
}

func expandMLEngineJobTrainingInputParameterServerConfigAcceleratorConfig(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedCount, err := expandMLEngineJobTrainingInputParameterServerConfigAcceleratorConfigCount(original["count"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedCount); val.IsValid() && !isEmptyValue(val) {
		transformed["count"] = transformedCount
	}

	transformedType, err := expandMLEngineJobTrainingInputParameterServerConfigAcceleratorConfigType(original["type"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedType); val.IsValid() && !isEmptyValue(val) {
		transformed["type"] = transformedType
	}

	return transformed, nil
}

func expandMLEngineJobTrainingInputParameterServerConfigAcceleratorConfigCount(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandMLEngineJobTrainingInputParameterServerConfigAcceleratorConfigType(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandMLEngineJobTrainingInputParameterServerConfigImageUri(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandMLEngineJobTrainingInputParameterServerConfigTPUTfVersion(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandMLEngineJobTrainingInputWorkerCount(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandMLEngineJobTrainingInputParameterServerCount(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandMLEngineJobTrainingInputPackageUris(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandMLEngineJobTrainingInputPythonModule(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandMLEngineJobTrainingInputArgs(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandMLEngineJobTrainingInputRegion(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandMLEngineJobTrainingInputJobDir(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandMLEngineJobTrainingInputRuntimeVersion(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandMLEngineJobTrainingInputPythonVersion(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandMLEngineJobTrainingOutput(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedCompletedTrialCount, err := expandMLEngineJobTrainingOutputCompletedTrialCount(original["completed_trial_count"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedCompletedTrialCount); val.IsValid() && !isEmptyValue(val) {
		transformed["completedTrialCount"] = transformedCompletedTrialCount
	}

	transformedTrials, err := expandMLEngineJobTrainingOutputTrials(original["trials"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedTrials); val.IsValid() && !isEmptyValue(val) {
		transformed["trials"] = transformedTrials
	}

	transformedConsumedMLUnits, err := expandMLEngineJobTrainingOutputConsumedMLUnits(original["consumed_ml_units"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedConsumedMLUnits); val.IsValid() && !isEmptyValue(val) {
		transformed["consumedMLUnits"] = transformedConsumedMLUnits
	}

	transformedIsHyperparameterTuningJob, err := expandMLEngineJobTrainingOutputIsHyperparameterTuningJob(original["is_hyperparameter_tuning_job"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedIsHyperparameterTuningJob); val.IsValid() && !isEmptyValue(val) {
		transformed["isHyperparameterTuningJob"] = transformedIsHyperparameterTuningJob
	}

	transformedIsBuiltInAlgorithmJob, err := expandMLEngineJobTrainingOutputIsBuiltInAlgorithmJob(original["is_built_in_algorithm_job"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedIsBuiltInAlgorithmJob); val.IsValid() && !isEmptyValue(val) {
		transformed["isBuiltInAlgorithmJob"] = transformedIsBuiltInAlgorithmJob
	}

	transformedBuiltInAlgorithmOutput, err := expandMLEngineJobTrainingOutputBuiltInAlgorithmOutput(original["built_in_algorithm_output"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedBuiltInAlgorithmOutput); val.IsValid() && !isEmptyValue(val) {
		transformed["builtInAlgorithmOutput"] = transformedBuiltInAlgorithmOutput
	}

	transformedHyperparameterMetricTag, err := expandMLEngineJobTrainingOutputHyperparameterMetricTag(original["hyperparameter_metric_tag"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedHyperparameterMetricTag); val.IsValid() && !isEmptyValue(val) {
		transformed["hyperparameterMetricTag"] = transformedHyperparameterMetricTag
	}

	return transformed, nil
}

func expandMLEngineJobTrainingOutputCompletedTrialCount(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandMLEngineJobTrainingOutputTrials(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	l := v.([]interface{})
	req := make([]interface{}, 0, len(l))
	for _, raw := range l {
		if raw == nil {
			continue
		}
		original := raw.(map[string]interface{})
		transformed := make(map[string]interface{})

		transformedTrialId, err := expandMLEngineJobTrainingOutputTrialsTrialId(original["trial_id"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedTrialId); val.IsValid() && !isEmptyValue(val) {
			transformed["trialId"] = transformedTrialId
		}

		transformedHyperparameters, err := expandMLEngineJobTrainingOutputTrialsHyperparameters(original["hyperparameters"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedHyperparameters); val.IsValid() && !isEmptyValue(val) {
			transformed["hyperparameters"] = transformedHyperparameters
		}

		transformedStartTime, err := expandMLEngineJobTrainingOutputTrialsStartTime(original["start_time"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedStartTime); val.IsValid() && !isEmptyValue(val) {
			transformed["startTime"] = transformedStartTime
		}

		transformedEndTime, err := expandMLEngineJobTrainingOutputTrialsEndTime(original["end_time"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedEndTime); val.IsValid() && !isEmptyValue(val) {
			transformed["endTime"] = transformedEndTime
		}

		transformedState, err := expandMLEngineJobTrainingOutputTrialsState(original["state"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedState); val.IsValid() && !isEmptyValue(val) {
			transformed["state"] = transformedState
		}

		transformedFinalMetric, err := expandMLEngineJobTrainingOutputTrialsFinalMetric(original["final_metric"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedFinalMetric); val.IsValid() && !isEmptyValue(val) {
			transformed["finalMetric"] = transformedFinalMetric
		}

		transformedIsTrialStoppedEarly, err := expandMLEngineJobTrainingOutputTrialsIsTrialStoppedEarly(original["is_trial_stopped_early"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedIsTrialStoppedEarly); val.IsValid() && !isEmptyValue(val) {
			transformed["isTrialStoppedEarly"] = transformedIsTrialStoppedEarly
		}

		transformedBuiltInAlgorithmOutput, err := expandMLEngineJobTrainingOutputTrialsBuiltInAlgorithmOutput(original["built_in_algorithm_output"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedBuiltInAlgorithmOutput); val.IsValid() && !isEmptyValue(val) {
			transformed["builtInAlgorithmOutput"] = transformedBuiltInAlgorithmOutput
		}

		req = append(req, transformed)
	}
	return req, nil
}

func expandMLEngineJobTrainingOutputTrialsTrialId(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandMLEngineJobTrainingOutputTrialsHyperparameters(v interface{}, d TerraformResourceData, config *Config) (map[string]string, error) {
	if v == nil {
		return map[string]string{}, nil
	}
	m := make(map[string]string)
	for k, val := range v.(map[string]interface{}) {
		m[k] = val.(string)
	}
	return m, nil
}

func expandMLEngineJobTrainingOutputTrialsStartTime(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandMLEngineJobTrainingOutputTrialsEndTime(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandMLEngineJobTrainingOutputTrialsState(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandMLEngineJobTrainingOutputTrialsFinalMetric(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedTrainingStep, err := expandMLEngineJobTrainingOutputTrialsFinalMetricTrainingStep(original["training_step"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedTrainingStep); val.IsValid() && !isEmptyValue(val) {
		transformed["trainingStep"] = transformedTrainingStep
	}

	transformedObjectiveValue, err := expandMLEngineJobTrainingOutputTrialsFinalMetricObjectiveValue(original["objective_value"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedObjectiveValue); val.IsValid() && !isEmptyValue(val) {
		transformed["objectiveValue"] = transformedObjectiveValue
	}

	return transformed, nil
}

func expandMLEngineJobTrainingOutputTrialsFinalMetricTrainingStep(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandMLEngineJobTrainingOutputTrialsFinalMetricObjectiveValue(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandMLEngineJobTrainingOutputTrialsIsTrialStoppedEarly(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandMLEngineJobTrainingOutputTrialsBuiltInAlgorithmOutput(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedFramework, err := expandMLEngineJobTrainingOutputTrialsBuiltInAlgorithmOutputFramework(original["framework"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedFramework); val.IsValid() && !isEmptyValue(val) {
		transformed["framework"] = transformedFramework
	}

	transformedRuntimeVresion, err := expandMLEngineJobTrainingOutputTrialsBuiltInAlgorithmOutputRuntimeVresion(original["runtime_vresion"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedRuntimeVresion); val.IsValid() && !isEmptyValue(val) {
		transformed["runtimeVresion"] = transformedRuntimeVresion
	}

	transformedPythonVersion, err := expandMLEngineJobTrainingOutputTrialsBuiltInAlgorithmOutputPythonVersion(original["python_version"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedPythonVersion); val.IsValid() && !isEmptyValue(val) {
		transformed["pythonVersion"] = transformedPythonVersion
	}

	transformedModelPath, err := expandMLEngineJobTrainingOutputTrialsBuiltInAlgorithmOutputModelPath(original["model_path"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedModelPath); val.IsValid() && !isEmptyValue(val) {
		transformed["modelPath"] = transformedModelPath
	}

	return transformed, nil
}

func expandMLEngineJobTrainingOutputTrialsBuiltInAlgorithmOutputFramework(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandMLEngineJobTrainingOutputTrialsBuiltInAlgorithmOutputRuntimeVresion(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandMLEngineJobTrainingOutputTrialsBuiltInAlgorithmOutputPythonVersion(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandMLEngineJobTrainingOutputTrialsBuiltInAlgorithmOutputModelPath(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandMLEngineJobTrainingOutputConsumedMLUnits(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandMLEngineJobTrainingOutputIsHyperparameterTuningJob(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandMLEngineJobTrainingOutputIsBuiltInAlgorithmJob(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandMLEngineJobTrainingOutputBuiltInAlgorithmOutput(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedFramework, err := expandMLEngineJobTrainingOutputBuiltInAlgorithmOutputFramework(original["framework"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedFramework); val.IsValid() && !isEmptyValue(val) {
		transformed["framework"] = transformedFramework
	}

	transformedRuntimeVersion, err := expandMLEngineJobTrainingOutputBuiltInAlgorithmOutputRuntimeVersion(original["runtime_version"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedRuntimeVersion); val.IsValid() && !isEmptyValue(val) {
		transformed["runtimeVersion"] = transformedRuntimeVersion
	}

	transformedPythonVersion, err := expandMLEngineJobTrainingOutputBuiltInAlgorithmOutputPythonVersion(original["python_version"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedPythonVersion); val.IsValid() && !isEmptyValue(val) {
		transformed["pythonVersion"] = transformedPythonVersion
	}

	transformedModelPath, err := expandMLEngineJobTrainingOutputBuiltInAlgorithmOutputModelPath(original["model_path"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedModelPath); val.IsValid() && !isEmptyValue(val) {
		transformed["modelPath"] = transformedModelPath
	}

	return transformed, nil
}

func expandMLEngineJobTrainingOutputBuiltInAlgorithmOutputFramework(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandMLEngineJobTrainingOutputBuiltInAlgorithmOutputRuntimeVersion(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandMLEngineJobTrainingOutputBuiltInAlgorithmOutputPythonVersion(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandMLEngineJobTrainingOutputBuiltInAlgorithmOutputModelPath(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandMLEngineJobTrainingOutputHyperparameterMetricTag(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}
