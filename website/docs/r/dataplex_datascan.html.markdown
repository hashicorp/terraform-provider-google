---
# ----------------------------------------------------------------------------
#
#     ***     AUTO GENERATED CODE    ***    Type: MMv1     ***
#
# ----------------------------------------------------------------------------
#
#     This file is automatically generated by Magic Modules and manual
#     changes will be clobbered when the file is regenerated.
#
#     Please read more about how to change this file in
#     .github/CONTRIBUTING.md.
#
# ----------------------------------------------------------------------------
subcategory: "Dataplex"
description: |-
  Represents a user-visible job which provides the insights for the related data source.
---

# google\_dataplex\_datascan

Represents a user-visible job which provides the insights for the related data source.


To get more information about Datascan, see:

* [API documentation](https://cloud.google.com/dataplex/docs/reference/rest)
* How-to Guides
    * [Official Documentation](https://cloud.google.com/dataplex/docs)

## Example Usage - Dataplex Datascan Basic Profile


```hcl
resource "google_dataplex_datascan" "basic_profile" {
  location     = "us-central1"
  data_scan_id = "tf-test-datascan%{random_suffix}"

  data {
	  resource = "//bigquery.googleapis.com/projects/bigquery-public-data/datasets/samples/tables/shakespeare"
  }

  execution_spec {
    trigger {
      on_demand {}
    }
  }

  data_profile_spec {  
  }

  project = "my-project-name"
}
```
## Example Usage - Dataplex Datascan Full Profile


```hcl
resource "google_dataplex_datascan" "full_profile" {
  location     = "us-central1"
  display_name = "Full Datascan Profile"
  data_scan_id = "tf-test-datascan%{random_suffix}"
  description  = "Example resource - Full Datascan Profile"
  labels = {
    author = "billing"
  }

  data {
    resource = "//bigquery.googleapis.com/projects/bigquery-public-data/datasets/samples/tables/shakespeare"
  }

  execution_spec {
    trigger {
      schedule {
        cron = "TZ=America/New_York 1 1 * * *"
      }
    }
  }

  data_profile_spec {
    sampling_percent = 80
    row_filter = "word_count > 10"
  }

  project = "my-project-name"
}
```
## Example Usage - Dataplex Datascan Basic Quality


```hcl
resource "google_dataplex_datascan" "basic_quality" {
  location     = "us-central1"
  data_scan_id = "tf-test-datascan%{random_suffix}"

  data {
    resource = "//bigquery.googleapis.com/projects/bigquery-public-data/datasets/samples/tables/shakespeare"
  }

  execution_spec {
    trigger {
      on_demand {}
    }
  }

  data_quality_spec {
    rules {
      dimension = "VALIDITY"
      table_condition_expectation {
        sql_expression = "COUNT(*) > 0"
      }
    }
  }

  project = "my-project-name"
}
```
## Example Usage - Dataplex Datascan Full Quality


```hcl
resource "google_dataplex_datascan" "full_quality" {
  location = "us-central1"
  display_name = "Full Datascan Quality"
  data_scan_id = "tf-test-datascan%{random_suffix}"
  description = "Example resource - Full Datascan Quality"
  labels = {
    author = "billing"
  }

  data {
    resource = "//bigquery.googleapis.com/projects/bigquery-public-data/datasets/austin_bikeshare/tables/bikeshare_stations"
  }

  execution_spec {
    trigger {
      schedule {
        cron = "TZ=America/New_York 1 1 * * *"
      }
    }
    field = "modified_date"
  }

  data_quality_spec {
    sampling_percent = 5
    row_filter = "station_id > 1000"
    rules {
      column = "address"
      dimension = "VALIDITY"
      threshold = 0.99
      non_null_expectation {}
    }

    rules {
      column = "council_district"
      dimension = "VALIDITY"
      ignore_null = true
      threshold = 0.9
      range_expectation {
        min_value = 1
        max_value = 10
        strict_min_enabled = true
        strict_max_enabled = false
      }
    }

    rules {
      column = "power_type"
      dimension = "VALIDITY"
      ignore_null = false
      regex_expectation {
        regex = ".*solar.*"
      }
    }

    rules {
      column = "property_type"
      dimension = "VALIDITY"
      ignore_null = false
      set_expectation {
        values = ["sidewalk", "parkland"]
      }
    }


    rules {
      column = "address"
      dimension = "UNIQUENESS"
      uniqueness_expectation {}
    }

    rules {
      column = "number_of_docks"
      dimension = "VALIDITY"
      statistic_range_expectation {
        statistic = "MEAN"
        min_value = 5
        max_value = 15
        strict_min_enabled = true
        strict_max_enabled = true
      }
    }

    rules {
      column = "footprint_length"
      dimension = "VALIDITY"
      row_condition_expectation {
        sql_expression = "footprint_length > 0 AND footprint_length <= 10"
      }
    }

    rules {
      dimension = "VALIDITY"
      table_condition_expectation {
        sql_expression = "COUNT(*) > 0"
      }
    }
  }


  project = "my-project-name"
}
```

## Argument Reference

The following arguments are supported:


* `data` -
  (Required)
  The data source for DataScan.
  Structure is [documented below](#nested_data).

* `execution_spec` -
  (Required)
  DataScan execution settings.
  Structure is [documented below](#nested_execution_spec).

* `location` -
  (Required)
  The location where the data scan should reside.

* `data_scan_id` -
  (Required)
  DataScan identifier. Must contain only lowercase letters, numbers and hyphens. Must start with a letter. Must end with a number or a letter.


<a name="nested_data"></a>The `data` block supports:

* `entity` -
  (Optional)
  The Dataplex entity that represents the data source(e.g. BigQuery table) for Datascan.

* `resource` -
  (Optional)
  The service-qualified full resource name of the cloud resource for a DataScan job to scan against. The field could be:
  (Cloud Storage bucket for DataDiscoveryScan)BigQuery table of type "TABLE" for DataProfileScan/DataQualityScan.

<a name="nested_execution_spec"></a>The `execution_spec` block supports:

* `trigger` -
  (Required)
  Spec related to how often and when a scan should be triggered.
  Structure is [documented below](#nested_trigger).

* `field` -
  (Optional)
  The unnested field (of type Date or Timestamp) that contains values which monotonically increase over time. If not specified, a data scan will run for all data in the table.


<a name="nested_trigger"></a>The `trigger` block supports:

* `on_demand` -
  (Optional)
  The scan runs once via dataScans.run API.

* `schedule` -
  (Optional)
  The scan is scheduled to run periodically.
  Structure is [documented below](#nested_schedule).


<a name="nested_schedule"></a>The `schedule` block supports:

* `cron` -
  (Required)
  Cron schedule for running scans periodically. This field is required for Schedule scans.

- - -


* `description` -
  (Optional)
  Description of the scan.

* `display_name` -
  (Optional)
  User friendly display name.

* `labels` -
  (Optional)
  User-defined labels for the scan. A list of key->value pairs.

* `data_quality_spec` -
  (Optional)
  DataQualityScan related setting.
  Structure is [documented below](#nested_data_quality_spec).

* `data_profile_spec` -
  (Optional)
  DataProfileScan related setting.
  Structure is [documented below](#nested_data_profile_spec).

* `project` - (Optional) The ID of the project in which the resource belongs.
    If it is not provided, the provider project is used.


<a name="nested_data_quality_spec"></a>The `data_quality_spec` block supports:

* `sampling_percent` -
  (Optional)
  The percentage of the records to be selected from the dataset for DataScan.

* `row_filter` -
  (Optional)
  A filter applied to all rows in a single DataScan job. The filter needs to be a valid SQL expression for a WHERE clause in BigQuery standard SQL syntax. Example: col1 >= 0 AND col2 < 10

* `rules` -
  (Optional)
  The list of rules to evaluate against a data source. At least one rule is required.
  Structure is [documented below](#nested_rules).


<a name="nested_rules"></a>The `rules` block supports:

* `column` -
  (Optional)
  The unnested column which this rule is evaluated against.

* `ignore_null` -
  (Optional)
  Rows with null values will automatically fail a rule, unless ignoreNull is true. In that case, such null rows are trivially considered passing. Only applicable to ColumnMap rules.

* `dimension` -
  (Required)
  The dimension a rule belongs to. Results are also aggregated at the dimension level. Supported dimensions are ["COMPLETENESS", "ACCURACY", "CONSISTENCY", "VALIDITY", "UNIQUENESS", "INTEGRITY"]

* `threshold` -
  (Optional)
  The minimum ratio of passing_rows / total_rows required to pass this rule, with a range of [0.0, 1.0]. 0 indicates default value (i.e. 1.0).

* `range_expectation` -
  (Optional)
  ColumnMap rule which evaluates whether each column value lies between a specified range.
  Structure is [documented below](#nested_range_expectation).

* `non_null_expectation` -
  (Optional)
  ColumnMap rule which evaluates whether each column value is null.

* `set_expectation` -
  (Optional)
  ColumnMap rule which evaluates whether each column value is contained by a specified set.
  Structure is [documented below](#nested_set_expectation).

* `regex_expectation` -
  (Optional)
  ColumnMap rule which evaluates whether each column value matches a specified regex.
  Structure is [documented below](#nested_regex_expectation).

* `uniqueness_expectation` -
  (Optional)
  ColumnAggregate rule which evaluates whether the column has duplicates.

* `statistic_range_expectation` -
  (Optional)
  ColumnAggregate rule which evaluates whether the column aggregate statistic lies between a specified range.
  Structure is [documented below](#nested_statistic_range_expectation).

* `row_condition_expectation` -
  (Optional)
  Table rule which evaluates whether each row passes the specified condition.
  Structure is [documented below](#nested_row_condition_expectation).

* `table_condition_expectation` -
  (Optional)
  Table rule which evaluates whether the provided expression is true.
  Structure is [documented below](#nested_table_condition_expectation).


<a name="nested_range_expectation"></a>The `range_expectation` block supports:

* `min_value` -
  (Optional)
  The minimum column value allowed for a row to pass this validation. At least one of minValue and maxValue need to be provided.

* `max_value` -
  (Optional)
  The maximum column value allowed for a row to pass this validation. At least one of minValue and maxValue need to be provided.

* `strict_min_enabled` -
  (Optional)
  Whether each value needs to be strictly greater than ('>') the minimum, or if equality is allowed.
  Only relevant if a minValue has been defined. Default = false.

* `strict_max_enabled` -
  (Optional)
  Whether each value needs to be strictly lesser than ('<') the maximum, or if equality is allowed.
  Only relevant if a maxValue has been defined. Default = false.

<a name="nested_set_expectation"></a>The `set_expectation` block supports:

* `values` -
  (Required)
  Expected values for the column value.

<a name="nested_regex_expectation"></a>The `regex_expectation` block supports:

* `regex` -
  (Required)
  A regular expression the column value is expected to match.

<a name="nested_statistic_range_expectation"></a>The `statistic_range_expectation` block supports:

* `statistic` -
  (Required)
  column statistics.
  Possible values are: `STATISTIC_UNDEFINED`, `MEAN`, `MIN`, `MAX`.

* `min_value` -
  (Optional)
  The minimum column statistic value allowed for a row to pass this validation.
  At least one of minValue and maxValue need to be provided.

* `max_value` -
  (Optional)
  The maximum column statistic value allowed for a row to pass this validation.
  At least one of minValue and maxValue need to be provided.

* `strict_min_enabled` -
  (Optional)
  Whether column statistic needs to be strictly greater than ('>') the minimum, or if equality is allowed.
  Only relevant if a minValue has been defined. Default = false.

* `strict_max_enabled` -
  (Optional)
  Whether column statistic needs to be strictly lesser than ('<') the maximum, or if equality is allowed.
  Only relevant if a maxValue has been defined. Default = false.

<a name="nested_row_condition_expectation"></a>The `row_condition_expectation` block supports:

* `sql_expression` -
  (Required)
  The SQL expression.

<a name="nested_table_condition_expectation"></a>The `table_condition_expectation` block supports:

* `sql_expression` -
  (Required)
  The SQL expression.

<a name="nested_data_profile_spec"></a>The `data_profile_spec` block supports:

* `sampling_percent` -
  (Optional)
  The percentage of the records to be selected from the dataset for DataScan.

* `row_filter` -
  (Optional)
  A filter applied to all rows in a single DataScan job. The filter needs to be a valid SQL expression for a WHERE clause in BigQuery standard SQL syntax. Example: col1 >= 0 AND col2 < 10

## Attributes Reference

In addition to the arguments listed above, the following computed attributes are exported:

* `id` - an identifier for the resource with format `projects/{{project}}/locations/{{location}}/dataScans/{{data_scan_id}}`

* `name` -
  The relative resource name of the scan, of the form: projects/{project}/locations/{locationId}/dataScans/{datascan_id}, where project refers to a project_id or project_number and locationId refers to a GCP region.

* `uid` -
  System generated globally unique ID for the scan. This ID will be different if the scan is deleted and re-created with the same name.

* `state` -
  Current state of the DataScan.

* `create_time` -
  The time when the scan was created.

* `update_time` -
  The time when the scan was last updated.

* `execution_status` -
  Status of the data scan execution.
  Structure is [documented below](#nested_execution_status).

* `type` -
  The type of DataScan.

* `data_quality_result` -
  The result of the data quality scan.
  Structure is [documented below](#nested_data_quality_result).

* `data_profile_result` -
  The result of the data profile scan.
  Structure is [documented below](#nested_data_profile_result).


<a name="nested_execution_status"></a>The `execution_status` block contains:

* `latest_job_end_time` -
  (Output)
  The time when the latest DataScanJob started.

* `latest_job_start_time` -
  (Output)
  The time when the latest DataScanJob ended.

<a name="nested_data_quality_result"></a>The `data_quality_result` block contains:

* `passed` -
  (Output)
  Overall data quality result -- true if all rules passed.

* `dimensions` -
  (Optional)
  A list of results at the dimension level.
  Structure is [documented below](#nested_dimensions).

* `rules` -
  (Output)
  A list of all the rules in a job, and their results.
  Structure is [documented below](#nested_rules).

* `row_count` -
  (Output)
  The count of rows processed.

* `scanned_data` -
  (Output)
  The data scanned for this result.
  Structure is [documented below](#nested_scanned_data).


<a name="nested_dimensions"></a>The `dimensions` block supports:

* `passed` -
  (Optional)
  Whether the dimension passed or failed.

<a name="nested_rules"></a>The `rules` block contains:

* `rule` -
  (Output)
  The rule specified in the DataQualitySpec, as is.
  Structure is [documented below](#nested_rule).

* `passed` -
  (Output)
  Whether the rule passed or failed.

* `evaluated_count` -
  (Output)
  The number of rows a rule was evaluated against. This field is only valid for ColumnMap type rules.
  Evaluated count can be configured to either
  1. include all rows (default) - with null rows automatically failing rule evaluation, or
  2. exclude null rows from the evaluatedCount, by setting ignore_nulls = true.

* `passed_count` -
  (Output)
  The number of rows which passed a rule evaluation. This field is only valid for ColumnMap type rules.

* `null_count` -
  (Output)
  The number of rows with null values in the specified column.

* `pass_ratio` -
  (Output)
  The ratio of passedCount / evaluatedCount. This field is only valid for ColumnMap type rules.

* `failing_rows_query` -
  (Output)
  The query to find rows that did not pass this rule. Only applies to ColumnMap and RowCondition rules.


<a name="nested_rule"></a>The `rule` block contains:

* `column` -
  (Optional)
  The unnested column which this rule is evaluated against.

* `ignore_null` -
  (Optional)
  Rows with null values will automatically fail a rule, unless ignoreNull is true. In that case, such null rows are trivially considered passing. Only applicable to ColumnMap rules.

* `dimension` -
  (Optional)
  The dimension a rule belongs to. Results are also aggregated at the dimension level. Supported dimensions are ["COMPLETENESS", "ACCURACY", "CONSISTENCY", "VALIDITY", "UNIQUENESS", "INTEGRITY"]

* `threshold` -
  (Optional)
  The minimum ratio of passing_rows / total_rows required to pass this rule, with a range of [0.0, 1.0]. 0 indicates default value (i.e. 1.0).

* `range_expectation` -
  (Output)
  ColumnMap rule which evaluates whether each column value lies between a specified range.
  Structure is [documented below](#nested_range_expectation).

* `non_null_expectation` -
  (Output)
  ColumnMap rule which evaluates whether each column value is null.

* `set_expectation` -
  (Output)
  ColumnMap rule which evaluates whether each column value is contained by a specified set.
  Structure is [documented below](#nested_set_expectation).

* `regex_expectation` -
  (Output)
  ColumnMap rule which evaluates whether each column value matches a specified regex.
  Structure is [documented below](#nested_regex_expectation).

* `uniqueness_expectation` -
  (Output)
  ColumnAggregate rule which evaluates whether the column has duplicates.

* `statistic_range_expectation` -
  (Output)
  ColumnAggregate rule which evaluates whether the column aggregate statistic lies between a specified range.
  Structure is [documented below](#nested_statistic_range_expectation).

* `row_condition_expectation` -
  (Output)
  Table rule which evaluates whether each row passes the specified condition.
  Structure is [documented below](#nested_row_condition_expectation).

* `table_condition_expectation` -
  (Output)
  Table rule which evaluates whether the provided expression is true.
  Structure is [documented below](#nested_table_condition_expectation).


<a name="nested_range_expectation"></a>The `range_expectation` block contains:

* `min_value` -
  (Optional)
  The minimum column value allowed for a row to pass this validation. At least one of minValue and maxValue need to be provided.

* `max_value` -
  (Optional)
  The maximum column value allowed for a row to pass this validation. At least one of minValue and maxValue need to be provided.

* `strict_min_enabled` -
  (Optional)
  Whether each value needs to be strictly greater than ('>') the minimum, or if equality is allowed.
  Only relevant if a minValue has been defined. Default = false.

* `strict_max_enabled` -
  (Optional)
  Whether each value needs to be strictly lesser than ('<') the maximum, or if equality is allowed.
  Only relevant if a maxValue has been defined. Default = false.

<a name="nested_set_expectation"></a>The `set_expectation` block contains:

* `values` -
  (Optional)
  Expected values for the column value.

<a name="nested_regex_expectation"></a>The `regex_expectation` block contains:

* `regex` -
  (Optional)
  A regular expression the column value is expected to match.

<a name="nested_statistic_range_expectation"></a>The `statistic_range_expectation` block contains:

* `statistic` -
  (Optional)
  column statistics.
  Possible values are: `STATISTIC_UNDEFINED`, `MEAN`, `MIN`, `MAX`.

* `min_value` -
  (Optional)
  The minimum column statistic value allowed for a row to pass this validation.
  At least one of minValue and maxValue need to be provided.

* `max_value` -
  (Optional)
  The maximum column statistic value allowed for a row to pass this validation.
  At least one of minValue and maxValue need to be provided.

* `strict_min_enabled` -
  (Optional)
  Whether column statistic needs to be strictly greater than ('>') the minimum, or if equality is allowed.
  Only relevant if a minValue has been defined. Default = false.

* `strict_max_enabled` -
  (Optional)
  Whether column statistic needs to be strictly lesser than ('<') the maximum, or if equality is allowed.
  Only relevant if a maxValue has been defined. Default = false.

<a name="nested_row_condition_expectation"></a>The `row_condition_expectation` block contains:

* `sql_expression` -
  (Optional)
  The SQL expression.

<a name="nested_table_condition_expectation"></a>The `table_condition_expectation` block contains:

* `sql_expression` -
  (Optional)
  The SQL expression.

<a name="nested_scanned_data"></a>The `scanned_data` block contains:

* `incremental_field` -
  (Optional)
  The range denoted by values of an incremental field
  Structure is [documented below](#nested_incremental_field).


<a name="nested_incremental_field"></a>The `incremental_field` block supports:

* `field` -
  (Optional)
  The field that contains values which monotonically increases over time (e.g. a timestamp column).

* `start` -
  (Optional)
  Value that marks the start of the range.

* `end` -
  (Optional)
  Value that marks the end of the range.

<a name="nested_data_profile_result"></a>The `data_profile_result` block contains:

* `row_count` -
  (Optional)
  The count of rows scanned.

* `profile` -
  (Output)
  The profile information per field.
  Structure is [documented below](#nested_profile).

* `scanned_data` -
  (Output)
  The data scanned for this result.
  Structure is [documented below](#nested_scanned_data).


<a name="nested_profile"></a>The `profile` block contains:

* `fields` -
  (Optional)
  List of fields with structural and profile information for each field.
  Structure is [documented below](#nested_fields).


<a name="nested_fields"></a>The `fields` block supports:

* `name` -
  (Optional)
  The name of the field.

* `type` -
  (Optional)
  The field data type.

* `mode` -
  (Optional)
  The mode of the field. Possible values include:
  1. REQUIRED, if it is a required field.
  2. NULLABLE, if it is an optional field.
  3. REPEATED, if it is a repeated field.

* `profile` -
  (Optional)
  Profile information for the corresponding field.
  Structure is [documented below](#nested_profile).


<a name="nested_profile"></a>The `profile` block supports:

* `null_ratio` -
  (Output)
  Ratio of rows with null value against total scanned rows.

* `distinct_ratio` -
  (Optional)
  Ratio of rows with distinct values against total scanned rows. Not available for complex non-groupable field type RECORD and fields with REPEATABLE mode.

* `top_n_values` -
  (Optional)
  The list of top N non-null values and number of times they occur in the scanned data. N is 10 or equal to the number of distinct values in the field, whichever is smaller. Not available for complex non-groupable field type RECORD and fields with REPEATABLE mode.
  Structure is [documented below](#nested_top_n_values).

* `string_profile` -
  (Output)
  String type field information.
  Structure is [documented below](#nested_string_profile).

* `integer_profile` -
  (Output)
  Integer type field information.
  Structure is [documented below](#nested_integer_profile).

* `double_profile` -
  (Output)
  Double type field information.
  Structure is [documented below](#nested_double_profile).


<a name="nested_top_n_values"></a>The `top_n_values` block supports:

* `value` -
  (Optional)
  String value of a top N non-null value.

* `count` -
  (Optional)
  Count of the corresponding value in the scanned data.

<a name="nested_string_profile"></a>The `string_profile` block contains:

* `min_length` -
  (Optional)
  Minimum length of non-null values in the scanned data.

* `max_length` -
  (Optional)
  Maximum length of non-null values in the scanned data.

* `average_length` -
  (Optional)
  Average length of non-null values in the scanned data.

<a name="nested_integer_profile"></a>The `integer_profile` block contains:

* `average` -
  (Optional)
  Average of non-null values in the scanned data. NaN, if the field has a NaN.

* `standard_deviation` -
  (Optional)
  Standard deviation of non-null values in the scanned data. NaN, if the field has a NaN.

* `min` -
  (Optional)
  Minimum of non-null values in the scanned data. NaN, if the field has a NaN.

* `quartiles` -
  (Optional)
  A quartile divides the number of data points into four parts, or quarters, of more-or-less equal size. Three main quartiles used are: The first quartile (Q1) splits off the lowest 25% of data from the highest 75%. It is also known as the lower or 25th empirical quartile, as 25% of the data is below this point. The second quartile (Q2) is the median of a data set. So, 50% of the data lies below this point. The third quartile (Q3) splits off the highest 25% of data from the lowest 75%. It is known as the upper or 75th empirical quartile, as 75% of the data lies below this point. Here, the quartiles is provided as an ordered list of quartile values for the scanned data, occurring in order Q1, median, Q3.

* `max` -
  (Optional)
  Maximum of non-null values in the scanned data. NaN, if the field has a NaN.

<a name="nested_double_profile"></a>The `double_profile` block contains:

* `average` -
  (Optional)
  Average of non-null values in the scanned data. NaN, if the field has a NaN.

* `standard_deviation` -
  (Optional)
  Standard deviation of non-null values in the scanned data. NaN, if the field has a NaN.

* `min` -
  (Optional)
  Minimum of non-null values in the scanned data. NaN, if the field has a NaN.

* `quartiles` -
  (Optional)
  A quartile divides the number of data points into four parts, or quarters, of more-or-less equal size. Three main quartiles used are: The first quartile (Q1) splits off the lowest 25% of data from the highest 75%. It is also known as the lower or 25th empirical quartile, as 25% of the data is below this point. The second quartile (Q2) is the median of a data set. So, 50% of the data lies below this point. The third quartile (Q3) splits off the highest 25% of data from the lowest 75%. It is known as the upper or 75th empirical quartile, as 75% of the data lies below this point. Here, the quartiles is provided as an ordered list of quartile values for the scanned data, occurring in order Q1, median, Q3.

* `max` -
  (Optional)
  Maximum of non-null values in the scanned data. NaN, if the field has a NaN.

<a name="nested_scanned_data"></a>The `scanned_data` block contains:

* `incremental_field` -
  (Optional)
  The range denoted by values of an incremental field
  Structure is [documented below](#nested_incremental_field).


<a name="nested_incremental_field"></a>The `incremental_field` block supports:

* `field` -
  (Optional)
  The field that contains values which monotonically increases over time (e.g. a timestamp column).

* `start` -
  (Optional)
  Value that marks the start of the range.

* `end` -
  (Optional)
  Value that marks the end of the range.

## Timeouts

This resource provides the following
[Timeouts](https://developer.hashicorp.com/terraform/plugin/sdkv2/resources/retries-and-customizable-timeouts) configuration options:

- `create` - Default is 5 minutes.
- `update` - Default is 5 minutes.
- `delete` - Default is 5 minutes.

## Import


Datascan can be imported using any of these accepted formats:

```
$ terraform import google_dataplex_datascan.default projects/{{project}}/locations/{{location}}/dataScans/{{data_scan_id}}
$ terraform import google_dataplex_datascan.default {{project}}/{{location}}/{{data_scan_id}}
$ terraform import google_dataplex_datascan.default {{location}}/{{data_scan_id}}
$ terraform import google_dataplex_datascan.default {{data_scan_id}}
```

## User Project Overrides

This resource supports [User Project Overrides](https://registry.terraform.io/providers/hashicorp/google/latest/docs/guides/provider_reference#user_project_override).
