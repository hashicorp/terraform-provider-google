---
# ----------------------------------------------------------------------------
#
#     ***     AUTO GENERATED CODE    ***    AUTO GENERATED CODE     ***
#
# ----------------------------------------------------------------------------
#
#     This file is automatically generated by Magic Modules and manual
#     changes will be clobbered when the file is regenerated.
#
#     Please read more about how to change this file in
#     .github/CONTRIBUTING.md.
#
# ----------------------------------------------------------------------------
subcategory: "ML Engine"
layout: "google"
page_title: "Google: google_ml_engine_job"
sidebar_current: "docs-google-ml-engine-job"
description: |-
  Represents a training or prediction job.
---

# google\_ml\_engine\_job

Represents a training or prediction job.



<div class = "oics-button" style="float: right; margin: 0 0 -15px">
  <a href="https://console.cloud.google.com/cloudshell/open?cloudshell_git_repo=https%3A%2F%2Fgithub.com%2Fterraform-google-modules%2Fdocs-examples.git&cloudshell_working_dir=ml_job_training&cloudshell_image=gcr.io%2Fgraphite-cloud-shell-images%2Fterraform%3Alatest&open_in_editor=main.tf&cloudshell_print=.%2Fmotd&cloudshell_tutorial=.%2Ftutorial.md" target="_blank">
    <img alt="Open in Cloud Shell" src="//gstatic.com/cloudssh/images/open-btn.svg" style="max-height: 44px; margin: 32px auto; max-width: 100%;">
  </a>
</div>
## Example Usage - Ml Job Training


```hcl
resource "google_ml_engine_job" "default" { {
  job_id = "ml_job_training"
 
  training_input {
    job_dir         = "gs://cloud-samples-data/scikit_learn_job_dir"
    package_uris    = ["gs://cloud-samples-data/scikit_learn_job_dir/packages/fd65c711be463b2d1e841325d389b6c557db097c2281d40423c8451579238523/census_training-0.0.0.tar.gz"]
    python_module   = "census_training.train"
    python_version  = "2.7"
    region          = "us-central1"
    runtime_version = "1.14"
    scale_tier      = "BASIC"
  }
}
```
<div class = "oics-button" style="float: right; margin: 0 0 -15px">
  <a href="https://console.cloud.google.com/cloudshell/open?cloudshell_git_repo=https%3A%2F%2Fgithub.com%2Fterraform-google-modules%2Fdocs-examples.git&cloudshell_working_dir=ml_job_tensor&cloudshell_image=gcr.io%2Fgraphite-cloud-shell-images%2Fterraform%3Alatest&open_in_editor=main.tf&cloudshell_print=.%2Fmotd&cloudshell_tutorial=.%2Ftutorial.md" target="_blank">
    <img alt="Open in Cloud Shell" src="//gstatic.com/cloudssh/images/open-btn.svg" style="max-height: 44px; margin: 32px auto; max-width: 100%;">
  </a>
</div>
## Example Usage - Ml Job Tensor


```hcl
resource "google_ml_engine_job" "default" {
  job_id = "ml_job_tesnor"

  training_input {
    job_dir         = "gs://cloud-samples-data/tensorflow_census_training"
    package_uris    = [""gs://cloud-samples-data/tensorflow_census_training/packages/aa1821b516689e2b95d2a0dec00302e50314e20e8ee303721ca3f31fd46091e3/preprocessing-1.0.tar.gz""]
    args            = ["--train-files=gs://workingnow/data/adult.data.csv", "--eval-files=gs://workingnow/data/adult.test.csv", "--train-steps=1000", "--eval-steps=100"]
    python_module   = "trainer.task"
    region          = "us-central1"
    runtime_version = "1.14"
    scale_tier      = "BASIC"
  }
}
```
<div class = "oics-button" style="float: right; margin: 0 0 -15px">
  <a href="https://console.cloud.google.com/cloudshell/open?cloudshell_git_repo=https%3A%2F%2Fgithub.com%2Fterraform-google-modules%2Fdocs-examples.git&cloudshell_working_dir=ml_job_container&cloudshell_image=gcr.io%2Fgraphite-cloud-shell-images%2Fterraform%3Alatest&open_in_editor=main.tf&cloudshell_print=.%2Fmotd&cloudshell_tutorial=.%2Ftutorial.md" target="_blank">
    <img alt="Open in Cloud Shell" src="//gstatic.com/cloudssh/images/open-btn.svg" style="max-height: 44px; margin: 32px auto; max-width: 100%;">
  </a>
</div>
## Example Usage - Ml Job Container


```hcl
resource "google_ml_engine_job" "default" {
  job_id = "ml_job_container"

  training_input {
    master_config {
      image_uri = "gcr.io/test-proj-234209/mnist_pytorch_custom_container:mnist_pytorch_cpu"
    }

    args = [
      "--model-dir=gs://aiptrainingbucket/pytorch_model_test4AIP20191105_143006",
      "--epochs=10",
    ]

    region        = "us-east1"
  }
}
```

## Argument Reference

The following arguments are supported:


* `job_id` -
  (Required)
  The user-specified id of the job.


- - -


* `labels` -
  (Optional)
  One or more labels that you can add, to organize your jobs. Each label is a key-value pair, 
  where both the key and the value are arbitrary Strings that you supply.

* `training_input` -
  (Optional)
  Input parameters to create a training job.  Structure is documented below.

* `training_output` -
  (Optional)
  The current training job result.  Structure is documented below.

* `project` - (Optional) The ID of the project in which the resource belongs.
    If it is not provided, the provider project is used.


The `training_input` block supports:

* `scale_tier` -
  (Optional)
  Specifies the machine types, the number of replicas for workers and parameter servers. 
  https://cloud.google.com/ml-engine/reference/rest/v1/projects.jobs#ScaleTier

* `master_type` -
  (Optional)
  Specifies the type of virtual machine to use for your training job's master worker

* `master_config` -
  (Optional)
  The configuration for your master worker.  Structure is documented below.

* `worker_type` -
  (Optional)
  Specifies the type of virtual machine to use for your training job's worker nodes.
  The supported values are the same as those described in the entry for masterType.
  This value must be consistent with the category of machine type that masterType uses. In other words, 
  both must be Compute Engine machine types or both must be legacy machine types.
  If you use cloud_tpu for this value, see special instructions for configuring a custom TPU machine.
  This value must be present when scaleTier is set to CUSTOM and workerCount is greater than zero.

* `worker_config` -
  (Optional)
  The configuration for workers.  Structure is documented below.

* `parameter_server_type` -
  (Optional)
  Specifies the type of virtual machine to use for your training job's parameter server.
  The supported values are the same as those described in the entry for masterType. 
  This value must be consistent with the category of machine type that masterType uses. 
  In other words, both must be Compute Engine machine types or both must be legacy machine types. This
  value must be present when scaleTier is set to CUSTOM and parameterServerCount is greater than zero.

* `parameter_server_config` -
  (Optional)
  The configuration for parameter servers.  Structure is documented below.

* `worker_count` -
  (Optional)
  The number of worker replicas to use for the training job. Each replica in the cluster will be of the type 
  specified in workerType

* `parameter_server_count` -
  (Optional)
  The number of parameter server replicas to use for the training job. Each replica in the cluster will be 
  of the type specified in parameterServerType.

* `package_uris` -
  (Optional)
  The Google Cloud Storage location of the packages with the training program and any additional dependencies.
   The maximum number of package URIs is 100.

* `python_module` -
  (Optional)
  The Python module name to run after installing the packages.

* `args` -
  (Optional)
  Command line arguments to pass to the program.

* `region` -
  (Required)
  The Google Compute Engine region to run the training job in. See the available regions for AI Platform services.

* `job_dir` -
  (Optional)
  A Google Cloud Storage path in which to store training outputs and other data needed for training. This path is passed 
  to your TensorFlow program as the '--job-dir' command-line argument. The benefit of specifying this field is that Cloud 
  ML validates the path for use in training.

* `runtime_version` -
  (Optional)
  The AI Platform runtime version to use for training. If not set, AI Platform uses the default stable version, 1.0. For more 
  information, see the runtime version list and how to manage runtime versions.

* `python_version` -
  (Optional)
  The version of Python used in training. If not set, the default version is '2.7'. Python '3.5' is available when 
  runtimeVersion is set to '1.4' and above. Python '2.7' works with all supported runtime versions.


The `master_config` block supports:

* `accelerator_config` -
  (Optional)
  Represents the type and number of accelerators used by the replica  Structure is documented below.

* `image_uri` -
  (Optional)
  The Docker image to run on the replica. This image must be in Container Registry.

* `tpu_tf_version` -
  (Optional)
  The AI Platform runtime version that includes a TensorFlow version matching the one used in the 
  custom container. This field is required if the replica is a TPU worker that uses a custom container. 
  Otherwise, do not specify this field.


The `accelerator_config` block supports:

* `count` -
  (Optional)
  The number of accelerators to attach to each machine running the job.

* `type` -
  (Optional)
  The type of accelerator to use.

The `worker_config` block supports:

* `accelerator_config` -
  (Optional)
  Represents the type and number of accelerators used by the replica  Structure is documented below.

* `image_uri` -
  (Optional)
  The Docker image to run on the replica. This image must be in Container Registry.

* `tpu_tf_version` -
  (Optional)
  The AI Platform runtime version that includes a TensorFlow version matching the one used in the 
  custom container. This field is required if the replica is a TPU worker that uses a custom container. 
  Otherwise, do not specify this field.


The `accelerator_config` block supports:

* `count` -
  (Optional)
  The number of accelerators to attach to each machine running the job.

* `type` -
  (Optional)
  The type of accelerator to use.

The `parameter_server_config` block supports:

* `accelerator_config` -
  (Optional)
  Represents the type and number of accelerators used by the replica  Structure is documented below.

* `image_uri` -
  (Optional)
  The Docker image to run on the replica. This image must be in Container Registry.

* `tpu_tf_version` -
  (Optional)
  The AI Platform runtime version that includes a TensorFlow version matching the one used in the 
  custom container. This field is required if the replica is a TPU worker that uses a custom container. 
  Otherwise, do not specify this field.


The `accelerator_config` block supports:

* `count` -
  (Optional)
  The number of accelerators to attach to each machine running the job.

* `type` -
  (Optional)
  The type of accelerator to use.

The `training_output` block supports:

* `completed_trial_count` -
  The number of hyperparameter tuning trials that completed successfully. Only set for hyperparameter tuning jobs.

* `trials` -
  (Optional)
  Results for individual Hyperparameter trials. Only set for hyperparameter tuning jobs.  Structure is documented below.

* `consumed_ml_units` -
  (Optional)
  The amount of ML units consumed by the job.

* `is_hyperparameter_tuning_job` -
  (Optional)
  Whether this job is a hyperparameter tuning job.

* `is_built_in_algorithm_job` -
  (Optional)
  Whether this job is a built-in Algorithm job.

* `built_in_algorithm_output` -
  (Optional)
  Details related to built-in algorithms jobs. Only set for built-in algorithms jobs.  Structure is documented below.

* `hyperparameter_metric_tag` -
  (Optional)
  The TensorFlow summary tag name used for optimizing hyperparameter tuning trials.


The `trials` block supports:

* `trial_id` -
  The trial id for these results.

* `hyperparameters` -
  The hyperparameters given to this trial. An object containing a list of "key": value pairs. 
  Example: { "name": "wrench", "mass": "1.3kg", "count": "3" }

* `start_time` -
  Start time for the trial.

* `end_time` -
  End time for the trial.

* `state` -
  The detailed state of a job.

* `final_metric` -
  (Optional)
  The final objective metric seen for this trial.  Structure is documented below.

* `is_trial_stopped_early` -
  True if the trial is stopped early.

* `built_in_algorithm_output` -
  (Optional)
  Details related to built-in algorithms jobs. Only set for trials of built-in algorithms jobs that have succeeded.  Structure is documented below.


The `final_metric` block supports:

* `training_step` -
  The global training step for this metric

* `objective_value` -
  The objective value at this training step.

The `built_in_algorithm_output` block supports:

* `framework` -
  Framework on which the built-in algorithm was trained.

* `runtime_vresion` -
  AI Platform runtime version on which the built-in algorithm was trained

* `python_version` -
  Python version on which the built-in algorithm was trained.

* `model_path` -
  The Cloud Storage path to the model/ directory where the training job saves the trained model. 
  Only set for successful jobs that don't use hyperparameter tuning

The `built_in_algorithm_output` block supports:

* `framework` -
  (Optional)
  Framework on which the built-in algorithm was trained.

* `runtime_version` -
  (Optional)
  AI Platform runtime version on which the built-in algorithm was trained

* `python_version` -
  (Optional)
  Python version on which the built-in algorithm was trained.

* `model_path` -
  (Optional)
  The Cloud Storage path to the model/ directory where the training job saves the trained model. 
  Only set for successful jobs that don't use hyperparameter tuning

## Attributes Reference

In addition to the arguments listed above, the following computed attributes are exported:


* `create_time` -
  When the job was created.

* `end_time` -
  When the job processing was completed.

* `state` -
  The detailed state of a job.

* `error_message` -
  The details of a failure or a cancellation.


## Timeouts

This resource provides the following
[Timeouts](/docs/configuration/resources.html#timeouts) configuration options:

- `create` - Default is 4 minutes.
- `delete` - Default is 4 minutes.

## Import

Job can be imported using any of these accepted formats:

```
$ terraform import google_ml_engine_job.default projects/{{project}}/jobs/{{name}}
$ terraform import google_ml_engine_job.default {{project}}/{{name}}
$ terraform import google_ml_engine_job.default {{name}}
```

-> If you're importing a resource with beta features, make sure to include `-provider=google-beta`
as an argument so that Terraform uses the correct provider to import your resource.

## User Project Overrides

This resource supports [User Project Overrides](https://www.terraform.io/docs/providers/google/guides/provider_reference.html#user_project_override).
