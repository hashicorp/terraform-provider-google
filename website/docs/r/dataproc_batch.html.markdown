---
# ----------------------------------------------------------------------------
#
#     ***     AUTO GENERATED CODE    ***    Type: MMv1     ***
#
# ----------------------------------------------------------------------------
#
#     This code is generated by Magic Modules using the following:
#
#     Configuration: https:#github.com/GoogleCloudPlatform/magic-modules/tree/main/mmv1/products/dataproc/Batch.yaml
#     Template:      https:#github.com/GoogleCloudPlatform/magic-modules/tree/main/mmv1/templates/terraform/resource.html.markdown.tmpl
#
#     DO NOT EDIT this file directly. Any changes made to this file will be
#     overwritten during the next generation cycle.
#
# ----------------------------------------------------------------------------
subcategory: "Dataproc"
description: |-
  Dataproc Serverless Batches lets you run Spark workloads without requiring you to
  provision and manage your own Dataproc cluster.
---

# google_dataproc_batch

Dataproc Serverless Batches lets you run Spark workloads without requiring you to
provision and manage your own Dataproc cluster.


To get more information about Batch, see:

* [API documentation](https://cloud.google.com/dataproc-serverless/docs/reference/rest/v1/projects.locations.batches)
* How-to Guides
    * [Dataproc Serverless Batches Intro](https://cloud.google.com/dataproc-serverless/docs/overview)

## Example Usage - Dataproc Batch Spark


```hcl
resource "google_dataproc_batch" "example_batch_spark" {

    batch_id      = "tf-test-batch%{random_suffix}"
    location      = "us-central1"
    labels        = {"batch_test": "terraform"}

    runtime_config {
      properties    = { "spark.dynamicAllocation.enabled": "false", "spark.executor.instances": "2" }
    }

    environment_config {
      execution_config {
        subnetwork_uri = "default"
        ttl            = "3600s"
        network_tags   = ["tag1"]
      }
    }

    spark_batch {
      main_class    = "org.apache.spark.examples.SparkPi"
      args          = ["10"]
      jar_file_uris = ["file:///usr/lib/spark/examples/jars/spark-examples.jar"]
    }
}
```
## Example Usage - Dataproc Batch Spark Full


```hcl
data "google_project" "project" {
}

data "google_storage_project_service_account" "gcs_account" {
}

resource "google_dataproc_batch" "example_batch_spark" {
    batch_id      = "dataproc-batch"
    location      = "us-central1"
    labels        = {"batch_test": "terraform"}

    runtime_config {
      properties    = { "spark.dynamicAllocation.enabled": "false", "spark.executor.instances": "2" }
      version = "2.2"
    }

    environment_config {
      execution_config {
        ttl = "3600s"
        network_tags = ["tag1"]
        kms_key = "example-key"
        network_uri = "default"
        service_account = "${data.google_project.project.number}-compute@developer.gserviceaccount.com"
        staging_bucket = google_storage_bucket.bucket.name
        authentication_config {
          user_workload_authentication_type = "SERVICE_ACCOUNT"
        }
      }
      peripherals_config {
        metastore_service = google_dataproc_metastore_service.ms.name
        spark_history_server_config {
          dataproc_cluster = google_dataproc_cluster.basic.id
        }
      }
    }

    spark_batch {
      main_class    = "org.apache.spark.examples.SparkPi"
      args          = ["10"]
      jar_file_uris = ["file:///usr/lib/spark/examples/jars/spark-examples.jar"]
    }

    depends_on = [
      google_kms_crypto_key_iam_member.crypto_key_member_1,
    ]
}

resource "google_storage_bucket" "bucket" {
  uniform_bucket_level_access = true
  name                        = "dataproc-bucket"
  location                    = "US"
  force_destroy               = true
}

resource "google_kms_crypto_key_iam_member" "crypto_key_member_1" {
  crypto_key_id = "example-key"
  role          = "roles/cloudkms.cryptoKeyEncrypterDecrypter"
  member        = "serviceAccount:service-${data.google_project.project.number}@dataproc-accounts.iam.gserviceaccount.com"
}

resource "google_dataproc_cluster" "basic" {
  name   = "dataproc-batch"
  region = "us-central1"

  cluster_config {
    # Keep the costs down with smallest config we can get away with
    software_config {
      override_properties = {
        "dataproc:dataproc.allow.zero.workers" = "true"
        "spark:spark.history.fs.logDirectory"  = "gs://${google_storage_bucket.bucket.name}/*/spark-job-history"
      }
    }
 
    endpoint_config {
      enable_http_port_access = true
    }

    master_config {
      num_instances = 1
      machine_type  = "e2-standard-2"
      disk_config {
        boot_disk_size_gb = 35
      }
    }

    metastore_config {
      dataproc_metastore_service = google_dataproc_metastore_service.ms.name
    }
  }   
}

 resource "google_dataproc_metastore_service" "ms" {
  service_id = "dataproc-batch"
  location   = "us-central1"
  port       = 9080
  tier       = "DEVELOPER"

  maintenance_window {
    hour_of_day = 2
    day_of_week = "SUNDAY"
  }

  hive_metastore_config {
    version = "3.1.2"
  }
}
```
## Example Usage - Dataproc Batch Sparksql


```hcl
resource "google_dataproc_batch" "example_batch_sparsql" {

    batch_id      = "tf-test-batch%{random_suffix}"
    location      = "us-central1"

    runtime_config {
      properties    = { "spark.dynamicAllocation.enabled": "false", "spark.executor.instances": "2" }
    }

    environment_config {
      execution_config {
        subnetwork_uri = "default"
      }
    }

    spark_sql_batch {
      query_file_uri   = "gs://dataproc-examples/spark-sql/natality/cigarette_correlations.sql"
      jar_file_uris    = ["file:///usr/lib/spark/examples/jars/spark-examples.jar"]
      query_variables  = {
        name = "value"
      }
    }
}
```
## Example Usage - Dataproc Batch Pyspark


```hcl
resource "google_dataproc_batch" "example_batch_pyspark" {
    batch_id      = "tf-test-batch%{random_suffix}"
    location      = "us-central1"

    runtime_config {
      properties    = { "spark.dynamicAllocation.enabled": "false", "spark.executor.instances": "2" }
    }

    environment_config {
      execution_config {
        subnetwork_uri = "default"
      }
    }

    pyspark_batch {
      main_python_file_uri = "https://storage.googleapis.com/terraform-batches/test_util.py"
      args                 = ["10"]
      jar_file_uris        = ["file:///usr/lib/spark/examples/jars/spark-examples.jar"]
      python_file_uris     = ["gs://dataproc-examples/pyspark/hello-world/hello-world.py"]
      archive_uris         = [
        "https://storage.googleapis.com/terraform-batches/animals.txt.tar.gz#unpacked",
        "https://storage.googleapis.com/terraform-batches/animals.txt.jar",
        "https://storage.googleapis.com/terraform-batches/animals.txt"
      ]
      file_uris            = ["https://storage.googleapis.com/terraform-batches/people.txt"]
    }
}
```
## Example Usage - Dataproc Batch Sparkr


```hcl
resource "google_dataproc_batch" "example_batch_sparkr" {

    batch_id      = "tf-test-batch%{random_suffix}"
    location      = "us-central1"
    labels        = {"batch_test": "terraform"}

    runtime_config {
      properties    = { "spark.dynamicAllocation.enabled": "false", "spark.executor.instances": "2" }
    }

    environment_config {
      execution_config {
        subnetwork_uri = "default"
        ttl            = "3600s"
        network_tags   = ["tag1"]
      }
    }

    spark_r_batch {
      main_r_file_uri  = "https://storage.googleapis.com/terraform-batches/spark-r-flights.r"
      args             = ["https://storage.googleapis.com/terraform-batches/flights.csv"]
    }
}
```
## Example Usage - Dataproc Batch Autotuning


```hcl
resource "google_dataproc_batch" "example_batch_autotuning" {

    batch_id      = "tf-test-batch%{random_suffix}"
    location      = "us-central1"
    labels        = {"batch_test": "terraform"}

    runtime_config {
      version       = "2.2"
      properties    = { "spark.dynamicAllocation.enabled": "false", "spark.executor.instances": "2" }
      cohort        = "tf-dataproc-batch-example"
      autotuning_config {
        scenarios = ["SCALING", "MEMORY"]
      }
    }

    environment_config {
      execution_config {
        subnetwork_uri = "default"
        ttl            = "3600s"
      }
    }

    spark_batch {
      main_class    = "org.apache.spark.examples.SparkPi"
      args          = ["10"]
      jar_file_uris = ["file:///usr/lib/spark/examples/jars/spark-examples.jar"]
    }
}
```

## Argument Reference

The following arguments are supported:



* `labels` -
  (Optional)
  The labels to associate with this batch.

  **Note**: This field is non-authoritative, and will only manage the labels present in your configuration.
  Please refer to the field `effective_labels` for all of the labels present on the resource.

* `runtime_config` -
  (Optional)
  Runtime configuration for the batch execution.
  Structure is [documented below](#nested_runtime_config).

* `environment_config` -
  (Optional)
  Environment configuration for the batch execution.
  Structure is [documented below](#nested_environment_config).

* `pyspark_batch` -
  (Optional)
  PySpark batch config.
  Structure is [documented below](#nested_pyspark_batch).

* `spark_batch` -
  (Optional)
  Spark batch config.
  Structure is [documented below](#nested_spark_batch).

* `spark_r_batch` -
  (Optional)
  SparkR batch config.
  Structure is [documented below](#nested_spark_r_batch).

* `spark_sql_batch` -
  (Optional)
  Spark SQL batch config.
  Structure is [documented below](#nested_spark_sql_batch).

* `location` -
  (Optional)
  The location in which the batch will be created in.

* `batch_id` -
  (Optional)
  The ID to use for the batch, which will become the final component of the batch's resource name.
  This value must be 4-63 characters. Valid characters are /[a-z][0-9]-/.

* `project` - (Optional) The ID of the project in which the resource belongs.
    If it is not provided, the provider project is used.



<a name="nested_runtime_config"></a>The `runtime_config` block supports:

* `version` -
  (Optional)
  Version of the batch runtime.

* `container_image` -
  (Optional)
  Optional custom container image for the job runtime environment. If not specified, a default container image will be used.

* `properties` -
  (Optional)
  A mapping of property names to values, which are used to configure workload execution.

* `effective_properties` -
  (Output)
  A mapping of property names to values, which are used to configure workload execution.

* `autotuning_config` -
  (Optional)
  Optional. Autotuning configuration of the workload.
  Structure is [documented below](#nested_runtime_config_autotuning_config).

* `cohort` -
  (Optional)
  Optional. Cohort identifier. Identifies families of the workloads having the same shape, e.g. daily ETL jobs.


<a name="nested_runtime_config_autotuning_config"></a>The `autotuning_config` block supports:

* `scenarios` -
  (Optional)
  Optional. Scenarios for which tunings are applied.
  Each value may be one of: `SCALING`, `BROADCAST_HASH_JOIN`, `MEMORY`.

<a name="nested_environment_config"></a>The `environment_config` block supports:

* `execution_config` -
  (Optional)
  Execution configuration for a workload.
  Structure is [documented below](#nested_environment_config_execution_config).

* `peripherals_config` -
  (Optional)
  Peripherals configuration that workload has access to.
  Structure is [documented below](#nested_environment_config_peripherals_config).


<a name="nested_environment_config_execution_config"></a>The `execution_config` block supports:

* `service_account` -
  (Optional)
  Service account that used to execute workload.

* `network_tags` -
  (Optional)
  Tags used for network traffic control.

* `kms_key` -
  (Optional)
  The Cloud KMS key to use for encryption.

* `ttl` -
  (Optional)
  The duration after which the workload will be terminated.
  When the workload exceeds this duration, it will be unconditionally terminated without waiting for ongoing
  work to finish. If ttl is not specified for a batch workload, the workload will be allowed to run until it
  exits naturally (or run forever without exiting). If ttl is not specified for an interactive session,
  it defaults to 24 hours. If ttl is not specified for a batch that uses 2.1+ runtime version, it defaults to 4 hours.
  Minimum value is 10 minutes; maximum value is 14 days. If both ttl and idleTtl are specified (for an interactive session),
  the conditions are treated as OR conditions: the workload will be terminated when it has been idle for idleTtl or
  when ttl has been exceeded, whichever occurs first.

* `staging_bucket` -
  (Optional)
  A Cloud Storage bucket used to stage workload dependencies, config files, and store
  workload output and other ephemeral data, such as Spark history files. If you do not specify a staging bucket,
  Cloud Dataproc will determine a Cloud Storage location according to the region where your workload is running,
  and then create and manage project-level, per-location staging and temporary buckets.
  This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.

* `network_uri` -
  (Optional)
  Network configuration for workload execution.

* `subnetwork_uri` -
  (Optional)
  Subnetwork configuration for workload execution.

* `authentication_config` -
  (Optional)
  Authentication configuration for a workload is used to set the default identity for the workload execution.
  Structure is [documented below](#nested_environment_config_execution_config_authentication_config).


<a name="nested_environment_config_execution_config_authentication_config"></a>The `authentication_config` block supports:

* `user_workload_authentication_type` -
  (Optional)
  Authentication type for the user workload running in containers.
  Possible values are: `SERVICE_ACCOUNT`, `END_USER_CREDENTIALS`.

<a name="nested_environment_config_peripherals_config"></a>The `peripherals_config` block supports:

* `metastore_service` -
  (Optional)
  Resource name of an existing Dataproc Metastore service.

* `spark_history_server_config` -
  (Optional)
  The Spark History Server configuration for the workload.
  Structure is [documented below](#nested_environment_config_peripherals_config_spark_history_server_config).


<a name="nested_environment_config_peripherals_config_spark_history_server_config"></a>The `spark_history_server_config` block supports:

* `dataproc_cluster` -
  (Optional)
  Resource name of an existing Dataproc Cluster to act as a Spark History Server for the workload.

<a name="nested_pyspark_batch"></a>The `pyspark_batch` block supports:

* `main_python_file_uri` -
  (Optional)
  The HCFS URI of the main Python file to use as the Spark driver. Must be a .py file.

* `args` -
  (Optional)
  The arguments to pass to the driver. Do not include arguments that can be set as batch
  properties, such as --conf, since a collision can occur that causes an incorrect batch submission.

* `python_file_uris` -
  (Optional)
  HCFS file URIs of Python files to pass to the PySpark framework.
  Supported file types: .py, .egg, and .zip.

* `jar_file_uris` -
  (Optional)
  HCFS URIs of jar files to add to the classpath of the Spark driver and tasks.

* `file_uris` -
  (Optional)
  HCFS URIs of files to be placed in the working directory of each executor.

* `archive_uris` -
  (Optional)
  HCFS URIs of archives to be extracted into the working directory of each executor.
  Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.

<a name="nested_spark_batch"></a>The `spark_batch` block supports:

* `args` -
  (Optional)
  The arguments to pass to the driver. Do not include arguments that can be set as batch
  properties, such as --conf, since a collision can occur that causes an incorrect batch submission.

* `jar_file_uris` -
  (Optional)
  HCFS URIs of jar files to add to the classpath of the Spark driver and tasks.

* `file_uris` -
  (Optional)
  HCFS URIs of files to be placed in the working directory of each executor.

* `archive_uris` -
  (Optional)
  HCFS URIs of archives to be extracted into the working directory of each executor.
  Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.

* `main_jar_file_uri` -
  (Optional)
  The HCFS URI of the jar file that contains the main class.

* `main_class` -
  (Optional)
  The name of the driver main class. The jar file that contains the class must be in the
  classpath or specified in jarFileUris.

<a name="nested_spark_r_batch"></a>The `spark_r_batch` block supports:

* `main_r_file_uri` -
  (Optional)
  The HCFS URI of the main R file to use as the driver. Must be a .R or .r file.

* `args` -
  (Optional)
  The arguments to pass to the driver. Do not include arguments that can be set as batch
  properties, such as --conf, since a collision can occur that causes an incorrect batch submission.

* `file_uris` -
  (Optional)
  HCFS URIs of files to be placed in the working directory of each executor.

* `archive_uris` -
  (Optional)
  HCFS URIs of archives to be extracted into the working directory of each executor.
  Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.

<a name="nested_spark_sql_batch"></a>The `spark_sql_batch` block supports:

* `query_file_uri` -
  (Optional)
  The HCFS URI of the script that contains Spark SQL queries to execute.

* `jar_file_uris` -
  (Optional)
  HCFS URIs of jar files to be added to the Spark CLASSPATH.

* `query_variables` -
  (Optional)
  Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).

## Attributes Reference

In addition to the arguments listed above, the following computed attributes are exported:

* `id` - an identifier for the resource with format `projects/{{project}}/locations/{{location}}/batches/{{batch_id}}`

* `name` -
  The resource name of the batch.

* `uuid` -
  A batch UUID (Unique Universal Identifier). The service generates this value when it creates the batch.

* `create_time` -
  The time when the batch was created.

* `runtime_info` -
  Runtime information about batch execution.
  Structure is [documented below](#nested_runtime_info).

* `state` -
  The state of the batch. For possible values, see the [API documentation](https://cloud.google.com/dataproc-serverless/docs/reference/rest/v1/projects.locations.batches#State).

* `state_message` -
  Batch state details, such as a failure description if the state is FAILED.

* `state_time` -
  Batch state details, such as a failure description if the state is FAILED.

* `creator` -
  The email address of the user who created the batch.

* `operation` -
  The resource name of the operation associated with this batch.

* `state_history` -
  Historical state information for the batch.
  Structure is [documented below](#nested_state_history).

* `terraform_labels` -
  The combination of labels configured directly on the resource
   and default labels configured on the provider.

* `effective_labels` -
  All of labels (key/value pairs) present on the resource in GCP, including the labels configured through Terraform, other clients and services.


<a name="nested_runtime_info"></a>The `runtime_info` block contains:

* `output_uri` -
  (Output)
  A URI pointing to the location of the stdout and stderr of the workload.

* `diagnostic_output_uri` -
  (Output)
  A URI pointing to the location of the diagnostics tarball.

* `endpoints` -
  (Output)
  Map of remote access endpoints (such as web interfaces and APIs) to their URIs.

* `approximate_usage` -
  (Output)
  Approximate workload resource usage, calculated when the workload completes(see [Dataproc Serverless pricing](https://cloud.google.com/dataproc-serverless/pricing))
  Structure is [documented below](#nested_runtime_info_approximate_usage).

* `current_usage` -
  (Output)
  Snapshot of current workload resource usage(see [Dataproc Serverless pricing](https://cloud.google.com/dataproc-serverless/pricing))
  Structure is [documented below](#nested_runtime_info_current_usage).


<a name="nested_runtime_info_approximate_usage"></a>The `approximate_usage` block contains:

* `milli_dcu_seconds` -
  (Output)
  DCU (Dataproc Compute Units) usage in (milliDCU x seconds)

* `shuffle_storage_gb_seconds` -
  (Output)
  Shuffle storage usage in (GB x seconds)

* `milli_accelerator_seconds` -
  (Output)
  Accelerator usage in (milliAccelerator x seconds)

* `accelerator_type` -
  (Output)
  Accelerator type being used, if any

<a name="nested_runtime_info_current_usage"></a>The `current_usage` block contains:

* `milli_dcu` -
  (Output)
  Milli (one-thousandth) Dataproc Compute Units (DCUs).

* `shuffle_storage_gb` -
  (Output)
  Shuffle Storage in gigabytes (GB).

* `milli_dcu_premium` -
  (Output)
  Milli (one-thousandth) Dataproc Compute Units (DCUs) charged at premium tier.

* `shuffle_storage_gb_premium` -
  (Output)
  Shuffle Storage in gigabytes (GB) charged at premium tier.

* `milli_accelerator` -
  (Output)
  Milli (one-thousandth) accelerator..

* `accelerator_type` -
  (Output)
  Accelerator type being used, if any.

* `snapshot_time` -
  (Output)
  The timestamp of the usage snapshot.

<a name="nested_state_history"></a>The `state_history` block contains:

* `state` -
  (Output)
  The state of the batch at this point in history. For possible values, see the [API documentation](https://cloud.google.com/dataproc-serverless/docs/reference/rest/v1/projects.locations.batches#State).

* `state_message` -
  (Output)
  Details about the state at this point in history.

* `state_start_time` -
  (Output)
  The time when the batch entered the historical state.

## Timeouts

This resource provides the following
[Timeouts](https://developer.hashicorp.com/terraform/plugin/sdkv2/resources/retries-and-customizable-timeouts) configuration options:

- `create` - Default is 10 minutes.
- `update` - Default is 20 minutes.
- `delete` - Default is 5 minutes.

## Import


Batch can be imported using any of these accepted formats:

* `projects/{{project}}/locations/{{location}}/batches/{{batch_id}}`
* `{{project}}/{{location}}/{{batch_id}}`
* `{{location}}/{{batch_id}}`


In Terraform v1.5.0 and later, use an [`import` block](https://developer.hashicorp.com/terraform/language/import) to import Batch using one of the formats above. For example:

```tf
import {
  id = "projects/{{project}}/locations/{{location}}/batches/{{batch_id}}"
  to = google_dataproc_batch.default
}
```

When using the [`terraform import` command](https://developer.hashicorp.com/terraform/cli/commands/import), Batch can be imported using one of the formats above. For example:

```
$ terraform import google_dataproc_batch.default projects/{{project}}/locations/{{location}}/batches/{{batch_id}}
$ terraform import google_dataproc_batch.default {{project}}/{{location}}/{{batch_id}}
$ terraform import google_dataproc_batch.default {{location}}/{{batch_id}}
```

## User Project Overrides

This resource supports [User Project Overrides](https://registry.terraform.io/providers/hashicorp/google/latest/docs/guides/provider_reference#user_project_override).
