---
# ----------------------------------------------------------------------------
#
#     ***     AUTO GENERATED CODE    ***    Type: MMv1     ***
#
# ----------------------------------------------------------------------------
#
#     This file is automatically generated by Magic Modules and manual
#     changes will be clobbered when the file is regenerated.
#
#     Please read more about how to change this file in
#     .github/CONTRIBUTING.md.
#
# ----------------------------------------------------------------------------
subcategory: "Google Distributed Cloud Edge"
description: |-
  "A set of Kubernetes nodes in a cluster with common configuration and specification.
---

# google\_edgecontainer\_node\_pool

"A set of Kubernetes nodes in a cluster with common configuration and specification."


To get more information about NodePool, see:

* [API documentation](https://cloud.google.com/distributed-cloud/edge/latest/docs/reference/container/rest/v1/projects.locations.clusters.nodePools)
* How-to Guides
    * [Google Distributed Cloud Edge](https://cloud.google.com/distributed-cloud/edge/latest/docs)

## Example Usage - Edgecontainer Node Pool


```hcl
resource "google_edgecontainer_cluster" "cluster" {
  name = "default"
  location = "us-central1"

  authorization {
    admin_users {
      username = "admin@hashicorptest.com"
    }
  }

  networking {
    cluster_ipv4_cidr_blocks = ["10.0.0.0/16"]
    services_ipv4_cidr_blocks = ["10.1.0.0/16"]
  }

  fleet {
    project = "projects/${data.google_project.project.number}"
  }
}

resource "google_edgecontainer_node_pool" "default" {
  name = "nodepool-1"
  cluster = google_edgecontainer_cluster.cluster.name
  location = "us-central1"
  node_location = "us-central1-edge-example-edgesite"
  node_count = 3

  labels = {
    my_key    = "my_val"
    other_key = "other_val"
  }
}

data "google_project" "project" {}
```
## Example Usage - Edgecontainer Node Pool With Cmek


```hcl
resource "google_edgecontainer_cluster" "cluster" {
  name = "default"
  location = "us-central1"

  authorization {
    admin_users {
      username = "admin@hashicorptest.com"
    }
  }

  networking {
    cluster_ipv4_cidr_blocks = ["10.0.0.0/16"]
    services_ipv4_cidr_blocks = ["10.1.0.0/16"]
  }

  fleet {
    project = "projects/${data.google_project.project.number}"
  }
}

resource "google_kms_crypto_key_iam_member" "crypto_key" {
  crypto_key_id = google_kms_crypto_key.crypto_key.id
  role          = "roles/cloudkms.cryptoKeyEncrypterDecrypter"
  member        = "serviceAccount:service-${data.google_project.project.number}@gcp-sa-edgecontainer.iam.gserviceaccount.com"
}

resource "google_kms_crypto_key" "crypto_key" {
  name     = "key"
  key_ring = google_kms_key_ring.key_ring.id
}

resource "google_kms_key_ring" "key_ring" {
  name     = "keyring"
  location = "us-central1"
}

resource "google_edgecontainer_node_pool" "default" {
  depends_on = [google_kms_crypto_key_iam_member.crypto_key]

  name = "nodepool-1"
  cluster = google_edgecontainer_cluster.cluster.name
  location = "us-central1"
  node_location = "us-central1-edge-example-edgesite"
  node_count = 3

  local_disk_encryption {
    kms_key = google_kms_crypto_key.crypto_key.id
  }
}

data "google_project" "project" {}
```
## Example Usage - Edgecontainer Local Control Plane Node Pool


```hcl
resource "google_edgecontainer_cluster" "default" {
  name = ""
  location = "us-central1"

  authorization {
    admin_users {
      username = "admin@hashicorptest.com"
    }
  }

  networking {
    cluster_ipv4_cidr_blocks = ["10.0.0.0/16"]
    services_ipv4_cidr_blocks = ["10.1.0.0/16"]
  }

  fleet {
    project = "projects/${data.google_project.project.number}"
  }

  external_load_balancer_ipv4_address_pools = ["10.100.0.0-10.100.0.10"]

  control_plane {
    local {
      node_location = "us-central1-edge-example-edgesite"
      node_count = 1
      machine_filter = "machine-name"
      shared_deployment_policy = "ALLOWED"
    }
  }
}

resource "google_edgecontainer_node_pool" "default" {
  name = "nodepool-1"
  cluster = google_edgecontainer_cluster.cluster.name
  location = "us-central1"
  node_location = "us-central1-edge-example-edgesite"
  node_count = 3
}

data "google_project" "project" {}
```

## Argument Reference

The following arguments are supported:


* `node_location` -
  (Required)
  Name of the Google Distributed Cloud Edge zone where this node pool will be created. For example: `us-central1-edge-customer-a`.

* `node_count` -
  (Required)
  The number of nodes in the pool.

* `name` -
  (Required)
  The resource name of the node pool.

* `location` -
  (Required)
  The location of the resource.

* `cluster` -
  (Required)
  The name of the target Distributed Cloud Edge Cluster.


- - -


* `labels` -
  (Optional)
  Labels associated with this resource.
  **Note**: This field is non-authoritative, and will only manage the labels present in your configuration.
  Please refer to the field `effective_labels` for all of the labels present on the resource.

* `machine_filter` -
  (Optional)
  Only machines matching this filter will be allowed to join the node pool.
  The filtering language accepts strings like "name=<name>", and is
  documented in more detail in [AIP-160](https://google.aip.dev/160).

* `local_disk_encryption` -
  (Optional)
  Local disk encryption options. This field is only used when enabling CMEK support.
  Structure is [documented below](#nested_local_disk_encryption).

* `node_config` -
  (Optional)
  Configuration for each node in the NodePool
  Structure is [documented below](#nested_node_config).

* `project` - (Optional) The ID of the project in which the resource belongs.
    If it is not provided, the provider project is used.


<a name="nested_local_disk_encryption"></a>The `local_disk_encryption` block supports:

* `kms_key` -
  (Optional)
  The Cloud KMS CryptoKey e.g. projects/{project}/locations/{location}/keyRings/{keyRing}/cryptoKeys/{cryptoKey} to use for protecting node local disks.
  If not specified, a Google-managed key will be used instead.

* `kms_key_active_version` -
  (Output)
  The Cloud KMS CryptoKeyVersion currently in use for protecting node local disks. Only applicable if kmsKey is set.

* `kms_key_state` -
  (Output)
  Availability of the Cloud KMS CryptoKey. If not KEY_AVAILABLE, then nodes may go offline as they cannot access their local data.
  This can be caused by a lack of permissions to use the key, or if the key is disabled or deleted.

<a name="nested_node_config"></a>The `node_config` block supports:

* `labels` -
  (Optional)
  "The Kubernetes node labels"

## Attributes Reference

In addition to the arguments listed above, the following computed attributes are exported:

* `id` - an identifier for the resource with format `projects/{{project}}/locations/{{location}}/clusters/{{cluster}}/nodePools/{{name}}`

* `create_time` -
  The time when the node pool was created.

* `update_time` -
  The time when the node pool was last updated.

* `node_version` -
  The lowest release version among all worker nodes.

* `terraform_labels` -
  The combination of labels configured directly on the resource
   and default labels configured on the provider.

* `effective_labels` -
  All of labels (key/value pairs) present on the resource in GCP, including the labels configured through Terraform, other clients and services.


## Timeouts

This resource provides the following
[Timeouts](https://developer.hashicorp.com/terraform/plugin/sdkv2/resources/retries-and-customizable-timeouts) configuration options:

- `create` - Default is 60 minutes.
- `update` - Default is 60 minutes.
- `delete` - Default is 60 minutes.

## Import


NodePool can be imported using any of these accepted formats:

* `projects/{{project}}/locations/{{location}}/clusters/{{cluster}}/nodePools/{{name}}`
* `{{project}}/{{location}}/{{cluster}}/{{name}}`
* `{{location}}/{{cluster}}/{{name}}`


In Terraform v1.5.0 and later, use an [`import` block](https://developer.hashicorp.com/terraform/language/import) to import NodePool using one of the formats above. For example:

```tf
import {
  id = "projects/{{project}}/locations/{{location}}/clusters/{{cluster}}/nodePools/{{name}}"
  to = google_edgecontainer_node_pool.default
}
```

When using the [`terraform import` command](https://developer.hashicorp.com/terraform/cli/commands/import), NodePool can be imported using one of the formats above. For example:

```
$ terraform import google_edgecontainer_node_pool.default projects/{{project}}/locations/{{location}}/clusters/{{cluster}}/nodePools/{{name}}
$ terraform import google_edgecontainer_node_pool.default {{project}}/{{location}}/{{cluster}}/{{name}}
$ terraform import google_edgecontainer_node_pool.default {{location}}/{{cluster}}/{{name}}
```

## User Project Overrides

This resource supports [User Project Overrides](https://registry.terraform.io/providers/hashicorp/google/latest/docs/guides/provider_reference#user_project_override).
