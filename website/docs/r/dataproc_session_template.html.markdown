---
# ----------------------------------------------------------------------------
#
#     ***     AUTO GENERATED CODE    ***    Type: MMv1     ***
#
# ----------------------------------------------------------------------------
#
#     This code is generated by Magic Modules using the following:
#
#     Configuration: https:#github.com/GoogleCloudPlatform/magic-modules/tree/main/mmv1/products/dataproc/SessionTemplate.yaml
#     Template:      https:#github.com/GoogleCloudPlatform/magic-modules/tree/main/mmv1/templates/terraform/resource.html.markdown.tmpl
#
#     DO NOT EDIT this file directly. Any changes made to this file will be
#     overwritten during the next generation cycle.
#
# ----------------------------------------------------------------------------
subcategory: "Dataproc"
description: |-
  A Dataproc Serverless session template defines the configuration settings for
  creating one or more Dataproc Serverless interactive sessions.
---

# google_dataproc_session_template

A Dataproc Serverless session template defines the configuration settings for
creating one or more Dataproc Serverless interactive sessions.


To get more information about SessionTemplate, see:

* [API documentation](https://cloud.google.com/dataproc-serverless/docs/reference/rest/v1/projects.locations.sessionTemplates)
* How-to Guides
    * [Dataproc Serverless Session Templates](https://cloud.google.com/dataproc-serverless/docs/guides/create-serverless-sessions-templates#create-dataproc-serverless-session-template)

## Example Usage - Dataproc Session Templates Jupyter


```hcl
resource "google_dataproc_session_template" "example_session_templates_jupyter" {
    name     = "projects/my-project-name/locations/us-central1/sessionTemplates/jupyter-session-template"
    location = "us-central1"
    labels   = {"session_template_test": "terraform"}

    runtime_config {
      properties    = { "spark.dynamicAllocation.enabled": "false", "spark.executor.instances": "2" }
    }

    environment_config {
      execution_config {
        subnetwork_uri = "default"
        ttl            = "3600s"
        network_tags   = ["tag1"]
      }
    }

    jupyter_session {
      kernel       = "PYTHON"
      display_name = "tf python kernel"
    }
}
```
## Example Usage - Dataproc Session Templates Jupyter Full


```hcl
data "google_project" "project" {
}

data "google_storage_project_service_account" "gcs_account" {
}

resource "google_dataproc_session_template" "dataproc_session_templates_jupyter_full" {
    name     = "projects/my-project-name/locations/us-central1/sessionTemplates/jupyter-session-template"
    location      = "us-central1"
    labels        = {"session_template_test": "terraform"}

    runtime_config {
      properties    = { "spark.dynamicAllocation.enabled": "false", "spark.executor.instances": "2" }
      version = "2.2"
      container_image = "us-docker.pkg.dev/my-project-name/s8s-spark-test-images/s8s-spark:latest"
    }

    environment_config {
      execution_config {
        ttl = "3600s"
        network_tags = ["tag1"]
        kms_key = "example-key"
	subnetwork_uri = "default"
        service_account = "${data.google_project.project.number}-compute@developer.gserviceaccount.com"
        staging_bucket = google_storage_bucket.bucket.name
      }
      peripherals_config {
        metastore_service = google_dataproc_metastore_service.ms.name
        spark_history_server_config {
          dataproc_cluster = google_dataproc_cluster.basic.id
        }
      }
    }

    jupyter_session {
      kernel       = "PYTHON"
      display_name = "tf python kernel"
    }

    depends_on = [
      google_kms_crypto_key_iam_member.crypto_key_member_1,
    ]
}

resource "google_storage_bucket" "bucket" {
  uniform_bucket_level_access = true
  name                        = "dataproc-bucket"
  location                    = "US"
  force_destroy               = true
}

resource "google_kms_crypto_key_iam_member" "crypto_key_member_1" {
  crypto_key_id = "example-key"
  role          = "roles/cloudkms.cryptoKeyEncrypterDecrypter"
  member        = "serviceAccount:service-${data.google_project.project.number}@dataproc-accounts.iam.gserviceaccount.com"
}

resource "google_dataproc_cluster" "basic" {
  name   = "jupyter-session-template"
  region = "us-central1"

  cluster_config {
    # Keep the costs down with smallest config we can get away with
    software_config {
      override_properties = {
        "dataproc:dataproc.allow.zero.workers" = "true"
        "spark:spark.history.fs.logDirectory"  = "gs://${google_storage_bucket.bucket.name}/*/spark-job-history"
      }
    }

    gce_cluster_config {
      subnetwork = "default"
    }

    endpoint_config {
      enable_http_port_access = true
    }

    master_config {
      num_instances = 1
      machine_type  = "e2-standard-2"
      disk_config {
        boot_disk_size_gb = 35
      }
    }

    metastore_config {
      dataproc_metastore_service = google_dataproc_metastore_service.ms.name
    }
  }
}

resource "google_dataproc_metastore_service" "ms" {
  service_id = "jupyter-session-template"
  location   = "us-central1"
  port       = 9080
  tier       = "DEVELOPER"

  maintenance_window {
    hour_of_day = 2
    day_of_week = "SUNDAY"
  }

  hive_metastore_config {
    version = "3.1.2"
  }

  network_config {
    consumers {
      subnetwork = "projects/my-project-name/regions/us-central1/subnetworks/default"
    }
  }
}
```
## Example Usage - Dataproc Session Templates Spark Connect


```hcl
resource "google_dataproc_session_template" "example_session_templates_spark_connect" {
    name     = "projects/my-project-name/locations/us-central1/sessionTemplates/sc-session-template"
    location      = "us-central1"
    labels        = {"session_template_test": "terraform"}

    runtime_config {
      properties    = { "spark.dynamicAllocation.enabled": "false", "spark.executor.instances": "2" }
    }

    environment_config {
      execution_config {
        subnetwork_uri = "default"
        ttl            = "3600s"
        network_tags   = ["tag1"]
      }
    }
}
```

## Argument Reference

The following arguments are supported:


* `name` -
  (Required)
  The resource name of the session template in the following format:
  projects/{project}/locations/{location}/sessionTemplates/{template_id}


* `labels` -
  (Optional)
  The labels to associate with this session template.

  **Note**: This field is non-authoritative, and will only manage the labels present in your configuration.
  Please refer to the field `effective_labels` for all of the labels present on the resource.

* `runtime_config` -
  (Optional)
  Runtime configuration for the session template.
  Structure is [documented below](#nested_runtime_config).

* `environment_config` -
  (Optional)
  Environment configuration for the session execution.
  Structure is [documented below](#nested_environment_config).

* `jupyter_session` -
  (Optional)
  Jupyter configuration for an interactive session.
  Structure is [documented below](#nested_jupyter_session).

* `spark_connect_session` -
  (Optional)
  Spark connect configuration for an interactive session.

* `location` -
  (Optional)
  The location in which the session template will be created in.

* `project` - (Optional) The ID of the project in which the resource belongs.
    If it is not provided, the provider project is used.



<a name="nested_runtime_config"></a>The `runtime_config` block supports:

* `version` -
  (Optional)
  Version of the session runtime.

* `container_image` -
  (Optional)
  Optional custom container image for the job runtime environment. If not specified, a default container image will be used.

* `properties` -
  (Optional)
  A mapping of property names to values, which are used to configure workload execution.

* `effective_properties` -
  (Output)
  A mapping of property names to values, which are used to configure workload execution.

<a name="nested_environment_config"></a>The `environment_config` block supports:

* `execution_config` -
  (Optional)
  Execution configuration for a workload.
  Structure is [documented below](#nested_environment_config_execution_config).

* `peripherals_config` -
  (Optional)
  Peripherals configuration that workload has access to.
  Structure is [documented below](#nested_environment_config_peripherals_config).


<a name="nested_environment_config_execution_config"></a>The `execution_config` block supports:

* `service_account` -
  (Optional)
  Service account that used to execute workload.

* `network_tags` -
  (Optional)
  Tags used for network traffic control.

* `kms_key` -
  (Optional)
  The Cloud KMS key to use for encryption.

* `ttl` -
  (Optional)
  The duration after which the workload will be terminated.
  When the workload exceeds this duration, it will be unconditionally terminated without waiting for ongoing
  work to finish. If ttl is not specified for a session workload, the workload will be allowed to run until it
  exits naturally (or run forever without exiting). If ttl is not specified for an interactive session,
  it defaults to 24 hours. If ttl is not specified for a batch that uses 2.1+ runtime version, it defaults to 4 hours.
  Minimum value is 10 minutes; maximum value is 14 days. If both ttl and idleTtl are specified (for an interactive session),
  the conditions are treated as OR conditions: the workload will be terminated when it has been idle for idleTtl or
  when ttl has been exceeded, whichever occurs first.

* `staging_bucket` -
  (Optional)
  A Cloud Storage bucket used to stage workload dependencies, config files, and store
  workload output and other ephemeral data, such as Spark history files. If you do not specify a staging bucket,
  Cloud Dataproc will determine a Cloud Storage location according to the region where your workload is running,
  and then create and manage project-level, per-location staging and temporary buckets.
  This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.

* `subnetwork_uri` -
  (Optional)
  Subnetwork configuration for workload execution.

<a name="nested_environment_config_peripherals_config"></a>The `peripherals_config` block supports:

* `metastore_service` -
  (Optional)
  Resource name of an existing Dataproc Metastore service.

* `spark_history_server_config` -
  (Optional)
  The Spark History Server configuration for the workload.
  Structure is [documented below](#nested_environment_config_peripherals_config_spark_history_server_config).


<a name="nested_environment_config_peripherals_config_spark_history_server_config"></a>The `spark_history_server_config` block supports:

* `dataproc_cluster` -
  (Optional)
  Resource name of an existing Dataproc Cluster to act as a Spark History Server for the workload.

<a name="nested_jupyter_session"></a>The `jupyter_session` block supports:

* `kernel` -
  (Optional)
  Kernel to be used with Jupyter interactive session.
  Possible values are: `PYTHON`, `SCALA`.

* `display_name` -
  (Optional)
  Display name, shown in the Jupyter kernelspec card.

## Attributes Reference

In addition to the arguments listed above, the following computed attributes are exported:

* `id` - an identifier for the resource with format `{{name}}`

* `uuid` -
  A session template UUID (Unique Universal Identifier). The service generates this value when it creates the session template.

* `create_time` -
  The time when the session template was created.

* `update_time` -
  The time when the session template was updated.

* `creator` -
  The email address of the user who created the session template.

* `terraform_labels` -
  The combination of labels configured directly on the resource
   and default labels configured on the provider.

* `effective_labels` -
  All of labels (key/value pairs) present on the resource in GCP, including the labels configured through Terraform, other clients and services.


## Timeouts

This resource provides the following
[Timeouts](https://developer.hashicorp.com/terraform/plugin/sdkv2/resources/retries-and-customizable-timeouts) configuration options:

- `create` - Default is 20 minutes.
- `update` - Default is 20 minutes.
- `delete` - Default is 20 minutes.

## Import


SessionTemplate can be imported using any of these accepted formats:

* `{{name}}`


In Terraform v1.5.0 and later, use an [`import` block](https://developer.hashicorp.com/terraform/language/import) to import SessionTemplate using one of the formats above. For example:

```tf
import {
  id = "{{name}}"
  to = google_dataproc_session_template.default
}
```

When using the [`terraform import` command](https://developer.hashicorp.com/terraform/cli/commands/import), SessionTemplate can be imported using one of the formats above. For example:

```
$ terraform import google_dataproc_session_template.default {{name}}
```

## User Project Overrides

This resource supports [User Project Overrides](https://registry.terraform.io/providers/hashicorp/google/latest/docs/guides/provider_reference#user_project_override).
